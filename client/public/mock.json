[
  {
    "id": "ai-interview-coach",
    "name": "AI Interview Coach",
    "summary": "AI Interview Coach is a web app that simulates behavioral and technical interviews using an on-device microphone and real-time feedback. It analyzes speech pace, filler words, and content structure, then generates tailored follow-up questions. Candidates can practice in focused modes like system design, data structures, or product sense, and receive a rubric-scored report at the end. The goal is to make job prep more efficient, confidence-building, and measurable.\n\nUnder the hood, the app uses Next.js for the frontend, a FastAPI service for evaluation, and OpenAI for question generation. We rely on WebRTC for low-latency audio streaming, Pinecone for semantic retrieval of company-specific question banks, and PostgreSQL via Prisma for persistence. The project earned a finalist mention at a local hackathon and was recognized for best use of RAG for interview preparation.",
    "details": "Overview\nAI Interview Coach helps candidates rehearse interviews in realistic conditions. It provides timed rounds, interviewer personas, and supports both voice and text modes. After each session, users receive a detailed breakdown with strengths, growth areas, and curated resources.\n\nFeatures\n- Real-time filler word detection and pacing tips\n- Interviewer personas for behavioral, product sense, and system design\n- Rubric-based scoring with benchmarks for target roles\n- Personalized drills powered by retrieval-augmented generation\n\nHow we built it\n- Frontend: Next.js 14, React 19, Tailwind CSS, Zustand\n- Backend: FastAPI, PostgreSQL, Prisma, Redis for job queues\n- AI: OpenAI function calling, Whisper for transcription, Pinecone for RAG\n- Realtime: WebRTC and WebSocket signaling\n\nChallenges\n- Keeping latency low while streaming audio and generating follow-ups\n- Building realistic interviewer personas without hallucinations\n- Designing fair, transparent scoring rubrics\n\nWhat we learned\n- Prompt engineering for multi-turn evaluation is delicate\n- Server load spikes require careful rate limiting and batching\n- Good UX for feedback increases practice retention\n\nWhat’s next\n- Company-specific interview packs\n- Mobile-friendly practice flows\n- More granular analytics and calibration",
    "github": "https://github.com/example/ai-interview-coach",
    "demo": "https://www.youtube.com/watch?v=VE-JG_ZzF8s"
  },
  {
    "id": "smart-garden-monitor",
    "name": "Smart Garden Monitor",
    "summary": "Smart Garden Monitor tracks soil moisture, light exposure, and ambient temperature to keep plants healthy. A dashboard visualizes trends and recommends watering schedules personalized to each plant species. Users can set alerts and let the system auto-water with a connected pump. It aims to remove guesswork and prevent overwatering.\n\nThe stack includes an ESP32 sensor network, a Node.js API, and a Next.js dashboard. Time-series data is stored in TimescaleDB and visualized with Recharts, while MQTT links devices to the cloud. The project won a sustainability prize at a campus makeathon and was featured in a student tech showcase.",
    "details": "Overview\nA full IoT solution that helps urban gardeners monitor and automate plant care. The system fuses sensor readings with species profiles to suggest actions.\n\nFeatures\n- Per-plant dashboards and health scores\n- Threshold-based alerts over email and push\n- Auto-watering with safety cutoffs\n- Historical trends and anomaly detection\n\nHow we built it\n- Hardware: ESP32, capacitive soil probes, DHT22, light sensors\n- Firmware: MicroPython, OTA updates\n- Cloud: Node.js, MQTT, Redis, TimescaleDB\n- Web: Next.js 14, Tailwind, Recharts\n\nChallenges\n- Calibrating low-cost sensors for reliability\n- Avoiding pump overshoot when soil is heterogeneous\n- Securely onboarding devices over Wi-Fi\n\nWhat we learned\n- Practical MQTT patterns on flaky networks\n- TimescaleDB compression and retention policies\n- UX for physical systems needs clear states\n\nWhat’s next\n- Plant disease detection via leaf images\n- Community plant profiles and tips\n- HomeKit and Google Home integrations",
    "github": "https://github.com/example/smart-garden-monitor",
    "demo": "/rocket.png"
  },
  {
    "id": "campus-wayfinder",
    "name": "Campus Wayfinder",
    "summary": "Campus Wayfinder is a navigation app for large university campuses that blends indoor and outdoor routing. It helps students locate classrooms, offices, and events with accessible paths and live crowd indicators. Real-time schedules sync with calendars to suggest when to leave and which entrance to use. The purpose is to save time and reduce first-week confusion.\n\nWe built the frontend with Next.js and Mapbox GL for interactive maps. A Go microservice computes routes with elevation and accessibility costs, while Redis caches popular paths. The app received an honorable mention in a mapping challenge and was piloted during orientation week.",
    "details": "Overview\nWayfinding for complex campuses with accessibility-first routing. Indoor maps are stitched from floor plans and connected to outdoor networks.\n\nFeatures\n- Indoor and outdoor turn-by-turn directions\n- Accessibility filters for elevators and ramps\n- Live crowd density overlays and event pins\n- Calendar sync for class schedules\n\nHow we built it\n- Maps: Mapbox GL, vector tiles, custom style layers\n- Routing: Go, Dijkstra with penalties, Redis cache\n- Web: Next.js 14, React 19, Tailwind\n- Data: PostGIS for geometry and elevation\n\nChallenges\n- Aligning indoor floor plan coordinates with outdoor basemaps\n- Performance on mobile for dense venues\n- Keeping crowd data private and aggregate-only\n\nWhat we learned\n- Vector tile optimization and layer pruning\n- Usability testing for accessibility features\n- Importance of offline fallbacks\n\nWhat’s next\n- Transit integration and bike parking availability\n- AR arrows for indoor corridors\n- Community-sourced POIs",
    "github": "https://github.com/example/campus-wayfinder",
    "demo": "/ucsc.png"
  },
  {
    "id": "pulse-guardian",
    "name": "Pulse Guardian",
    "summary": "Pulse Guardian is a wellness companion that analyzes wearable data to forecast stress and recovery windows. It gives actionable suggestions like micro-breaks, hydration cues, and breathing exercises keyed to your current state. Users can set goals and track progress with weekly reports. The goal is to turn raw biometrics into gentle, timely nudges.\n\nThe app uses React Native for mobile, a Flask API for analytics, and a small on-device model for offline heuristics. We trained forecasting with Prophet and light gradient boosting, and store data in DynamoDB. The project was shortlisted for a health-tech prize and praised for balanced privacy defaults.",
    "details": "Overview\nA cross-platform companion that converts wearable signals into wellbeing actions. Works with common devices and supports offline heuristics.\n\nFeatures\n- Stress and recovery forecasting windows\n- Personalized breathing and micro-break plans\n- Weekly summaries with trends and streaks\n- Privacy-first with easy export and delete\n\nHow we built it\n- Mobile: React Native, Expo\n- Backend: Flask, DynamoDB, AWS Lambda\n- Modeling: Prophet, LightGBM, feature stores\n\nChallenges\n- Dealing with noisy and missing sensor data\n- Avoiding over-notification and alert fatigue\n- Designing clear consent and privacy controls\n\nWhat we learned\n- Practical feature engineering for time series\n- User testing to balance helpful vs. intrusive\n- Building compassionate UX for health contexts\n\nWhat’s next\n- Guided programs for sleep and focus\n- Secure sharing with clinicians\n- WearOS and watchOS complications",
    "github": null,
    "demo": "https://www.youtube.com/watch?v=H8UdKZf8uWk"
  },
  {
    "id": "chain-sage",
    "name": "Chain Sage",
    "summary": "Chain Sage is a portfolio tracker with explainable alerts for crypto assets. It consolidates wallets and exchanges, computes cost basis, and flags risk events with plain-language summaries. A learning mode breaks down jargon and simulates trades for practice. The purpose is to make crypto monitoring calmer and more transparent.\n\nWe built the interface with Next.js and shadcn UI, and used Rust microservices for indexing. A Kafka pipeline processes on-chain events, and PostgreSQL stores normalized positions. The app was a category winner for developer tooling at a regional hackathon.",
    "details": "Overview\nAn explainable portfolio tracker that connects to multiple wallets and exchanges. Emphasis on clear language and risk awareness.\n\nFeatures\n- Unified balances with cost basis and PnL\n- Risk alerts with human-readable rationales\n- Practice mode with sandboxed orders\n- Portfolio heatmaps and drift notifications\n\nHow we built it\n- Frontend: Next.js 14, Tailwind, shadcn UI\n- Services: Rust, Kafka, PostgreSQL\n- Integrations: Ethers.js, exchange APIs\n\nChallenges\n- Normalizing disparate exchange formats\n- Latency for on-chain event ingestion\n- Clear explanations without information overload\n\nWhat we learned\n- Type-safe API contracts across Rust and TS\n- Practical Kafka compaction settings\n- Visual encodings that reduce cognitive load\n\nWhat’s next\n- Tax export helpers\n- Multi-sig monitoring and alerts\n- Mobile-first views",
    "github": "https://github.com/example/chain-sage",
    "demo": "/vercel.svg"
  },
  {
    "id": "ar-home-designer",
    "name": "AR Home Designer",
    "summary": "AR Home Designer lets users visualize furniture and paint colors in their rooms using augmented reality. You can scan a space, drop true-scale models, and compare styles with side-by-side snapshots. An inspiration feed recommends layouts and palettes based on your preferences. The aim is to reduce returns and boost confidence before buying.\n\nWe used Three.js and WebXR for AR rendering with a Next.js shell. A small Python service generates color harmonies and runs object segmentation for occlusion. The project received a design excellence nod at a web graphics showcase.",
    "details": "Overview\nTry before you buy with browser-based AR for interiors. Supports accurate scale, occlusion, and lighting estimation.\n\nFeatures\n- Place and resize 3D models at true scale\n- Paint preview with material swapping\n- Style compare with A/B snapshots\n- Inspiration feed with saved boards\n\nHow we built it\n- Web: Next.js 14, Three.js, WebXR, React Three Fiber\n- Services: Python, FastAPI for color and segmentation\n- Assets: GLTF pipeline with Draco compression\n\nChallenges\n- Reliable plane detection on varied devices\n- Occlusion masks without depth sensors\n- Performance budgets for complex scenes\n\nWhat we learned\n- Three.js materials and PBR tuning\n- UX for constrained AR interactions\n- Asset pipelines and caching strategies\n\nWhat’s next\n- Multi-user design sessions\n- Retail catalog integrations\n- Room measurements and estimates",
    "github": null,
    "demo": "/next.svg"
  },
  {
    "id": "quillboard",
    "name": "Quillboard",
    "summary": "Quillboard is a collaborative whiteboard for brainstorming with sticky notes, sketches, and timers. Teams can cluster ideas with AI-assisted grouping and vote on priorities. Sessions are exportable to slides or task trackers with templates. The purpose is to keep workshops energetic yet structured.\n\nThe app runs on Next.js with a tRPC backend and a WebSocket signaling layer. We use CRDTs for conflict-free real-time editing and store sessions in PlanetScale. It won best real-time app at a hack night and was praised for latency and facilitation tools.",
    "details": "Overview\nA facilitation-first collaborative whiteboard with voting and timers. Works well on touch and stylus devices.\n\nFeatures\n- Sticky notes, pen, shapes, and templates\n- AI clustering and label suggestions\n- Dot voting and timeboxed rounds\n- Export to slides, Notion, and Jira\n\nHow we built it\n- Frontend: Next.js 14, Tailwind, tRPC\n- Realtime: WebSockets, CRDTs (Yjs)\n- Data: PlanetScale MySQL\n\nChallenges\n- Maintaining consistency across many cursors\n- Designing facilitation flows that scale\n- Ensuring smooth performance on large boards\n\nWhat we learned\n- Practical CRDT trade-offs\n- Keyboard shortcuts drive adoption\n- Templates speed up recurring ceremonies\n\nWhat’s next\n- Voice rooms with spatial audio\n- Advanced analytics for ideation patterns\n- Public board galleries",
    "github": "https://github.com/example/quillboard",
    "demo": "https://www.youtube.com/watch?v=Z9AYPxH5NTM"
  },
  {
    "id": "flavor-finder",
    "name": "Flavor Finder",
    "summary": "Flavor Finder recommends recipes based on what you have in the fridge and your dietary preferences. You can scan ingredients, filter by time and difficulty, and get step-by-step guidance. A pantry mode suggests smart substitutions for missing items. The aim is to make home cooking more flexible and less wasteful.\n\nWe used Next.js for the web app, a Python FastAPI for retrieval, and a small ranking model for taste compatibility. Images are processed with a lightweight vision model, and vector search runs on Pinecone. The project received a food innovation award and positive feedback from home cooks.",
    "details": "Overview\nA practical cooking companion that adapts recipes to your pantry and goals. Focus on saving time and reducing waste.\n\nFeatures\n- Ingredient scanning and recognition\n- Substitution suggestions and dietary filters\n- Step-by-step mode with timers\n- Smart grocery list generation\n\nHow we built it\n- Web: Next.js 14, Tailwind, Headless UI\n- Backend: FastAPI, Pinecone for vector search\n- Modeling: small ranking model for taste fit\n\nChallenges\n- Reliable ingredient detection with messy photos\n- Balancing nutrition with taste preferences\n- Keeping recommendations diverse and novel\n\nWhat we learned\n- RAG for recipes needs strict grounding\n- UX around substitutions needs clear confidence\n- Small models can be very effective\n\nWhat’s next\n- Seasonal bundles and meal planning\n- Smart oven integrations\n- Community recipe sharing",
    "github": null,
    "demo": "/lynbrook.png"
  },
  {
    "id": "lingua-mentor",
    "name": "Lingua Mentor",
    "summary": "Lingua Mentor is a conversational language tutor with role-play scenarios and spaced repetition. Learners practice speaking with real-time pronunciation scoring and cultural notes. The app adapts difficulty and reviews weak spots with memory-safe intervals. The purpose is to make practice immersive and sticky.\n\nThe stack includes Next.js for the client, a NestJS API, and WebRTC for voice calls. We use speech-to-text with Whisper, text-to-speech with edge voices, and vector search to surface thematic prompts. It earned a teaching and learning honorable mention for progress-tracking clarity.",
    "details": "Overview\nA tutor that focuses on conversation, feedback, and memory. Role-play covers travel, work, and daily life.\n\nFeatures\n- Real-time pronunciation scoring\n- Adaptive dialogue with grammar hints\n- Spaced repetition reviews\n- Cultural tidbits and idioms\n\nHow we built it\n- Client: Next.js 14, Zustand, Tailwind\n- Server: NestJS, PostgreSQL, Redis\n- Voice: WebRTC, Whisper, neural TTS\n\nChallenges\n- Accurate feedback without overcorrection\n- Smooth turn-taking in voice mode\n- Designing a fair proficiency rubric\n\nWhat we learned\n- Conversation flows as state machines\n- Mixing implicit and explicit correction\n- The value of gentle, formative feedback\n\nWhat’s next\n- CEFR-aligned assessments\n- Homework packs and teacher dashboards\n- Offline practice on mobile",
    "github": "https://github.com/example/lingua-mentor",
    "demo": "/usc.png"
  },
  {
    "id": "crisis-relief-map",
    "name": "Crisis Relief Map",
    "summary": "Crisis Relief Map aggregates requests for aid and available resources during disasters. Volunteers can verify posts, match needs to offers, and visualize logistics on a live map. A triage queue helps coordinators prioritize by urgency and proximity. The mission is to improve coordination when minutes matter.\n\nWe built a resilient web app with Next.js and a Go API that persists to PostgreSQL with point and polygon types. WebSockets power live updates and moderation workflows, and offline caches keep maps usable if connectivity drops. The project received a community impact award from a civic tech group.",
    "details": "Overview\nA disaster coordination tool to match needs and offers on an up-to-date map. Designed for reliability under strain.\n\nFeatures\n- Geocoded posts with urgency and categories\n- Volunteer verification and moderation\n- Matching engine for needs and offers\n- Offline caches and low-bandwidth tiles\n\nHow we built it\n- Web: Next.js 14, Tailwind, Mapbox GL\n- API: Go, PostgreSQL with PostGIS\n- Realtime: WebSockets, optimistic UI\n\nChallenges\n- Handling sudden traffic spikes\n- Preventing misinformation and duplicates\n- Supporting low-connectivity environments\n\nWhat we learned\n- Operational playbooks matter\n- Preloaded tiles speed up first render\n- Clear status states reduce confusion\n\nWhat’s next\n- SMS bridges for feature phones\n- NGO and municipal data connectors\n- Translation workflows",
    "github": "https://github.com/example/crisis-relief-map",
    "demo": "https://www.youtube.com/watch?v=9bZkp7q19f0"
  }
]
