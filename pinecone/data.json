[
  {
    "id": "talktuahbank",
    "name": "TalkTuahBank",
    "summary": "TalkTuahBank is a voice-based banking assistant designed to make financial services accessible to underserved populations globally. By facilitating interactions through simple phone calls, it eliminates barriers such as the need for internet access, smartphone ownership, or digital literacy. Users can perform essential banking tasks like checking balances, transferring funds, and paying bills using natural voice commands in multiple languages, ensuring inclusivity for communities often overlooked by traditional banks. This innovative approach empowers individuals to manage their finances effortlessly, regardless of their technological capabilities or geographical location.\n\nThe project utilizes a robust tech stack, including Retell AI for natural language processing, OpenAI Swarm for dialogue orchestration, and Pinata (IPFS) for secure, decentralized data storage. An admin dashboard built with Next.js, Tailwind CSS, ShadCN UI, and Aeternity supports user and transaction management. TalkTuahBank's groundbreaking work in democratizing financial technology earned it top recognition at HackUTD 2024: Ripple Effect, where it won both the General Category and the Goldman Sachs Award for innovation and inclusivity.",
    "details": "Inspiration\nAccess to banking services is a fundamental aspect of economic participation, yet over 1.7 billion adults worldwide remain unbanked. Traditional banking solutions often require internet access, digital literacy, or proximity to physical branches—barriers that leave underserved populations without essential financial tools. Inspired by the need for inclusivity and the power of conversational AI, we envisioned TalkTuahBank, a voice-based banking assistant that democratizes financial services by making them accessible to everyone, regardless of their technological capabilities or geographical location.\n\nWhat It Does\nTalkTuahBank revolutionizes the banking experience by offering a fully conversational AI assistant accessible through simple phone calls. Users can effortlessly manage their finances by performing tasks such as checking account balances, transferring funds, and paying bills using natural voice commands. Key features include:\n\n- Voice-Activated Services: Conduct banking transactions hands-free via phone.\n- Multi-Language Support: Interact in multiple languages and dialects to accommodate diverse users.\n- Accessibility: Operates on any phone without the need for internet or smartphones.\n- AI Assistance: Receive personalized financial advice and support through intelligent interactions.\n\nHow We Built It\n- **Conversational AI**: Retell AI for natural language understanding and generation, enabling smooth voice interactions.\n- **Multi-Agent Framework**: OpenAI Swarm for orchestration of dialogues and backend processes.\n- **Secure Storage**: Pinata (IPFS) for decentralized, secure transaction and user data storage.\n- **Admin Dashboard**: Next.js, Tailwind CSS, ShadCN UI, and Aeternity for monitoring, management, and support.\n- **Telephony Integration**: Retell API for handling and processing calls.\n\nChallenges\n- Ensuring robust **security** for sensitive financial data.\n- Managing the **integration** of multiple advanced technologies.\n- Building a smooth, natural **voice user experience** across languages.\n\nAccomplishments\n- Delivered a **functional prototype** enabling essential banking tasks via voice.\n- Enabled **multi-language support**, broadening accessibility.\n- Implemented **secure decentralized storage** with Pinata.\n- Developed a **comprehensive admin dashboard** for user and transaction management.\n- Earned recognition at **HackUTD 2024: Ripple Effect**, winning both the **General Category** and the **Goldman Sachs Award** for innovation and inclusivity in financial technology.\n\nWhat We Learned\n- Nuances of **voice technology** and user-centered design.\n- Importance of **layered security** in financial systems.\n- Strategies for **integrating diverse technologies** into a seamless solution.\n- Value of **inclusive design** for underserved communities.\n\nWhat’s Next\n- Add more **languages and dialects**.\n- Introduce **fraud detection** with machine learning.\n- Provide **financial literacy modules** for users.\n- Develop a **complementary mobile app** for smartphone users.\n- Build partnerships with **local banks and NGOs** to expand reach.\n- Continuously enhance **security protocols** to protect user data.\n",
    "github": "https://github.com/aurelisajuan/TalkTuahBank",
    "demo": "https://youtu.be/YsH_z1azXSA"
  },
  {
    "id": "dispatch-ai",
    "name": "Dispatch AI",
    "summary": "Dispatch AI is an innovative, AI-powered system designed to revolutionize emergency call handling by providing empathetic and intelligent support. The platform centralizes 911 calls, categorizing them by severity and extracting crucial details such as location, time, and caller emotions to recommend appropriate actions. Human dispatchers maintain full control, making final decisions to ensure a balance between AI efficiency and human oversight. By addressing the staffing challenges faced by 82% of emergency call centers, Dispatch AI aims to reduce wait times and enhance response effectiveness during critical situations.\n\nBuilt using a robust tech stack, Dispatch AI features a frontend developed with Next.js, TailwindCSS, and Leaflet, while the backend utilizes Python, Twilio, and Google Maps APIs. The system integrates a fine-tuned Mistral model, optimized with Intel Dev Cloud and Intel Extension for PyTorch, achieving an 80% reduction in inference time. The project has been acclaimed for its innovation and impact, winning the Grand Prize at the UC Berkeley AI Hackathon 2024, including a $25,000 Berkeley SkyDeck Fund investment, the AI For Good Award by AIC, and the Best Use of Intel AI.",
    "details": "Inspiration\nImagine: A major earthquake hits. Thousands call 911 simultaneously. In the call center, a handful of operators face an impossible task. Every line is ringing. Every second counts. There aren't enough people to answer every call. This isn't just hypothetical. It's a real risk in today's emergency services. A startling 82% of emergency call centers are understaffed, pushed to their limits by non-stop demands. During crises, when seconds mean lives, staffing shortages threaten our ability to mitigate emergencies.\n\nWhat it does\nDispatchAI reimagines emergency response with an empathetic AI-powered system. It leverages advanced technologies to enhance the 911 call experience, providing intelligent, emotion-aware assistance to both callers and dispatchers.\n\nEmergency calls are aggregated onto a single platform, and filtered based on severity. Critical details such as location, time of emergency, and caller's emotions are collected from the live call. These details are leveraged to recommend actions, such as dispatching an ambulance to a scene.\n\nOur human-in-the-loop system enforces control of human operators is always put at the forefront. Dispatchers make the final say on all recommended actions, ensuring that no AI system stands alone.\n\nHow we built it\nWe developed DispatchAI using a comprehensive tech stack:\n\nFrontend:\n- Next.js with React for a responsive and dynamic user interface\n- TailwindCSS and Shadcn for efficient, customizable styling\n- Framer Motion for smooth animations\n- Leaflet for interactive maps\n\nBackend:\n- Python for server-side logic\n- Twilio for handling calls\n- Hume and Hume's EVI for emotion detection and understanding\n- Retell for implementing a voice agent\n- Google Maps geocoding API and Street View for location services\n- Custom-finetuned Mistral model using our proprietary 911 call dataset\n- Intel Dev Cloud for model fine-tuning and improved inference\n\nChallenges we ran into\n- Curated a diverse 911 call dataset\n- Integrating multiple APIs and services seamlessly\n- Fine-tuning the Mistral model to understand and respond appropriately to emergency situations\n- Balancing empathy and efficiency in AI responses\n\nAccomplishments that we're proud of\n- Successfully fine-tuned a Mistral model for emergency response scenarios\n- Developed a custom 911 call dataset for training\n- Integrated emotion detection to provide more empathetic responses\n- Leveraged Intel Dev Cloud for live demonstrations and optimized performance\n- Achieved an 80% reduction in inference time using Intel® Extension for PyTorch (IPEX)\n- **Winner of UC Berkeley AI Hackathon 2024 Grand Prize**: $25,000 Berkeley SkyDeck Fund investment + Golden Ticket to SkyDeck Pad-13\n- **Winner of AI For Good Award** by Academic Innovation Catalyst (AIC)\n- **Winner of Best Use of Intel AI**\n\nInnovation\n- First empathetic, AI-powered dispatcher agent designed to support first responders during resource-constrained situations\n- Novel integration of emotion detection and AI assistance in emergency calls\n\nImpact\n- Addresses the 82% of understaffed call centers\n- Aims to reduce wait times in critical emergencies (e.g., Oakland’s 1+ minute 911 wait times)\n- Potential to save lives by ensuring every emergency call is answered promptly\n\nBonus Points\n- Open-sourced fine-tuned LLM on HuggingFace: https://huggingface.co/spikecodes/ai-911-operator\n- Published training dataset: https://huggingface.co/datasets/spikecodes/911-call-transcripts\n- Submitted to Powered By Intel LLM leaderboard: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n- Promoted project on Twitter (X) using #HackwithIntel: https://x.com/spikecodes/status/1804826856354725941\n\nWhat we learned\n- How to integrate multiple technologies into a cohesive system\n- The potential of AI to augment critical public services\n\nWhat's next for DispatchAI\n- Expand dataset with more diverse emergency scenarios\n- Collaborate with local emergency services for real-world testing\n- Explore future integrations\n",
    "github": "https://github.com/IdkwhatImD0ing/DispatchAI",
    "demo": "https://youtu.be/hdpdgxrilQM"
  },
  {
    "id": "teachme-3p7bw1",
    "name": "AdaptEd",
    "summary": "AdaptEd is an innovative AI-driven educational platform designed to transform traditional lectures into engaging, interactive conversations. By utilizing a live humanoid AI lecturer, AdaptEd allows students to engage in real-time dialogue, where the lecture slides and content dynamically adjust based on their responses. This approach aims to provide personalized learning experiences, making quality education accessible to a broader audience by adapting the educational system to the needs of each student rather than the other way around.\n\nThe platform leverages advanced technologies such as Gemini 1.5 Pro for aggregating diverse data sources, Fetch.ai for enabling multitasking agents, and Hume for real-time emotion detection. Intel Developer Cloud was used for fine-tuning models to ensure accurate, real-time responses. Recognized for its innovation, AdaptEd won the Google Company Challenge at LA Hacks 2024 and received accolades at the Intel Dev Cloud Hackathon for its multidimensional agentic system design, highlighting its potential to revolutionize personalized learning on a global scale.",
    "details": "As a K-12 tutor for Kumon, one of our teammates realized the massive impact that personalized, 1:1 tutoring had on students. However, not everyone has access to enrichment programs, creating a compounding limitation that affects success in future endeavors.\n\nOut of 16 million university students in the United States, 50% of them are falling behind due to static and one-sided teaching without personalized engagement. At the same time, less than 3% of students have access to quality tutoring programs.\n\nAdaptEd turns the educational paradigm on its head: instead of students adapting to the system, our AI lecturer adapts to students. Through real-time conversation, verbal/visual content changes dynamically.\n\nWhat it does\nAdaptEd turns lectures into conversations with a live humanoid lecturer that a user can speak to directly. Corresponding lecture slides are generated live based on the user's verbal response, creating an adaptive learning experience.\n\nIt performs 3 main functions:\n- Responsive AI: Users can hold natural conversations with the lecturer.\n- Dynamic Content: Slideshow and whiteboard adapt in real time based on verbal response.\n- Emotion Detection: Measures the user's attention and confusion.\n\nHow we built it\nMulti-source Aggregation Pipeline:\n- Gemini 1.5 Pro: Huge context length allows us to aggregate data from audio (YouTube videos, podcasts), video (lectures), and massive text sources (textbooks, Wikipedia).\n\nAction-Taking Dynamic Agent:\n- Interruption and End-of-Turn Detection allows for natural conversation.\n- Real-time action-taking lets the agent modify slides/whiteboard dynamically.\n\nIntel Developer Cloud:\n- Used to fine-tune an open-source model for more accurate responses.\n- Served as a real-time inference engine for fast generation.\n\nFetch.ai:\n- Agents perform side tasks without interrupting the main speaker.\n- Proxy queries and callbacks allow seamless multitasking while lectures continue.\n\nOther Integrations:\n- Google Search: Adds images and dynamic content to slides.\n- MongoDB: Stores lectures for later review.\n- Auth0: Saves user statistics and progress.\n- Hume: Real-time emotion detection for tailored responses.\n\nChallenges we ran into\n- Steep learning curve with Fetch.AI and Intel Developer Cloud (IDC).\n- Sparse documentation and hardware limitations restricted full model performance.\n\nAccomplishments that we're proud of\n- Functional multidimensional agentic system integrating multiple AI agents.\n- Successful fine-tuning of models on IDC for real-time education use.\n- **Winner of the Google Company Challenge at LA Hacks 2024.**\n- Recognition at the Intel Dev Cloud Hackathon for multidimensional agentic system design.\n\nWhat we learned\n- How to design agentic, multi-layered education systems.\n- Importance of leveraging example notebooks and platform-specific tools.\n- Trade-offs of hardware vs. model complexity in real-time systems.\n\nWhat's next for AdaptEd\n- Add more agents for expanded capabilities.\n- Integrate with Google Docs for collaborative learning.\n- Personal history/statistics visualizer for deeper personalization.\n- Enhance scaling to reach broader classrooms and institutions.",
    "github": "https://github.com/IdkwhatImD0ing/AdaptEd/pulls",
    "demo": "https://youtu.be/8o1YJUFBcAw"
  },
  {
    "id": "courtvision-gtui7w",
    "name": "CourtVision",
    "summary": "CourtVision revolutionizes sports analysis by transforming standard 2D game footage into dynamic 3D replays, enabling athletes to thoroughly review their performances from multiple angles. This tool captures intricate details often missed by traditional film, such as body alignment, foot placement, and reaction timing. By integrating a multimodal language model, CourtVision interprets posture and intent, offering real-time, voice-controlled feedback through Vapi’s Voice API. This combination of features allows athletes to engage in a more interactive and comprehensive performance analysis.\n\nThe project employs Video Gaussians for 3D scene reconstruction, a multimodal LLM for detailed analysis, and Vapi’s Voice API for seamless voice interactions. Designed for both desktop and VR, CourtVision facilitates remote coaching and shared 3D walkthroughs. Its innovation in sports analytics was recognized at the UC Berkeley AI Hackathon 2025, highlighting its impact on enhancing athlete training through AI-driven insights and immersive replays.",
    "details": "Inspiration / Problem:\nAthletes often rely on film to improve, but standard 2D footage is flat, fixed, and misses critical details like body alignment and reaction timing. CourtVision was built to address these gaps.\n\nWhat It Does:\n- Converts 2D game clips into accurate 3D Gaussian Splat replays.\n- Allows athletes to rotate, zoom, slow down, and view plays from multiple perspectives.\n- Integrates with a multimodal LLM to interpret posture, intent, and motion.\n- Provides real-time voice-controlled Q&A using Vapi’s Voice API.\n\nKey Features:\n- **Skill Review:** Study real performance rather than limited camera angles.\n- **Posture & Form:** Detect back rounding, missteps, or poor landings.\n- **Decision Analysis:** Explore alternative moves and identify missed opportunities.\n- **Trainer Feedback:** Enables remote coaching with shared 3D walkthroughs.\n\nHow It Was Built:\n- **Video Gaussians:** 3D scene reconstruction from 2D footage with time consistency.\n- **Multimodal LLM:** Frame-by-frame interpretation of posture and intent.\n- **Vapi Voice API:** Natural voice interactions for immediate performance insights.\n\nAccomplishments:\n- Delivered immersive 3D replay technology that enhances player self-review.\n- Created a voice-first interface for intuitive tactical feedback.\n- Designed athlete-focused workflows for both desktop and VR use.\n- **Recognized at the UC Berkeley AI Hackathon 2025** for innovation in sports analytics.\n\nChallenges:\n- Ensuring accurate time-synced Gaussian splats.\n- Designing a natural feedback loop between athletes and AI.\n\nWhat We Learned:\n- Advanced integration of Gaussian reconstruction with real-time feedback systems.\n- Practical applications of multimodal AI in sports contexts.\n\nWhat’s Next:\n- Shared VR sessions for team film review.\n- Automated body tracking with statistical overlays.\n- AI-driven “What if” simulation tools for alternate decision testing.\n- Voice-enabled coach-athlete interactions during replays.",
    "github": "https://github.com/aurelisajuan/ai2025.git",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/502/448/datas/gallery.jpg"
  },
  {
    "id": "sentinelai-dec0jp",
    "name": "SentinelAI",
    "summary": "SentinelAI is a cutting-edge public safety orchestration system designed to enhance security in crowded environments like schools, malls, and stadiums. It continuously monitors live audio streams to detect early signs of chaos, such as screams or alarms, and triggers AI-driven responses in under a second. By seamlessly integrating with smart building systems, SentinelAI provides calm and context-aware evacuation or lockdown instructions, ensuring rapid and coordinated responses while allowing human operators to maintain control. This system effectively transforms buildings into proactive safety partners, potentially saving lives during critical moments.\n\nUtilizing advanced technologies like fine-tuned Whisper and YAMNet models for real-time panic detection, SentinelAI employs a streaming LLM Superintendent powered by vLLM and Llama 3.3 for instant guidance. It integrates with Twilio and Retell for real-time, interruptible voice interactions and features an interactive 3D safety dashboard built with Next.js and Three.js. SentinelAI's innovative approach has been recognized as the winner of the Best AI/ML Hack at HackDavis 2025, highlighting its potential to revolutionize emergency response systems.",
    "details": "Inspiration / Problem:\nIn crisis scenarios such as shootings, fires, or terrorist threats, panic spreads within seconds, but coordinated emergency responses often take 9 minutes or more. Over 70% of casualties occur in the first 5 minutes, yet fewer than 5% of commercial spaces have real-time detection or orchestration systems. SentinelAI was created to close this gap and transform buildings into proactive safety partners.\n\nWhat It Does:\n- Monitors live audio streams to detect signs of chaos (screams, alarms, glass breaking).\n- Uses an LLM-powered Superintendent to provide calm, spoken evacuation or lockdown instructions in under one second.\n- Integrates with smart doors, alarms, signage, and lighting systems.\n- Coordinates responses across multiple zones with human-in-the-loop oversight via a 3D dashboard.\n\nKey Features:\n- **Panic Detection Engine:** Fine-tuned Whisper + YAMNet for real-time recognition of panic audio.\n- **Sentinel Superintendent:** Streaming vLLM + Llama 3.3 for instant guidance.\n- **Building Integration:** Direct control over IoT-connected alarms, locks, and signage.\n- **Multi-Zone Awareness:** Tailored responses for different areas simultaneously.\n- **Command Dashboard:** 3D visualization for security teams built with Next.js + Three.js.\n\nChallenges:\n- Achieving sub-second latency without compromising accuracy.\n- Limited real-world panic datasets for fine-tuning.\n- Simulating IoT integrations before physical hardware deployment.\n- Maintaining synchronized state across high-load environments with Supabase.\n\nAccomplishments:\n- Consistently reached <500 ms detection-to-response pipeline.\n- Built the first streaming LLM superintendent for evacuation and lockdown scenarios.\n- Integrated Twilio & Retell for real-time, interruptible phone-call guidance.\n- Delivered an interactive 3D building safety dashboard.\n- **Winner of Best AI/ML Hack at HackDavis 2025.**\n\nWhat We Learned:\n- Multi-modal pipelines require tight optimization for trust and reliability.\n- Human oversight features are critical to user adoption.\n- Edge-first design lowers latency and improves resilience.\n- Simulation environments accelerate hardware integration.\n\nWhat’s Next:\n- Heatmap and camera fusion for combined audio-visual incident detection.\n- On-device fail-safes for offline resilience.\n- Real-time multilingual instruction support.\n- A mobile companion app for alerts and remote overrides.\n\nHook:\n“What if every building had its own Jarvis during a crisis?” SentinelAI acts instantly, guiding people through danger before human responders can reach them.",
    "github": "https://github.com/Christopher-Chhim/HackDavis",
    "demo": "https://youtu.be/qr3pMPU8lFY"
  },
  {
    "id": "slugmeditate",
    "name": "SlugMeditate",
    "summary": "SlugMeditate transforms personal thoughts into immersive virtual reality experiences, offering a unique approach to mindfulness and stress reduction. By converting text prompts like \"a peaceful forest at dusk\" into vivid 3D environments, it provides users with personalized meditative spaces that enhance emotional well-being. This innovative tool blends creativity and technology to make meditation more accessible and engaging.\n\nThe project leverages cutting-edge technologies, including Google Imagen 3 for text-to-image generation, Veo 2 for animating visuals, and Gaussian Splatting for constructing navigable 3D worlds. It utilizes Niantic Studio WebXR for seamless browser-based VR rendering and Google MusicFX for creating ambient soundscapes. Recognized for its innovation, SlugMeditate won the MLH Best Use of Gemini API Award and the Niantic Studio WebXR Track Award at CruzHacks 2025.",
    "details": "Inspiration / Problem:\nMany students and individuals struggle with stress, burnout, and mental fatigue. Meditation is powerful, but traditional methods can feel repetitive or inaccessible. SlugMeditate reimagines mindfulness by turning any calming thought into a personalized VR escape — blending creativity, technology, and wellness.\n\nWhat It Does:\n- Converts a text prompt (e.g., 'a peaceful forest at dusk') into an immersive VR environment.\n- Generates images using Google Imagen 3 and animates them with Veo 2.\n- Reconstructs video into a navigable 3D world with Gaussian Splatting.\n- Renders the space in-browser via Niantic Studio WebXR.\n- Complements visuals with ambient music created through Google MusicFX.\n\nKey Features:\n- **Text-to-Image:** Rich visuals powered by Imagen 3.\n- **Image-to-Video:** Cinematic video loops with Veo 2.\n- **3D Scene Mapping:** Gaussian Splatting for VR navigation.\n- **Browser VR Rendering:** Seamless WebXR deployment using Niantic Studio.\n- **Generative Audio:** Personalized ambient music via MusicFX.\n\nChallenges:\n- Limited documentation and trial-and-error with Google AI tools.\n- Hardware constraints in Gaussian Splatting workflows.\n- Balancing camera motion for accurate 3D reconstruction.\n- Learning Niantic Studio WebXR from scratch.\n- Unstable Wi-Fi at the hackathon when generating AI assets.\n\nAccomplishments:\n- Built a complete text-to-VR pipeline within 48 hours.\n- Combined cutting-edge generative AI with VR rendering in real time.\n- Delivered a creative, emotionally resonant meditation experience.\n- Quickly mastered Imagen 3, Veo 2, and Gaussian Splatting.\n- **Winner of the MLH Best Use of Gemini API Award at CruzHacks 2025.**\n- **Winner of the Niantic Studio WebXR Track Award at CruzHacks 2025.**\n\nWhat We Learned:\n- Practical use of Google AI Gemini for generative workflows.\n- How to merge AI media generation with spatial computing.\n- Rapid prototyping and team collaboration under pressure.\n\nWhat’s Next:\n- **Interactivity:** Enable users to move and interact in VR spaces.\n- **Customization:** Let users adjust music, ambient sounds, or guided meditations.\n- **Optimization:** Reduce generation and rendering times.\n- **Community Gallery:** A platform to share meditative creations.\n\nHook:\n“Imagine turning your thoughts into a world you can step into.” SlugMeditate makes mindfulness deeply personal, immersive, and creative.",
    "github": "https://github.com/briankhoi/slugmeditate",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/374/704/datas/gallery.jpg"
  },
  {
    "id": "secway",
    "name": "SecWay",
    "summary": "SecWay is an AI-powered Chrome extension designed to simplify digital privacy and cybersecurity for everyday users. It continuously monitors websites and extensions to identify risky permissions, suspicious data requests, and potential phishing threats. The extension provides real-time privacy scanning, visual risk ratings, and one-click permission cleanup, empowering users to maintain online safety without needing technical expertise. With educational nudges and AI-powered guidance, SecWay helps users make informed decisions about their digital privacy.\n\nThe project utilizes Gemini 2.5-Turbo for conversational guidance, leveraging Google Safe Browsing and PhishTank APIs for real-time risk assessment. Built with a modular architecture, SecWay is designed for easy expansion to browsers like Firefox and Edge. Its privacy-first architecture ensures no backend data collection, focusing on user-centric security. SecWay’s innovative approach to accessible cybersecurity was recognized at DiamondHacks 2025, highlighting its commitment to making digital privacy both educational and proactive.",
    "details": "Inspiration\nToday’s web is full of hidden threats—phishing scams, data leaks, and overreaching permissions. But most users don’t read the fine print. In fact, 62% never change app or website permissions after first use. This puts millions at risk, especially those who aren't tech-savvy.\n\nInspired by the need for simple, accessible cybersecurity, SecWay is an AI-powered Chrome extension that demystifies digital privacy.\n\nWhat It Does\nSecWay acts as a lightweight, always-on browser companion that helps users see and understand when websites or extensions are asking for too much. Key features include:\n\n- **Real-Time Privacy Scanner:** Detects risky permissions, overreaching data requests, and suspicious website behavior.\n- **AI-Powered Privacy Guidance:** Gemini 2.5-Turbo provides human-friendly explanations and smart recommendations—no technical knowledge needed.\n- **Educational Nudges:** “Did You Know?” style insights explain why a permission could be dangerous, tailored to user behavior and comfort level.\n- **Risk Indicators:** Simple visual privacy ratings appear as you browse—green (safe), orange (review), red (danger).\n- **Secure Me Button:** One-click cleanup lets users instantly apply safe permission defaults suggested by AI.\n\nChallenges We Ran Into\n- Balancing simplicity and depth—making the experience clear for non-tech users while still delivering powerful analysis.\n- Designing a local-only architecture without backend data collection.\n- Tuning phishing detection models to minimize false positives.\n\nAccomplishments We're Proud Of\n- Built a **privacy-first architecture**: no backend, no data collection.\n- Developed a **modular extension design** for easy expansion to Firefox and Edge.\n- Created a **Gemini 2.5-Turbo conversational UX** that educates while protecting.\n- Implemented **one-click AI-powered protection** for immediate safety.\n- **Recognized at DiamondHacks 2025** for advancing accessible cybersecurity.\n\nWhat We Learned\n- The importance of user-centric security design over fear-based tactics.\n- Practical applications of conversational AI in browser extensions.\n- How to combine APIs like Google Safe Browsing and PhishTank for real-time risk intelligence.\n\nWhat's Next for SecWay\n- Extend support to **Firefox and Edge**.\n- Build outreach programs with **schools and libraries** for privacy awareness.\n- Add a **community-driven scam detection leaderboard**.\n- Develop **mobile browser tips** for smartphones.\n- Introduce a **Parental Control Mode** for families.\n- Enhance **threat intelligence** with continual model updates.",
    "github": "https://github.com/aurelisajuan/DiamondHacks",
    "demo": "https://youtu.be/s5A4NHD3QEo"
  },
  {
    "id": "weeee-i-love-reading-documentation",
    "name": "Vocalyze",
    "summary": "Vocalyze is a conversational AI assistant designed to simplify complex banking processes, like credit and loan applications, through natural phone conversations. By converting speech into accurately completed forms, it eliminates the need for manual entry and reduces errors. Vocalyze also clarifies financial terminology in real-time, providing a more accessible and stress-free experience for users while easing the workload on bank staff.\n\nThe project integrates advanced technologies including Retell AI for seamless voice call interactions, OpenAI GPT-4 for intelligent and context-aware dialogue, Letta for managing conversational flow, and Supabase for real-time database management. Vocalyze's innovative approach earned it the Letta AI Award and the Business and Finance Award at HackMerced X, showcasing its potential to transform the banking industry with its scalable and future-ready architecture.",
    "details": "Inspiration / Problem:\nBanking applications are often tedious, confusing, and filled with jargon. Customers abandon forms or make costly mistakes, while bank staff face high workloads from repetitive inquiries. Vocalyze was created to simplify this process and make finance more accessible through natural conversation.\n\nWhat It Does:\n- **Conversational Banking:** Users talk through credit and loan applications over a phone call.\n- **Automatic Form Filling:** Speech is converted directly into accurate, completed forms.\n- **Real-Time Clarification:** Explains financial terms as users encounter them.\n- **Error Reduction:** Ensures fewer mistakes through guided, conversational input.\n\nHow It Works:\n- **Retell AI:** Handles voice call integration for smooth conversational experiences.\n- **OpenAI GPT-4:** Powers intelligent, context-aware responses and term explanations.\n- **Letta:** Manages dialogue state to ensure seamless conversation flow.\n- **Supabase:** Provides real-time database management for instant updates and storage.\n\nChallenges:\n- Outdated and incomplete documentation slowed development.\n- Integrating voice calls, real-time databases, and conversational agents required extensive troubleshooting.\n- Ensuring accuracy and low latency in real-time data syncing was critical.\n\nAccomplishments:\n- Built a fully functional conversational AI banking assistant from scratch.\n- Successfully integrated Retell AI, GPT-4, Letta, and Supabase into one cohesive system.\n- Delivered a demo-ready prototype showcasing real-world applicability.\n- **Winner of the Letta AI Award at HackMerced X.**\n- **Winner of the Business and Finance Award at HackMerced X.**\n\nDifferentiators:\n- **Interruptible Conversations:** Users can naturally cut in, creating a human-like flow.\n- **Instant Application Completion:** Eliminates manual form entry entirely.\n- **Scalable Multilingual Support:** Architecture allows for future expansion to multiple languages.\n\nWhat We Learned:\n- Best practices for combining stateful conversational AI with real-time voice calls.\n- Effective strategies for handling unreliable documentation.\n- Building resilient systems under time and integration constraints.\n\nWhat’s Next:\n- Implement multilingual support to expand financial inclusivity.\n- Partner with banks to pilot real-world deployments.\n- Add financial literacy features to empower users with knowledge during conversations.\n\nHook:\n\"Banking should be as simple as a conversation.\" Vocalyze brings that vision to life by removing complexity, reducing stress, and making finance more accessible.",
    "github": "https://github.com/IdkwhatImD0ing/idkwhatthisprojectis",
    "demo": "https://youtu.be/s8wF-xCPY04"
  },
  {
    "id": "tft-team-food-tactics",
    "name": "TFT: TeamFood Tactics",
    "summary": "TFT: TeamFood Tactics is an innovative AI-powered platform that addresses food waste and insecurity by connecting suppliers with surplus food to individuals with specific dietary needs. Utilizing a simple voice-first interface, the platform is accessible to anyone with a phone, allowing suppliers to update available inventory and users to receive personalized recommendations for nearby food sources. By processing voice and image inputs, TFT ensures that surplus food is efficiently redirected to those who need it most, fostering a more sustainable food distribution system.\n\nThe platform is built using Next.js and Tailwind CSS for the frontend, with a backend powered by Python and FastAPI. It leverages APIs such as Retell AI and OpenAI for advanced data processing, with Supabase handling database management. TFT's impactful approach earned it the Sustainability – Green City Award at SpartaHack X, highlighting its significant contributions to social and environmental sustainability.",
    "details": "Inspiration\n\"Americans waste about 60 million tons of food every year.\"\n\nFood insecurity and food waste are major challenges in our communities. As we saw local bakeries and food banks struggling with leftover food and limited resources, we were inspired to create a solution that not only reduces waste but also bridges the gap between surplus food and those in need. The idea was born from the desire to make sustainable food distribution accessible—even for those without smart devices—by leveraging a simple, voice-first interface.\n\nWhat it does\nTeamFoodTactics is an AI-powered sustainable food distribution platform that connects suppliers (like gluten-free bakeries with surplus bread) with users who need specialized diets (such as individuals needing gluten-free options). Users simply call a dedicated phone number to describe their dietary requirements or inventory details, and our system:\n\n- **For Suppliers:** Facilitates the donation of leftover food by allowing bakery owners to update available food through voice calls or image uploads.\n- **For Users:** Provides personalized recommendations for the nearest food source that meets their specific dietary needs.\n- **Behind the Scenes:** Uses advanced AI to process voice and image data, generate dynamic SQL queries, and match supply with demand in real-time.\n\nHow we built it\n- **Frontend:** Next.js, Tailwind CSS\n- **Backend:** Python, FastAPI\n- **APIs:** Retell AI, OpenAI, OpenAI Swarm\n- **Database:** Supabase\n\nChallenges we ran into\n- Voice Interface Integration: Ensuring a natural, seamless voice interaction over any phone, particularly for users without access to smartphones.\n- Dynamic Query Generation: Training AI models to generate accurate SQL queries based on variable user inputs.\n- Real-Time Data Processing: Managing and updating live inventory data from multiple suppliers, including OCR extraction from uploaded images.\n- Scalability: Building a system secure and scalable enough to handle unpredictable donation and distribution volumes.\n\nAccomplishments that we're proud of\n- Inclusive Design: Built a voice-first interface accessible to anyone with a phone.\n- AI-Driven Matching: Developed real-time AI-powered query generation to connect surplus food with those in need.\n- Seamless Multi-Component Integration: Combined telephony, OCR, geolocation, and database management into a unified platform.\n- Sustainability Impact: Reduced food waste while improving equitable food access.\n- **Winner of the Sustainability – Green City Award at SpartaHack X.**\n\nWhat we learned\n- How to integrate conversational AI, OCR, and databases in real-world contexts.\n- Agile problem solving for dynamic queries and live data streams.\n- Building secure, scalable systems with Supabase and API orchestration.\n\nWhat's next for TeamFoodTactics\n- **Expanded Language Support:** Add multilingual access to serve diverse communities.\n- **Mobile App Integration:** Complement voice-first access with an app for enhanced functionality.\n- **Predictive Analytics:** Use ML to forecast demand and optimize logistics.\n- **Enhanced Personalization:** Introduce dashboards, profiles, and sustainability metrics.\n- **Partnership Expansion:** Scale impact through collaboration with food banks, nonprofits, and local governments.",
    "github": "https://github.com/aurelisajuan/spartaHacks",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/249/428/datas/gallery.jpg"
  },
  {
    "id": "talktuahduck",
    "name": "TalkTuahDuck",
    "summary": "TalkTuahDuck is an innovative AI-powered study tool designed to streamline the process of reviewing unstructured materials such as handwritten notes, diagrams, PDFs, and images. By encouraging users to \"talk\" through their study materials, the platform leverages the power of active learning to boost retention by up to 30%. Users can upload their documents, which are then parsed and organized for easy querying through a conversational AI interface. This allows for targeted, context-aware answers and the optional generation of animations to visualize complex concepts.\n\nThe project utilizes a robust tech stack, including the Sycamore parsing library for content extraction, SingleStore for high-speed vector data retrieval, and a Next.js frontend for an interactive user experience. It integrates a retrieval-augmented generation (RAG) pipeline to provide real-time, context-rich explanations. Recognized for its innovation at SB Hacks XI, TalkTuahDuck exemplifies how AI-driven educational tools can transform study habits and enhance comprehension.",
    "details": "Inspiration\nAccessing study materials can be a messy process—especially when dealing with handwritten notes, diagrams, PDFs, and image-based documents. Inspired by “RubberDuckyProgramming,” TalkTuahDuck encourages learners to “talk” through their study materials in a conversational setting. Research shows that actively explaining concepts can boost retention by up to 30% compared to passive study methods—TalkTuahDuck taps into this principle to supercharge understanding.\n\nWhat It Does\nTalkTuahDuck is an interactive study and explanation tool. Users can:\n- Upload PDFs, images, or messy whiteboard snapshots.\n- Parse & organize these documents into structured embeddings using the Sycamore parsing library.\n- Query the content through a conversational AI interface.\n- Retrieve targeted segments from SingleStore for ultra-fast, context-aware answers.\n- Optionally generate animations to visualize tricky concepts.\n\nArchitecture\n- **Data Ingestion:** Sycamore processes unstructured content via OCR and structured extraction.\n- **Data Storage:** SingleStore serves as a vector database for lightning-fast retrieval.\n- **Retrieval-Augmented Generation (RAG):** Real-time Q&A sessions reference exact document chunks.\n- **Frontend:** Next.js conversational interface with transcripts and dynamic source displays.\n\nKey Features\n- Advanced parsing for multi-format notes.\n- High-speed vector retrieval with SingleStore.\n- Conversational RAG for context-rich explanations.\n- Live transcripts of interactions.\n- Optional AI-generated animations for deeper clarity.\n\nAccomplishments\n- Built a complete RAG-powered study companion from scratch.\n- Integrated Sycamore, SingleStore, Retell, and Next.js into a cohesive system.\n- Delivered optional AI animation support for complex concepts.\n- Created accessible study workflows for students across diverse materials.\n- **Recognized at SB Hacks XI for innovation in AI-driven learning.**\n\nChallenges\n- Achieving reliable OCR parsing across varied input formats.\n- Training AI to generate dynamic SQL queries for flexible retrieval.\n- Balancing conversational flow with accurate sourcing.\n\nWhat We Learned\n- How to combine parsing, embeddings, and RAG for real-world study use cases.\n- The importance of building interfaces that reduce friction for overwhelmed students.\n- Best practices for interactive educational AI tools.\n\nWhat’s Next\n- **Better LLM Integrations:** More nuanced explanations for technical content.\n- **Enhanced Animations:** Richer instructional visuals.\n- **More Teaching Tools:** Expansion beyond Q&A into broader tutoring features.\n\nHook\n“Talk through your notes, and watch them explain themselves back.” TalkTuahDuck turns messy study material into an intelligent tutor that adapts to your needs.",
    "github": "https://github.com/AureliaSindhu/sbhacks",
    "demo": "https://youtu.be/hlBz5ejdrUc"
  },
  {
    "id": "splatnft",
    "name": "SplatNFT",
    "summary": "SplatNFT is an innovative platform that transforms 2D videos into interactive 3D art using Gaussian Splatting, offering a unique blend of creativity and technology. Users can seamlessly convert personal videos into dynamic digital assets, which can then be minted as NFTs with just a few clicks. The platform also provides a personal NFT gallery, allowing users to view, manage, and trade their collections securely. By combining elements of Web3, machine learning, and art, SplatNFT redefines how personal moments can be captured and owned in the digital realm.\n\nThe platform is built with a robust tech stack, featuring Next.js for the frontend, Node.js for backend processing, and leveraging the Solana blockchain alongside the Anyone Protocol for decentralized minting and storage. This integration ensures a secure and efficient user experience from video upload to NFT management. SplatNFT's innovative approach was recognized at SoCal Tech Week 2024, where it won the SolanaU Sponsor Challenge, highlighting its successful merge of digital art and blockchain technology.",
    "details": "Inspiration\nWe're a team of artists and programmers passionate about exploring how art can intersect with Web3 and machine learning. SplatNFT was created as a way to turn personal moments into interactive pieces of art, redefining how creativity and technology intersect.\n\nWhat It Does\n- **Gaussian Splat Conversion:** Converts uploaded videos into Gaussian Splat representations, offering visually dynamic and interactive 3D art.\n- **Seamless Minting Process:** Allows users to mint their Gaussian Splat as an NFT with just a few clicks.\n- **Personal NFT Gallery:** Lets users view, browse, and manage their NFT collection.\n- **Secure Protocol:** Built with the Anyone Protocol for secure, decentralized NFT storage and minting.\n\nHow We Built It\n- **Frontend:** Next.js for responsive, dynamic UI.\n- **Backend:** Node.js for efficient data handling and video processing.\n- **Blockchain:** Solana and Anyone Protocol for decentralized minting and NFT storage.\n\nChallenges We Ran Into\n- Integrating blockchain functionality on the Solana network.\n- Managing secure API workflows for NFT minting and video conversion.\n\nAccomplishments We're Proud Of\n- Built an end-to-end platform where users can upload, convert, preview, and mint NFTs in one flow.\n- Combined Gaussian Splatting with blockchain to create a novel digital art medium.\n- Delivered an intuitive gallery experience for managing personal NFT collections.\n- **Winner of the SolanaU Sponsor Challenge at SoCal Tech Week 2024.**\n\nWhat We Learned\n- Gained hands-on experience in video processing and blockchain integration.\n- Improved backend process management and NFT minting workflows.\n- Learned how to design responsive, user-friendly interfaces for complex AI + blockchain systems.\n\nWhat's Next for SplatNFT\n- Support additional file formats for uploads.\n- Enhance scalability and optimization of Gaussian Splatting workflows.\n- Add customization options for 3D art rendering.\n- Partner with digital art communities to introduce galleries, tutorials, and collaborative showcases.",
    "github": "https://github.com/KevinWu098/SplatNFT",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/130/048/datas/gallery.jpg"
  },
  {
    "id": "magicloops",
    "name": "InstaRizz",
    "summary": "InstaRizz is an innovative wearable technology project that enhances social interaction by combining AI and real-time personalization. Utilizing Ray-Ban smart glasses, it streams live video to Instagram Live, captures facial snapshots with OpenCV, and matches identities using a custom-trained classifier. The system then generates personalized bios and pickup lines, showcasing the potential of AI to augment human charm while highlighting important privacy implications. InstaRizz aims to spark discussions around the ethical use of AI in enhancing live experiences.\n\nThe technology stack behind InstaRizz includes Ray-Ban smart glasses for live streaming, OpenCV for facial recognition, and a custom classifier for identity matching. The integration of Magic Loops and Claude allows for the generation of personalized content in real time. Recognized at the AI ATL 2024 event, the project stands out for its technical sophistication and its role in prompting discourse on the ethics of wearable AI.",
    "details": "Inspiration / Problem:\nTraditional wearable tech rarely blends entertainment, AI, and real-time social interaction. InstaRizz was created to explore how AI could augment human charm and live interactions while raising awareness of privacy implications.\n\nWhat It Does:\n- **Live Streaming:** Streams video from Ray-Ban smart glasses directly to Instagram Live.\n- **Facial Recognition:** Captures and processes snapshots using OpenCV.\n- **Identity Matching:** Uses a custom-trained classifier to identify consenting individuals.\n- **AI Personalization:** Runs a Magic Loops + perplexity search pipeline to retrieve public data, then generates a short bio and three pickup lines using Claude.\n\nHow It Works:\n1. Ray-Ban glasses stream the live feed.\n2. OpenCV captures and processes snapshots in real time.\n3. A custom classifier matches identities from a database of consenting participants.\n4. Magic Loops and Claude generate personalized bios and pickup lines displayed instantly.\n\nChallenges:\n- **Real-Time Synchronization:** Aligning live video streams with facial recognition was difficult.\n- **Classifier Accuracy:** Required fine-tuning to ensure identity recognition worked reliably.\n- **Privacy Concerns:** Designing safeguards and ethical considerations around facial recognition in public.\n\nAccomplishments:\n- Built a fully functioning, end-to-end system combining computer vision, LLMs, and real-time streaming.\n- Successfully generated bios and personalized pickup lines in seconds.\n- Proved the potential of integrating wearable AI into live social experiences.\n- **Showcased at AI ATL 2024, sparking discussions around the ethics of wearable AI.**\n\nWhat We Learned:\n- Optimizing real-time facial recognition pipelines.\n- Balancing fun, technical innovation, and ethics in AI design.\n- First-hand lessons in privacy risks associated with wearable AI.\n\nWhat’s Next:\n- Publish methods and best practices to protect individuals from unwanted scanning.\n- Release public guidelines on ethical use of wearable AI.\n- Explore broader social applications of real-time personalization technology.\n\nHook:\n\"AI meets charm.\" InstaRizz is a playful yet thought-provoking prototype that shows how AI can make live interactions more dynamic while prompting deeper discussions about ethics in the age of smart glasses.",
    "github": "https://github.com/coderkai03/AI-ATL-2024",
    "demo": "https://youtu.be/xjMcK34BBu4"
  },
  {
    "id": "maybe-zc19va",
    "name": "SoundSearch",
    "summary": "SoundSearch is an innovative tool designed to enhance web accessibility by transforming website navigation into a guided voice experience. Users simply place a phone call to receive real-time, step-by-step voice instructions that are synchronized with the website they are browsing. This service is particularly beneficial for individuals with visual impairments or those who are not fluent in the website's primary language, as it guides them through tasks such as filling out forms, applying filters, and exploring complex interfaces.\n\nThe project leverages a robust tech stack, including AWS for scalable backend infrastructure, NLX.ai for natural language processing, and Next.js for a seamless frontend experience. It integrates with platforms like Google Flights to demonstrate its ability to navigate complex websites with ease. SoundSearch's innovative approach to accessibility was recognized at AI ATL 2024, where it won the prestigious NLX Overall Award, highlighting its impact and potential in making the digital world more inclusive.",
    "details": "Inspiration\nNavigating new websites can be a daunting task. Platforms like Amazon and Google Flights, while powerful, often overwhelm users with countless filters and options. This complexity poses a significant hurdle, especially for individuals with visual impairments or those who are not fluent in the website's primary language. SoundSearch was created to bridge this accessibility gap, making the digital world more inclusive and user-friendly for everyone.\n\nWhat it does\nSoundSearch transforms the way users interact with websites by providing real-time, step-by-step voice guidance over a simple phone call. When a user dials the number, SoundSearch synchronizes with the website being used, highlighting specific sections and guiding them through each step of the process. It tells users how to fill out forms, adjust filters, and navigate complex interfaces. This makes online platforms more accessible, especially for those who have difficulty seeing or are not proficient in the website's language.\n\nHow it was built\nSeveral cutting-edge technologies were combined to bring SoundSearch to life:\n\nAWS (Amazon Web Services): Provided backend infrastructure, ensuring scalability and reliability of the service.\nNLX.ai: Powered the natural language processing, enabling voice interaction, understanding inputs, and delivering accurate, context-aware guidance.\nGoogle Flights Search: Integrated as a demonstration of navigating complex websites with numerous filters and options.\nNext.js: Used for the frontend, providing a seamless and responsive interface that works in tandem with the voice guidance system.\n\nChallenges encountered\nThe development process involved several obstacles:\n\nSponsor Platform Integration: Integrating with the sponsor's platform presented hurdles due to unfamiliar APIs and limited documentation.\nAWS Complexity: Configuring AWS services was challenging, requiring significant effort to set up infrastructure, manage permissions, and deploy the application.\n\nAccomplishments\n- Mastered AWS deployment despite it being the team’s first time working with the platform.\n- Delivered a functional prototype demonstrating real-time, voice-guided web navigation.\n- **Winner of the NLX Overall Award at AI ATL 2024**, validating the project’s impact on accessibility and innovation.\n\nKey learnings\nAWS Proficiency: Gained extensive knowledge about AWS services, deployment processes, debugging, and log management.\nIntegration Skills: Developed effective strategies for combining multiple technologies (AWS, NLX.ai, Google Flights API, Next.js) into a cohesive system.\nProblem-Solving: Enhanced ability to troubleshoot issues that arise during development, especially with unfamiliar platforms.\n\nWhat's next for SoundSearch\nUser Input Integration: Enabling users to input information via voice commands to make interaction even more seamless.\nExpanding Website Support: Extending compatibility to more websites, ensuring broader accessibility.\nPersonalization: Adding user profiles to tailor guidance based on individual preferences and needs.\nMultilingual Support: Introducing support for multiple languages to assist non-native speakers more effectively.",
    "github": "https://github.com/IdkwhatImD0ing/AIATL",
    "demo": "https://youtu.be/RgH-i9SYj-o"
  },
  {
    "id": "skysearch-t5ci1v",
    "name": "SkySearch",
    "summary": "SkySearch is an advanced search-and-rescue platform designed to streamline emergency response operations using fleets of autonomous drones. It enables operators to manage drone deployments, reconstruct disaster-affected environments, and extract critical insights from expansive data sets in real-time. By integrating video feeds, telemetry, and environmental data into a single interface, SkySearch empowers operators to swiftly identify hazards, locate individuals, and devise optimized, risk-aware rescue routes. The platform's capabilities include semantic search, 3D reconstruction via Gaussian splatting, and autonomous drone swarming for large-scale missions.\n\nThe project was built using a sophisticated tech stack that includes a drone SDK for live video streaming, TP-Link antennas for robust data transmission, OpenCV, and Apple Depth Pro for data processing and classification, alongside SingleStore for real-time database management. This integration facilitates seamless communication between software and hardware components, overcoming challenges such as low-battery drones and interface integration. SkySearch was showcased at **Cal Hacks 11.0**, highlighting its innovative use of computer vision, AI, and drone technology to revolutionize emergency response and expedite life-saving decisions.",
    "details": "Inspiration\nHurricane Milton, the most devastating disaster in over 30 years, left more than 3 million people without power and overwhelmed emergency services scrambling to respond. While drone technology now floods us with vast amounts of data, the real challenge lies in making sense of it in rapidly evolving environments—operators are still stuck manually sifting through critical information when time is running out. In moments of chaos, the ability to scale autonomous search-and-rescue missions and intelligently uncover patterns from data becomes essential.\n\nSkySearch enables operators to uncover hidden insights on the environment in vast seas of data by integrating real-time data on the environment, telemetry, and previous missions into a single pane of glass (software mission control system). Operators can deploy fleets of drones to investigate regions and collect video feed used to reconstruct the scene, enabling them to drill-down on areas of interest through a semantic search engine.\n\nWhat it does\nOur goal is to enable operators to interact with data and uncover hidden patterns effortlessly.\n\nSkySearch is built around the end-to-end search-and-rescue workflow in the following use cases:\n\nSearch: Drones are deployed through the software by operators and autonomously navigate through terrain to identify objects of interest in real-time.\nRescue: Operators can interact with live data to isolate hazards and locate people through a unified search interface. Based on this data, the system then recommends risk-aware, optimized rescue routes for first responders.\n\nCore features\n- Environment reconstruction of damaged regions and infrastructure with Gaussian splatting\n- Risk-aware pathfinding for rescue operations\n- Semantic search through disparate data sources to uncover patterns and recommend actions\n\nHow we built it\nWe designed an embedded architecture that enables software and hardware interfaces to bidirectionally communicate information and commands.\n- Drone SDK for live video streaming\n- TP-Link antennas for a local WiFi system to create a more robust data pipeline between the drone and the software interface\n- OpenCV and Apple Depth Pro to process footage and classify data\n- SingleStore for real-time database management\n\nChallenges we ran into\n- Accounting for low-battery drones\n- Integration between hardware and software interfaces\n- Balancing human judgment with autonomy\n\nAccomplishments that we're proud of\n- Implemented an autonomous swarming framework for large-scale missions\n- Integrated Gaussian splatting for realistic 3D reconstruction\n- Developed risk-aware map traversal and recommended \"safe routes\" for emergency responders\n- Built dynamic data generation for efficient testing, analysis, and responsiveness\n- **Submitted to Cal Hacks 11.0**, showcasing the platform’s ability to merge computer vision, AI, and drone swarms for emergency response\n",
    "github": "https://github.com/KevinWu098/calhacks24",
    "demo": "https://youtu.be/_AtPD2fjSvs"
  },
  {
    "id": "swarmaid",
    "name": "SwarmAid",
    "summary": "SwarmAid addresses the critical issue of global food waste by employing a decentralized AI system to connect suppliers with surplus food to food banks and shelters in real-time. The project uses intelligent agents to detect surplus, match it with demand, and plan efficient logistics while ensuring compliance with food safety standards. By automating the redistribution process, SwarmAid minimizes waste and helps feed communities in need. This innovative approach highlights the potential for technology to bridge the gap between excess and scarcity.\n\nThe project utilizes Swarm’s multi-agent orchestration framework, with a backend built in Python and an interactive mapping interface powered by Leaflet.js. Real-time APIs facilitate data synchronization and seamless agent collaboration. SwarmAid's proof of concept was recognized at Hack Dearborn: Rewind Reality, where it won 2nd Place, showcasing its potential to leverage decentralized AI for tackling urgent social challenges.",
    "details": "Inspiration / Problem:\nRoughly one-third of all food produced globally is wasted while millions go hungry. SwarmAid was born out of the urgent need to bridge this gap by applying intelligent, decentralized coordination to redirect surplus food to communities in need.\n\nWhat It Does:\n- **Supply Agents:** Detect surplus food from suppliers like restaurants, grocery stores, and farms.\n- **Demand Agents:** Match available surplus with food banks and shelters.\n- **Logistics Agents:** Plan efficient transport routes, minimizing delays and environmental impact.\n- **Compliance Agents:** Ensure all food handling and transport meet safety standards.\n\nHow It Works:\nSwarmAid uses Swarm’s multi-agent orchestration framework to manage collaboration across agents. The backend is built with Python, while Leaflet.js provides an interactive mapping interface. Real-time APIs enable data synchronization, creating a smooth, automated redistribution process.\n\nChallenges:\n- Achieving seamless communication between agents without creating bottlenecks.\n- Coordinating real-time logistics and compliance while maintaining scalability.\n- Experimenting with architectures and APIs to balance efficiency and reliability.\n\nAccomplishments:\n- Built a working proof of concept showcasing decentralized coordination in action.\n- Demonstrated real-time supply-demand matching with efficient logistics planning.\n- Highlighted the potential for technology to significantly reduce food waste while feeding communities.\n- **Won 2nd Place at Hack Dearborn: Rewind Reality**, recognizing SwarmAid’s innovative approach to tackling food waste.\n\nWhat We Learned:\n- Practical implementation of Swarm’s multi-agent framework.\n- Techniques for optimizing logistics and handling real-time data.\n- Insights into applying technology to large-scale social challenges such as hunger and waste reduction.\n\nWhat’s Next:\n- Integrate with real-world food inventory systems for live surplus data.\n- Incorporate traffic and weather data for more accurate logistics planning.\n- Expand coverage to broader geographic regions.\n- Develop a mobile app for accessibility and use predictive machine learning to anticipate supply and demand patterns.\n\nHook:\n\"Turning surplus into sustenance.\" SwarmAid shows how decentralized AI can tackle food waste and hunger through intelligent collaboration.",
    "github": "https://github.com/IdkwhatImD0ing/SwarmAid",
    "demo": "https://youtu.be/Vk73UiZksvo"
  },
  {
    "id": "bikstar",
    "name": "BikstAR",
    "summary": "BikstAR is an innovative mixed-reality biking game designed to transform traditional cycling into an engaging, social, and competitive experience. By overlaying a bunny-themed virtual world onto real-world trails, bikers can collect carrots, acquire power-ups, and compete in 2v2 races. The game encourages outdoor activity by turning cycling into a shared adventure, complete with live voice chat, real-time position tracking, and dynamic challenges that enhance the biking experience. BikstAR aims to foster community connections while making exercise fun and interactive.\n\nThe project was developed in just 36 hours, leveraging cutting-edge technology to create an immersive environment. Oculus Quests provided the platform for mixed reality gameplay, while Unity powered the game engine and asset integration. Normcore enabled seamless multiplayer connectivity, and Blender was used for designing custom 3D models and environments. BikstAR was submitted to **DreamXR**, highlighting its potential to revolutionize outdoor activities with mixed reality.",
    "details": "Inspiration:\nAs natives of suburban Vermont and Irvine, we were all too familiar with the exciting nature of biking. Yet, despite biking being one of the most popular active sports with over 10,000 bikers per square mile, the repetitiveness of daily routes gets boring. What if we gamified biking into a mixed-reality experience that transforms mundane daily hassles into magical experiences?\n\nWhat It Does:\nbikstAR is an active AR game where bikers can bike IRL on a live path with an extra competitive kick: collecting carrots (coin system) and power-ups to compete with another player. Our goal is to transform a typically individual venture into a social sport connecting communities through a fun, immersive game.\n\nAs you pedal through your favorite trails, bikstAR overlays a captivating, bunny-themed virtual universe onto your surroundings. Your goal is to collect as many carrots as possible within a set time limit while navigating your actual bike trail, now enhanced with power-ups and challenges.\n\nCore Game Loop:\n- Collect carrots (reward system) on a user-generated path\n- Unpredictability through randomization of power-ups\n- Outpacing rivals in 2v2 competitive biking\n\nKey Features:\n- Real-time teammate position tracking\n- Live voice chat for communication\n- Dynamic updates on virtual objects and power-ups\n\nWhether you're stealing carrots from your opponents, launching projectiles, or racing to collect the most points, bikstAR offers a unique and entertaining experience that motivates outdoor activity.\n\nHow We Built It:\nIn just 36 sleepless hours, our team integrated cutting-edge technologies into a live, immersive environment:\n- **Oculus Quests:** Enabled immersive, mixed reality gameplay\n- **Normcore:** Facilitated seamless multiplayer connectivity\n- **Unity:** Powered the game engine and asset integration\n- **Blender:** Created custom 3D models and environments\n\nThe Team:\n- **Dieter:** 3D modeling, multiplayer implementation, and pitching\n- **Bill:** Backend development and real-time data synchronization\n- **Thomas:** Frontend design and user experience\n- **Jasmine:** Spatial design and bunny-themed game aesthetic",
    "github": "https://github.com/SerenityUX/dream",
    "demo": "https://youtu.be/Mcirws6Bh4A"
  },
  {
    "id": "linguify-katunw",
    "name": "Linguify",
    "summary": "Linguify is an innovative language learning platform designed to transform everyday environments into immersive educational experiences. By leveraging contextual image recognition, it generates relevant vocabulary and phrases from users' surroundings, while AI-powered conversations offer adaptive dialogues to match users' proficiency levels. The platform provides immediate feedback on pronunciation and comprehension, and integrates cultural insights to enhance real-world communication skills. Linguify aims to bridge the gap between theoretical study and authentic conversation, motivating learners through gamified elements and personalized learning paths.\n\nThe project utilizes advanced technologies such as AI for adaptive learning and image recognition to create an engaging user experience. It was developed using a robust tech stack that supports its interactive and dynamic features. Linguify was recognized for its groundbreaking approach at **VTHacks 12**, where it won the **Apex Center – Best Startup Hack** award. This accolade underscores its potential to revolutionize language learning by offering a holistic and practical solution for learners worldwide.",
    "details": "Linguify: Revolutionizing Language Learning Through Real-World Interaction\n\nThe Problem:\nTraditional language learning apps are repetitive and fail to prepare users for real-life conversations. A Rosetta Stone survey found that 73% of language learners feel anxious about using their new skills in daily life, and users of popular apps often lack listening proficiency and speaking practice.\n\nOur Solution:\nLinguify transforms your environment into an immersive language learning experience through:\n- **Contextual Learning with Image Recognition:** Snap a photo and generate relevant vocabulary and phrases from your surroundings.\n- **AI-Powered Conversations:** Practice dialogues that adapt to your level and progress.\n- **Immediate Feedback:** Get instant guidance on pronunciation and comprehension.\n- **Cultural Integration:** Learn social norms and cultural nuances to build deeper connections.\n\nValue Proposition:\n- Bridges the gap to real conversations by simulating everyday scenarios.\n- Personalized, adaptive learning keeps users engaged and motivated.\n- Gamified elements like streaks and leaderboards encourage consistency.\n- Covers speaking, listening, reading, and writing for holistic mastery.\n\nMarket Opportunity:\nWith over 500 million global language learners and widespread dissatisfaction with existing apps, there’s a strong demand for more effective, real-world solutions.\n\nWhy Linguify:\nBy turning the world into your classroom, Linguify offers a unique, immersive approach that builds fluency, confidence, and cultural awareness beyond what traditional apps provide.",
    "github": "https://github.com/IdkwhatImD0ing/Linguify",
    "demo": "https://youtu.be/1JRYNXzAk-A"
  },
  {
    "id": "safety-blanket-vyp089",
    "name": "Safety Blanket",
    "summary": " \nSafety Blanket is a mobile application designed to provide peace of mind for individuals walking alone, especially women who often experience heightened anxiety in such situations. It features an AI companion that offers real-time conversational support and reassurance, automated check-ins that escalate to location sharing with trusted contacts if necessary, and a safety timer that alerts emergency contacts with your location if not deactivated. This virtual lifeline empowers users by combining emotional security with practical safety measures.  \n\nThe application was built using a robust tech stack, including FastAPI, WebSockets, Firebase, OpenAI API, Retell AI, Langchain, Twilio, and Google Maps API for the backend, and Next.js with TailwindCSS for the frontend. Designed and developed by a team participating in **VenusHacks 2024**, Safety Blanket demonstrates the potential of AI in enhancing personal safety and providing emotional support. The project successfully overcame integration challenges, showcasing the team's ability to collaborate effectively and create a cohesive product.",
    "details": "Inspiration:\nImagine feeling a knot of anxiety every time you walk alone after dark, even in your own neighborhood. One in two women feel unsafe walking alone after dark in a quiet street near their home, compared to just one in seven men. This fear is even greater in busier areas—in parks or open spaces, the figure jumps to four out of five.\n\nThe statistics are even more alarming for younger women: two out of three women aged 16 to 34 have experienced harassment in the past year. Women often rely on friends or family for check-ins, but what happens when no one is available? Safety Blanket was created to be a lifeline — a virtual AI companion offering emotional security and reliable safety features.\n\nWhat It Does:\n- **AI Companion:** Real-time phone calls with conversational AI for reassurance.\n- **Check-ins:** AI text chat that escalates to location sharing with trusted contacts if you don’t respond.\n- **Safety Timer:** Alerts emergency contacts with your location if not deactivated with a password.\n\nHow We Built It:\n- **Backend:** FastAPI, WebSockets, Firebase, OpenAI API, Retell AI, Langchain, Twilio, Google Maps API.\n- **Frontend:** Next.js, TailwindCSS, Figma.\n\nChallenges:\n- Integrating individually built parts into one unified product.\n- Balancing varying skill sets in a team working together for the first time.\n\nAccomplishments:\n- Built a cohesive app with AI companionship, real-time location sharing, and emergency protocols.\n- Overcame integration and design challenges as a new team.\n\nWhat’s Next:\n- Integrating with wearables.\n- Enhancing AI personalization.\n- Expanding international emergency services.\n\nSafety Blanket continues to evolve, providing confidence and peace of mind to users worldwide.",
    "github": "https://github.com/kllarena07/safety-blanket",
    "demo": "https://youtu.be/E8bzwcbllKY"
  },
  {
    "id": "abseas",
    "name": "ABSeas",
    "summary": "ABSeas is a web application designed to make preschool education accessible and engaging through the power of music and interactive visuals. It covers fundamental topics such as counting, letters, sharing, and manners, transforming them into memorable learning experiences with catchy songs and vibrant images. The app focuses on creating an intuitive and enjoyable interface for children, promoting both cognitive development and social skills through progressive learning modules.\n\nThe project utilizes a modern web development stack, supported by an AI-powered backend featuring Anthropic Claude for text generation, Suno AI for music creation, OpenAI DALL·E 3 for image generation, Eleven Labs for voice cloning and text-to-speech, and OpenAI Whisper for speech recognition. Real-time data management is handled through Firebase. ABSeas was submitted to DiamondHacks 2024, where it received recognition by winning Track 4: Captain’s Classroom, highlighting its innovative approach to early childhood education.",
    "details": "Inspiration:\nSchooling is expensive. The average private preschool tuition in California is $12,967 per year—a cost not every parent can afford. Preschool sets up children for long-term academic success, but access is limited. Music, however, enhances recall and brain development. This inspired us to create a way to teach preschool knowledge through song and dance.\n\nWhat It Does:\nABSeas is a web app that teaches preschool topics such as counting, letters, sharing, and manners through music and visuals. Kids can sing along while engaging with colorful pictures that reinforce learning. Our approach focuses on:\n- **Content Synchronization:** Blending preschool concepts with catchy songs and visuals.\n- **Intuitive Interaction:** Bold visuals and interactive elements designed for children.\n- **Progressive Learning:** Starting with basics like ABCs and advancing to social skills as children grow.\n\nDesign Process:\nABSeas emphasizes simplicity, bold colors, and large interactive elements. Content flows smoothly so children and parents can navigate easily. Each module feels natural and fun, creating a tailored and educational experience.\n\nHow We Built It:\nFrontend: Modern web development stack.\nBackend: AI-powered pipeline integrating:\n- Anthropic Claude (text generation)\n- Suno AI (AI-generated music)\n- OpenAI DALL·E 3 (images)\n- Eleven Labs (voice cloning & TTS)\n- OpenAI Whisper (speech recognition)\n- Firebase (real-time database)\n\nChallenges:\nSynchronizing Firebase with multiple APIs was a key challenge, requiring careful integration of interactivity and stability.\n\nAccomplishments:\nWe infused generative AI into children’s learning, combining visuals and music in a fun and educational way.\n\nWhat We Learned:\nWe improved skills in multiprocessing, audio processing, frontend design, and user experience refinement, while building a professional and intuitive app.",
    "github": "https://github.com/IdkwhatImD0ing/ABSeas",
    "demo": "https://youtu.be/YeeddNGpoN8"
  },
  {
    "id": "tidbits-n9mrxa",
    "name": "Tidbits",
    "summary": "Tidbits is an innovative platform designed to transform lengthy university lectures into engaging, short-form videos. By leveraging storytelling and a TikTok-style interface, it converts both audio and visual lecture content into 1–2 minute summaries, enhancing retention and engagement. The platform offers real-time translation and personalized learning experiences, adapting content to each student's preferences. This approach addresses the inefficiencies of traditional note-taking and memorization, making education more accessible and enjoyable.\n\nThe project is built using a robust tech stack that includes Anthropic Claude, Suno AI, Eleven Labs, D-iD, Whisper, and DALL·E. The development pipeline utilizes FastAPI, Next.js, and Supabase, with advanced concurrency techniques to optimize video creation time from 30 minutes to just 5. Tidbits was showcased at **YHACK 2024**, where it earned **1st Place**, highlighting its impact and innovation in the field of educational technology.",
    "details": "Inspiration:\nUniversity students spend over 3000 hours in lectures, yet 80% of lecture content is often forgotten. Traditional note-taking and memorization are inefficient. Tidbits leverages storytelling and short-form video to transform lectures into engaging, bite-sized educational content.\n\nWhat It Does:\nTidbits converts lectures (audio or visual) into short, scrollable videos with one-click upload. It mimics TikTok’s familiar interface, making learning fast, engaging, and accessible. Key features include:\n- Story-based learning: 1–2 minute summaries of 1–2 hour lectures.\n- International translation: complex English material translated into any language.\n- Personalized flows: AI adapts content for each student.\n\nDesign Process:\nFocused on accessibility and engagement:\n- Bold branding and easy navigation for mobile.\n- Automated personalization—users only choose topics, AI handles the rest.\n\nHow We Built It:\nTech stack integrates Anthropic Claude, Suno AI, Eleven Labs, D-iD, Whisper, and DALL·E. The pipeline uses FastAPI + Next.js, Supabase, and advanced concurrency techniques (asyncio, aiohttp, threading). Optimizations reduced reel creation time from 30 minutes to 5.\n\nChallenges:\nSynchronizing multiple AI outputs (captions, audio, images, timestamps) was complex. Reverse-engineering Suno AI, frontend video auto-play limitations, and managing state/UI updates were key hurdles.\n\nAccomplishments:\nSuccessfully produced synchronized educational reels. Optimized processing for a 7.5x speedup. Built a functional prototype within hackathon time constraints.\n\nWhat We Learned:\n- Async multithreading for performance optimization\n- Audio/video editing with Python\n- Supabase schema design and flexibility\n- Importance of planned frontend state management\n\nWhat’s Next:\nMore customization options for generated tidbits, including screenshots from lectures and supplementary web images.",
    "github": "https://github.com/dylanvu/Tidbits",
    "demo": "https://youtu.be/6EjKhcIuaYo"
  },
  {
    "id": "gemui",
    "name": "GemUI",
    "summary": " \nGemUI is an innovative AI-powered platform designed to simplify online navigation by generating intuitive, minimalistic user interfaces. It is particularly beneficial for elderly users, children, and individuals with accessibility needs, as it extracts and displays only the essential UI elements such as buttons, forms, and text. By interpreting user requests, GemUI transforms complex websites into user-friendly experiences, making the internet more accessible for everyone. This project was inspired by the challenge faced by one of the team member's grandmothers, a breast cancer survivor, who struggled with online tasks.\n\nThe project leverages a robust tech stack that includes Gemini 1.5 for contextual reasoning, Whisper for transcription, and Selenium for browser control, with a frontend built using Next.js, TypeScript, and Tailwind. Real-time interaction is facilitated through WebSockets, enabling seamless chatbot-browser communication. Hosted on Vercel, GemUI was showcased at the Google AI Hackathon and the Google x MHacks AI Hackathon, highlighting its potential to revolutionize web accessibility.",
    "details": "Inspiration:\nGemUI was inspired by Dylan’s grandmother, a breast cancer survivor who struggles with online navigation for tasks like booking labs. The team wanted to simplify the internet for users like her. Inspired by Vercel’s generative UI demo at Google Next 2024, they built a dynamic solution that generates simplified UI directly from real websites.\n\nWhat It Does:\nGemUI interprets user requests and generates only the relevant UI elements—buttons, forms, and text—making the web accessible to elderly users, children, and anyone needing a simplified interface. It acts as a universal, stripped-down UI layer for any website.\n\nHow We Built It:\nThe system integrates Gemini 1.5 for large-context reasoning, Whisper for transcription, Selenium for live browser control, and real-time WebSockets for chatbot-browser interaction. Frontend: Next.js, TypeScript, ShadCN, Tailwind, and Vercel AI SDK. Backend: FastAPI, Python SDK, Gemini 1.5, and Selenium.\n\nCommunity Impact:\nGemUI empowers elderly users, children, and people with accessibility needs by simplifying website navigation. It also allows personalization of fonts, display size, and interaction methods to fit unique user requirements.\n\nWhat’s Next:\nThe team plans to incorporate direct voice interaction (bypassing transcription) so users who can’t type or read can navigate the internet through speech alone.",
    "github": "https://github.com/KevinWu098/gemUI",
    "demo": "https://youtu.be/27MgQSSHuuQ"
  },
  {
    "id": "doggo-ai",
    "name": "Doggo AI",
    "summary": "Doggo AI is an interactive plush companion designed to support hospitalized children during crucial developmental stages. It offers responsive AI conversations, real-time emotion detection, and a caretaker dashboard to provide comfort and connection. By integrating the familiar presence of a stuffed animal with advanced AI, Doggo AI fosters emotional well-being and facilitates learning, creating a supportive environment for children in long-term care. The plush toy not only entertains but also empathetically responds to children's needs, ensuring an inclusive and engaging experience.\n\nThe project utilizes a sophisticated tech stack, featuring multi-threading, multiprocessing, and asyncio for seamless conversation flow. Emotion detection is achieved through the Hume EVI API, while GPT-4 powers the conversational capabilities. Real-time communication is facilitated by a FastAPI backend with WebSockets, and the dashboard is developed using Next.js, TypeScript, ShadCN, Tailwind, and Figma. Doggo AI was submitted to HackDavis 2024, where it earned the distinction of Best Use of Intel® Developer Cloud, highlighting its innovative approach and impactful design.",
    "details": "Inspiration:\nOver 5 million children are hospitalized in the US every year, often without caretakers during critical developmental periods. Many toddlers bond with stuffed animals, which aid healthy attachment. Doggo AI was created to combine that comfort with an AI companion to support hospitalized children.\n\nWhat It Does:\nDoggo AI is an interactive plush companion that children can talk to. It tells stories, teaches, and responds naturally. Key functions include:\n1. Responsive AI conversations that adapt to interruptions, questions, and statements.\n2. Real-time emotion detection that allows the plush to respond empathetically.\n3. A caretaker dashboard with live transcripts and emotion reports.\n\nAll of this is built into a soft stuffed toy designed for hugging and play, encouraging intuitive and inclusive engagement.\n\nDesign Process:\nThe team created paper mockups, conducted user research with patients and caretakers, and developed personas to guide design. They designed low- and high-fidelity prototypes, prioritizing accessibility with WCAG-compliant colors, high-contrast text, and natural voice interaction. The plush encourages intuitive, inclusive engagement.\n\nHow We Built It:\nDoggo AI combines multi-threading, multiprocessing, and asyncio for natural conversation flow. Emotion detection is powered by the Hume EVI API, conversations by GPT-4, and real-time communication via WebSockets with a FastAPI backend. The dashboard is built with Next.js, TypeScript, ShadCN, Tailwind, and Figma. The plush contains a webcam, microphone, speaker, Raspberry Pi, and power bank, all sewn into a custom toy.\n\nImpact:\nDoggo AI bridges emotional and educational gaps for children in long-term care, offering companionship, comfort, and a sense of connection in otherwise isolating environments.",
    "github": "https://github.com/IdkwhatImD0ing/DoggoAI",
    "demo": "https://youtu.be/RAe3fLyPIa0"
  },
  {
    "id": "mad-lyrics",
    "name": "Mad Lyrics",
    "summary": "Mad Lyrics is an innovative multiplayer party game that combines creativity and technology to deliver a unique musical experience. Drawing inspiration from the classic game Mad Libs, it allows players to collaboratively fill in blanks to generate quirky and humorous song lyrics. These lyrics are brought to life through AI-generated music, enhanced by immersive 3D visualizations, creating an engaging and interactive environment where laughter and creativity flourish.\n\nThe project leverages a robust tech stack, including WebSockets for managing multiplayer state, OpenAI for text generation, Suno for AI music, and Three.js for dynamic 3D visualizations. Built using Next.js and React for the web client and Python with FastAPI for the backend, Mad Lyrics showcases advanced integration of cutting-edge technologies. Recognition for its innovative design came at Uncommon Hacks 2024, where it won the award for Best in Track: Programmatic Art.",
    "details": "Inspiration:\nMadLyrics was born from the idea of blending music creation, multiplayer collaboration, and party dynamics. Inspired by Mad Libs, the team wanted to create a game that unites people through humor and creativity.\n\nWhat It Does:\nMadLyrics challenges users to laugh, create, and make beats together. Players collaboratively fill in blanks to generate lyrics, which are then brought to life with AI-generated music and immersive 3D visualizations.\n\nHow We Built It:\n- Multiplayer state management via WebSockets\n- AI pipeline using OpenAI for text generation and Suno for AI music\n- 3D music visualizations powered by Three.js\n- Web client built with Next.js and React\n- Backend built with Python and FastAPI\n\nChallenges:\nManaging multiplayer state across users proved to be the most difficult technical challenge.\n\nAccomplishments:\n- Reverse-engineered Suno’s custom AI music generation\n- Built a live 3D music visualizer\n- Survived Chicago’s cold weather (as Californians!)\n\nWhat We Learned:\n- Efficient techniques for handling multiplayer state\n- How to integrate cutting-edge AI models into a party game\n- CapitalOne discounts coffee by 50%\n- Five Guys gives free peanuts",
    "github": "https://github.com/dylanvu/Mad-Lyrics",
    "demo": "https://youtu.be/Yu8hFL5SS0g"
  },
  {
    "id": "pypointer",
    "name": "PyPointer",
    "summary": "PyPointer revolutionizes computer interaction by enabling users to control screens using only natural gestures and voice commands. By tracking the index finger to move the cursor and using simple voice commands for actions like clicking, it eliminates the need for traditional input devices like keyboards and mice. This innovative approach enhances accessibility and offers a futuristic way to interact with technology.\n\nThe project utilizes advanced technologies, including Meta’s SAM model for screen segmentation, OpenCV and MediaPipe for finger tracking, OpenAI Whisper for speech recognition, and PyAutoGUI for cursor control. PyPointer was presented at HackMerced IX, where it earned 2nd Place in the Spatial & Interactivity Track, highlighting its impact and potential.",
    "details": "Inspiration:\nPyPointer was inspired by a video showcasing a gaming setup without a keyboard or mouse. The team wanted to challenge conventional interaction methods by enabling hand and voice as the only inputs, pushing the boundaries of what’s possible under hackathon constraints.\n\nWhat It Does:\nPyPointer allows users to interact with any screen through natural gestures and speech. The index finger moves the cursor, while simple voice commands (e.g., 'click') perform actions like a left mouse click.\n\nHow We Built It:\n- Screen segmentation using Meta’s SAM model, OpenCV contour detection, and convex hull algorithms\n- Finger tracking with OpenCV and MediaPipe\n- Voice commands powered by OpenAI Whisper for speech recognition\n- Cursor/keyboard control via PyAutoGUI\n\nChallenges:\n- **Multithreading:** Python’s single-threaded nature caused conflicts when running webcam tracking and speech-to-text simultaneously\n- **Perspective distortion:** Mapping finger movements consistently across angled screens was a challenge\n- **Pivot:** Originally planned gesture shortcuts for controls but pivoted to voice commands for feasibility and accessibility\n\nAccomplishments:\n- Successfully segmented laptop screens with high accuracy\n- Achieved rough but functional cursor control via finger tracking\n- Overcame threading limitations by implementing multiprocessing in Python\n\nWhat We Learned:\n- Handling parallel processes in Python with the multiprocessing library\n- Techniques for robust image segmentation\n- Practical applications of PyAutoGUI for input control\n\nWhat’s Next:\n- Add full gesture recognition to complement voice commands\n- Improve coordinate accuracy for smoother cursor movement\n- Fine-tune PyAutoGUI drag speed for seamless interaction",
    "github": "https://github.com/dylanvu/PyPointer",
    "demo": "https://youtu.be/EUbloWRjQHo"
  },
  {
    "id": "journeyes",
    "name": "JournEyes",
    "summary": "JournEyes is an innovative VR travel assistant designed to enrich the travel experience by offering real-time identification and explanation of landmarks, signs, and objects. Users can interact with their surroundings through voice commands and image recognition, turning their journey into an immersive educational experience. By acting as a local guide, historian, and botanist, JournEyes helps explorers navigate and understand unfamiliar environments with ease.\n\nThe project was built using Unity with the Meta XR SDK to create the VR experience, leveraging Python with FastAPI for the backend and Google Lens for image recognition. OpenAI Whisper was employed for voice transcription, while the infrastructure utilized Docker for deployment and real-time communication through websockets and SocketIO. The team successfully overcame challenges related to video feed limitations, demonstrating innovative problem-solving and technical prowess. JournEyes was proudly submitted to TreeHacks 2024, marking a significant achievement in VR and AI integration.",
    "details": "Inspiration:\nOur team’s love for travel and curiosity about diverse cultures sparked the idea for JournEyes. We wanted a travel companion that enhances journeys by instantly translating signs, identifying objects, and guiding explorers in unfamiliar environments.\n\nWhat It Does:\nJournEyes is a VR travel assistant that lets users point and ask questions about the world around them. Using voice commands and image recognition, it identifies monuments, plants, or objects, and provides instant explanations—acting as a local guide, historian, and botanist all in one.\n\nHow We Built It:\n- Unity with Meta XR SDK for VR experience\n- Backend: Python with FastAPI, integrated with AI capabilities\n- Image recognition: Google Lens via SERP API\n- Voice transcription: OpenAI Whisper\n- Image cropping: Meta research paper techniques for precise object focus\n- Infrastructure: Docker for deployment, ngrok for connectivity, real-time communication via websockets and SocketIO\n\nChallenges:\n- Steep learning curve with Unity and Meta XR SDK\n- Meta restriction: no passthrough video feed capture from third-party software, requiring a workaround to stream casted video feed into the backend\n\nAccomplishments:\n- Built a functional VR travel app from scratch\n- Overcame video feed limitations with a custom streaming solution\n- Transitioned from web development to VR innovation within the hackathon timeframe\n\nWhat We Learned:\n- Unity integration with Meta Quest 2 and 3\n- Handling HTTP calls within Unity\n- Transforming screenshots into Base64 strings\n- Using Google Cloud for hosting and Google Lens for real-time object identification\n- Deeper knowledge of VR development and AI integration",
    "github": "https://github.com/spikecodes/JournEyes",
    "demo": "https://youtu.be/QdcCzyXl3Cg"
  },
  {
    "id": "talking-terry",
    "name": "Talking Terry",
    "summary": "Talking Terry is a voice-first, portable device designed to enhance accessibility and inclusivity for individuals who have difficulty interacting with phone screens. Utilizing the power of speech, it provides users with a seamless experience to ask questions, receive personalized recommendations, make calls, and analyze climate data. The device is equipped with features such as GPS tracking, multilingual text-to-speech capabilities, and integrates multiple APIs to enhance its functionality. Its mission is to improve quality of life by empowering users through intuitive voice interactions.\n\nThe project is powered by a Raspberry Pi and employs advanced technologies including Python, LangChain, OpenAI, and GPT-4 Turbo for its core operations. It integrates APIs such as SerpAPI, Yelp, Twilio, and Google Cloud for data storage and processing. Notably, Talking Terry won the \"Best Use of Google Cloud\" award at **QWER Hacks 2024**, showcasing its innovative use of cloud technology to store and retrieve user interactions for a personalized experience.",
    "details": "Inspiration:\nTalking Terry was inspired by our mission to foster inclusivity and empower those unable to interact with phone screens. We wanted to create a portable, accessible solution that elevates quality of life through speech-first interactions.\n\nWhat It Does:\nTalking Terry is a Raspberry Pi–powered, voice-first device equipped with GPS, microphone, and speaker. It integrates multiple APIs (SerpAPI, Yelp, Twilio, Weather, BruinLearn/Canvas, Global-Warming.org) and uses retrieval-augmented generation to answer questions, provide recommendations, make calls/texts, and analyze climate data. It also continuously stores transcripts in Google Cloud and remembers past interactions for personalized responses.\n\nHow We Built It:\n- Python with LangChain, OpenAI, and GPT-4 Turbo for core intelligence\n- APIs: SerpAPI, Yelp, Twilio, Weather API, BruinLearn/Canvas, Global-Warming.org\n- Hardware: Raspberry Pi, GPS module, microphone, and speaker\n- Google Cloud for transcript storage\n- Web scraping Ebsco’s LGBTQ+ Source for book recommendations\n\nChallenges:\n- API restrictions (Uber, Lyft, Spotify required logins/approvals)\n- Microphone issues (static recordings)\n- Hardware supply shortages and lack of soldering tools until late in hackathon\n\nAccomplishments:\n- Integrated multiple APIs into a retrieval-augmented generation pipeline\n- Built a working portable device with speech input/output\n- Achieved multilingual text-to-speech, including fluent Chinese\n- Enabled calling and texting via Twilio\n\nWhat We Learned:\n- Advanced use of LangChain and large language models\n- Retrieval-augmented generation with multiple APIs\n- Hands-on hardware integration (Raspberry Pi, GPS, mic, speaker)\n- Text-to-speech across multiple languages\n\nWhat’s Next:\n- Launching a web interface for extended functionality\n- Adding new agents (e.g., voice reminder app)\n- Gmail API integration for drafting and sending emails\n- Refining Yelp agent for richer restaurant and event insights\n- Expanding personalized and accessible features for users",
    "github": "https://github.com/tranbrandon1233/TalkingTerry",
    "demo": "https://youtu.be/nFpyDH8eSrE"
  },
  {
    "id": "xplore-p1dnvc",
    "name": "Xplore",
    "summary": " \nXplore is an intuitive travel planning app designed to make itinerary creation effortless. By allowing users to input their destination cities, stay duration, travel radius, and preferred attractions, Xplore provides AI-driven, personalized recommendations for attractions. Users can choose between manual or automatic selection of these suggestions, and the app generates and saves optimized travel routes to enhance the travel experience. The application aims to streamline travel planning, ensuring a seamless journey for its users.\n\nThe project leverages a robust tech stack, including Next.js and Chakra UI for the frontend, with Firebase and Clerk handling the backend and authentication processes. GraphQL is utilized for efficient API call batching, while MelissaAPI is integrated for precise address verification. The project was awarded \"Best Travel Hack\" at IrvineHacks 2024, highlighting its innovative approach to simplifying travel planning.",
    "details": "Inspiration:\nThe idea for Xplore came from a lighthearted slip-up when one of us couldn’t pronounce the word 'itinerary.' From that moment came the concept of an intuitive app that makes travel planning effortless.\n\nWhat It Does:\nXplore is a personalized travel companion with three core functions:\n- Travelers input city destinations, stay duration, travel radius, and preferred attractions.\n- AI-driven recommendations suggest tailored attractions, with the choice of manual or automatic selection.\n- Xplore generates and saves optimized travel routes for a seamless experience.\n\nHow We Built It:\n- User-centered design optimized for both desktop and mobile\n- Authentication system to save and retrieve routes\n- MelissaAPI integration for address verification\n- GraphQL for batching API calls\n- Next.js and Chakra UI for the frontend\n- Firebase and Clerk for backend and authentication\n- Multithreading for performance improvements\n\nChallenges:\n- Integrating and configuring MelissaAPI\n- Learning and applying new frameworks quickly\n- Refining the codebase for stability and performance\n- Synchronizing frontend and backend components\n- Implementing secure authentication\n- Managing fatigue during long hackathon nights\n\nAccomplishments:\n- Successfully batching API calls with GraphQL\n- Adding Progressive Web App (PWA) functionality\n- Designing and implementing a custom ML algorithm to optimize travel routes\n\nWhat We Learned:\n- Backend: GraphQL, Firebase, Clerk\n- Frontend: Next.js, Chakra UI\n- Git lesson: never let two people work on the same branch simultaneously\n- And most importantly: 'Itinerary' is essential in travel planning—and trickier to say than you’d think!\n\nWhat’s Next:\n- Real-time weather updates for destinations\n- Local public transport details (routes, schedules, fares)\n- Quick access to emergency contacts such as hospitals and embassies",
    "github": "https://github.com/IdkwhatImD0ing/Xplore",
    "demo": "https://youtu.be/U1GTyyG9-MY"
  },
  {
    "id": "besustainable",
    "name": "BeSustainable",
    "summary": "**Summary:**\n\nBeSustainable is a mobile-first application designed to empower individuals to incorporate sustainable habits into their daily lives. By providing features like barcode scanning and ingredient insights, the app helps users make informed decisions that positively impact the environment. It emphasizes the importance of small, manageable changes that collectively lead to significant improvements in sustainability and personal well-being. The app aims to make adopting sustainable practices seamless and rewarding for busy individuals.\n\nBuilt with a robust tech stack, BeSustainable utilizes Figma for design, MaterialUI for the frontend, Next.js for the backend, and MongoDB for database management. It integrates APIs such as Spoonacular for ingredient data and Google OCR for barcode scanning. The project also incorporates Auth0 for secure login and registration processes. BeSustainable was submitted to CruzHacks 2024, reflecting our commitment to innovative solutions in sustainability.",
    "details": "Inspiration:\nAs college students juggling heavy coursework, tackling global issues can feel overwhelming. We wanted a way for busy individuals to still make an impact. That led us to create BeSustainable — an app that empowers average people to take small, meaningful steps toward sustainability. Our belief is that small changes have a ripple effect, creating big change.\n\nWhat It Does:\n- Encourages sustainability habits by integrating them into daily life.\n- Empowers individuals to make small but impactful changes.\n- Facilitates smooth adoption of sustainable practices, improving health and well-being without major disruption.\n\nHow We Built It:\n- Designs in Figma\n- Frontend with MaterialUI\n- Backend with Next.js\n- Database: MongoDB\n- APIs: Spoonacular API, Google OCR\n\nChallenges:\n- Balancing creativity with feasibility in design (e.g., camera page flow).\n- Learning and adapting to MaterialUI framework.\n- Merge conflicts and formatting consistency.\n- Creating a sustainability score using ingredient lists and barcodes.\n- Connecting MongoDB to the project.\n- Integrating Auth0 for login/registration.\n\nAccomplishments:\n- Building an app designed to encourage sustainability.\n- Creating a simple, easy-to-use interface accessible via mobile.\n- Learning and applying new technologies to deliver a working product.\n\nWhat We Learned:\n- The importance of discussing features and flows to stay aligned.\n- The value of timelines and role assignments for development efficiency.\n- Technical growth in frameworks, APIs, and authentication systems.\n\nWhat’s Next:\n- Add an AI chatbot for recommendations, feedback, and ingredient insights.\n- Enable users to connect and interact with friends’ posts.\n- Allow users with high sustainability scores to connect globally, not just locally.",
    "github": "https://github.com/IdkwhatImD0ing/BeSustainable",
    "demo": "https://youtu.be/ukji3S3eCow"
  },
  {
    "id": "counsel",
    "name": "Counsely",
    "summary": "Counsely is a real-time AI assistant designed to support mental health professionals by reducing cognitive stress and offering valuable insights during and after therapy sessions. It provides real-time AI-powered conclusions and suggestions to therapists during sessions and offers comprehensive dashboards post-session with performance data and client insights. This tool helps counselors better understand their clients and improve the therapeutic experience, addressing the challenges faced by both new and seasoned therapists, especially in the realm of telehealth.\n\nBuilt with a user-centered approach, Counsely employs technologies such as Material UI for the frontend and Firebase for the backend. The development process overcame challenges like noisy microphone inputs and backend integration issues, achieving seamless real-time feedback through multithreading. The project, noted for its calming and empathetic user interface, was submitted to **SB Hacks X**, showcasing its potential impact in the mental health field.",
    "details": "Inspiration:\nLife is hard, and mental health professionals play a crucial role in helping us navigate it. The demand for mental health services has surged — Statista reports that 41.7 million U.S. adults received services in 2021, reflecting massive need. Yet new counselors are often thrown into the deep end, and even seasoned therapists face challenges in the digital age, especially with telehealth. Counsely aims to bridge the gap between counselor and client, reducing cognitive stress and providing unobtrusive insights.\n\nWhat It Does:\n- Acts as a real-time assistant for therapists.\n- During sessions: Provides AI-powered conclusions and suggestions in real time.\n- After sessions: Delivers dashboards with performance data and client insights to help counselors reflect and better understand their clients.\n\nHow It Was Built:\n- User-centered design guided by problem space research.\n- User-flow diagrams for both therapist and patient.\n- Feature prioritization stories to ensure efficient, user-focused implementation.\n\nChallenges:\n- Noisy laptop microphone quality, solved with noise filtering.\n- Backend package version errors when integrating statistics into the dashboard.\n\nAccomplishments:\n- Achieved real-time feedback flow using multithreading, ensuring insights arrive during sessions, not just after.\n- Designed a calming, empathetic UI to support mental health contexts.\n\nWhat We Learned:\n- Frontend: deeper experience with Material UI.\n- Backend: greater comfort with Firebase.\n- General: adaptability in the face of API issues, background noise, and other unexpected challenges.\n\nWhat’s Next:\n- Add a video-call feature to make the tool more versatile for telehealth sessions.",
    "github": "https://github.com/kaeladair/sbhacks24",
    "demo": "https://youtu.be/Q56mbQdtSnk"
  },
  {
    "id": "webweaver",
    "name": "WebWeaver",
    "summary": "WebWeaver is an AI-driven platform designed to simplify website creation for small businesses, freelancers, and non-profits. By leveraging an interactive AI chatbot, users can gather requirements and generate initial website drafts, which can be further refined through real-time collaboration. The platform caters to both non-technical users and experienced developers, offering a streamlined dashboard for managing the entire website lifecycle and advanced code editing interfaces for deeper customization.\n\nThe project is built using Next.js with Material UI for a responsive frontend, and FastAPI with Redis for efficient backend operations and state synchronization. GPT-Vision powers its AI capabilities, enabling multimodal website generation and drafting. WebWeaver has been recognized for its innovation, winning the **MLH: Most Innovative Startup Idea** award, and was submitted to **AI ATL (Atlanta)**, showcasing its potential to transform web development through AI-enhanced tools.",
    "details": "Inspiration / Problem:\nBuilding and maintaining a professional website remains a challenge for small businesses, freelancers, and non-profits due to technical barriers and limited resources. WebWeaver aims to democratize website creation with AI-powered collaboration.\n\nWhat It Does:\n- Provides an interactive AI chatbot to gather requirements and generate website drafts.\n- Allows real-time collaboration with the AI to refine content and design.\n- Includes advanced code editing interfaces for experienced developers.\n- Offers a streamlined dashboard for managing the full website lifecycle.\n\nHow It Was Built:\n- Frontend: Next.js with Material UI for sleek, responsive design.\n- Backend: FastAPI for APIs, Redis for state synchronization, Axios for client-server requests.\n- AI: GPT-Vision for multimodal website generation and drafting.\n\nChallenges:\n- Ensuring real-time synchronization across multiple clients.\n- Balancing simplicity for non-technical users with flexibility for developers.\n\nAccomplishments:\n- Successfully integrated AI into the web development workflow.\n- Built collaborative real-time editing tools.\n- Designed an accessible UI for both beginners and advanced users.\n\nWhat We Learned:\n- Strategies for combining AI with traditional web development.\n- Technical approaches to real-time collaborative editing.\n- Best practices in component-based architecture and state management.\n- The importance of UX in lowering barriers for non-technical users.\n\nWhat’s Next:\n- Support for more frameworks like React, Angular, and Vue.\n- AI-powered optimization for performance and SEO.\n- A mobile app version for on-the-go management.\n- Tutorials, guides, and a community forum for user support.",
    "github": "https://github.com/IdkwhatImD0ing/WebsiteGenerator",
    "demo": "https://youtu.be/V977Ydky1DI"
  },
  {
    "id": "pilltok",
    "name": "PillTok",
    "summary": "PillTok is an innovative healthtech application designed to simplify medication management by allowing users to upload prescription information through photos. The app identifies potential prescription interactions and generates personalized medication schedules that adapt to users' routines and feedback. By sending timely reminders and alerts, PillTok aims to enhance medication adherence, improve patient safety, and decrease healthcare costs, especially benefiting disabled individuals, non-native English speakers, and those in remote areas.\n\nThe app is built using a robust tech stack that includes Google Vision OCR for text extraction, React and Next.js for an interactive frontend, and FastAPI coupled with RedisCloud for high-performance backend operations. This seamless integration ensures a user-friendly experience, while the adaptive scheduling system and interaction checker demonstrate the app's advanced capabilities. PillTok was submitted to **HackSC X**, showcasing its potential to revolutionize medication management and enhance accessibility on a global scale.",
    "details": "Inspiration:\nPillTok was created to address the challenges of medication non-adherence and prescription interaction risks. The goal is to streamline medication management, safeguard patient health, and reduce healthcare costs, while ensuring accessibility for disabled individuals, non-native English speakers, and those in remote areas.\n\nWhat It Does:\n- Allows effortless prescription data uploads via photo capture.\n- Identifies and alerts users to problematic prescription interactions.\n- Generates personalized weekly medication schedules tailored to user routines.\n- Sends reminders and nudges to improve adherence.\n- Adapts schedules in real time based on user feedback.\n\nHow It Was Built:\n- Google Vision OCR for extracting text from prescription labels.\n- React and Next.js for building a dynamic, engaging frontend.\n- FastAPI for robust, high-performance backend services.\n- RedisCloud for fast and reliable database operations.\n\nChallenges:\n- Obtaining real prescription bottles for accurate data testing.\n- Extracting text from the curved surfaces of pill bottles.\n- Designing a scheduling algorithm capable of handling complex medication regimens.\n- Updating schedules responsively in real time based on user interactions.\n\nAccomplishments:\n- Successful OCR implementation with Google Vision API.\n- Built a thorough medication interaction checker.\n- Developed a smart, adaptive scheduling system.\n- Delivered a responsive, user-friendly interface with Next.js.\n- Integrated FastAPI and RedisCloud for optimal backend performance.\n\nWhat We Learned:\n- How to integrate Next.js, FastAPI, and RedisCloud into a seamless healthtech app.\n- The complexities of building secure backend services for health data.\n- Techniques for accurate OCR text extraction from curved and irregular surfaces.\n\nWhat’s Next:\n- Improving computer vision algorithms for greater accuracy.\n- Adding multi-language support for accessibility worldwide.\n- Developing an AI-powered chatbot for personalized health guidance.\n\nHow To Run:\n1. Clone the repository: git clone <url>\n2. Run frontend: cd client && npm install && npm start\n3. Run backend: cd server && pip install -r requirements.txt && uvicorn main:app --reload",
    "github": "https://github.com/rdszhao/pilltok",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/660/588/datas/gallery.jpg"
  },
  {
    "id": "studyai",
    "name": "StudyAI",
    "summary": "StudyAI is a cutting-edge voice-activated study assistant designed to enhance the self-study experience by addressing common challenges such as information overload, poor note organization, and lack of personalization. Utilizing advanced technologies like machine learning, voice recognition, and natural language understanding, it offers students a range of intuitive tools, including text and video summarization, explanatory images, personalized study notes, and educational video recommendations. The goal is to provide a seamless and efficient learning environment that adapts to individual student needs.\n\nThe project is built with a robust tech stack, featuring Reflex.dev for the frontend with custom React components and FastAPI for the backend. It employs 11Labs and Whisper for voice recognition and text-to-speech, OpenAI function calling agents for autonomous decision-making, Mistral-7B-Instruct-v0.1 for text summarization, and Stable Diffusion for image generation. Recognized for its innovative approach, StudyAI was submitted to **Cal Hacks 10.0**, showcasing its potential to revolutionize educational technology.",
    "details": "Overview:\nStudyAI (Systematic Teaching Using Dynamic Yielding and Autonomous Intelligence) is a voice-activated study assistant designed to redefine the self-study experience. By leveraging machine learning, voice recognition, and natural language understanding, it provides students with efficient and intuitive study tools.\n\nInspiration:\nWith the overwhelming amount of information available today, self-study often leads to information overload, poor note management, and a lack of personalization. StudyAI was built to address these issues, offering a streamlined and intelligent approach to learning.\n\nGoals:\n- Deliver a seamless, voice-activated study assistant.\n- Provide text and video summarization.\n- Generate explanatory images and visualizations.\n- Recommend educational videos.\n- Manage personalized study notes.\n\nBuilt With:\n- Frontend: Reflex.dev with custom React components\n- Backend: FastAPI (via Reflex.dev)\n- Voice Recognition & TTS: 11Labs, Whisper\n- Autonomous Agents: OpenAI function calling agents\n- Text Summarization: mistralai/Mistral-7B-Instruct-v0.1 (TogetherAI)\n- Image Generation: stabilityai/stable-diffusion-2-1 (TogetherAI)\n\nChallenges:\n- Achieving real-time voice recognition with less than 5 second latency.\n- Seamless integration of multiple ML models and APIs.\n- Balancing scalability, performance, and reliability.\n- Maintaining strict data privacy and security.\n\nAccomplishments:\n- Built a voice-activated command parsing system.\n- Integrated autonomous decision-making via OpenAI function calling.\n- Developed accurate data-fetching and summarization modules.\n- Integrated YouTube API for educational content recommendations.\n\nWhat We Learned:\n- Effective integration of machine learning with voice technologies.\n- Importance of modular design for scalability.\n- Handling and optimizing large datasets for quick retrieval.\n- Designing user-friendly experiences tailored for education.\n\nWhat’s Next:\n- Personalized learning paths based on user behavior.\n- Expanding academic resources and journal access.\n- Developing a mobile app for accessibility on the go.\n- Partnering with educational institutions for wider adoption.\n\nFeatures:\n- Voice activation and command parsing.\n- Autonomous decision-making to adapt to student needs.\n- Text summarization for concise academic material.\n- Image and visualization generation for complex topics.\n- YouTube video suggestions for extended learning.",
    "github": "https://github.com/IdkwhatImD0ing/STUDYAI",
    "demo": ""
  },
  {
    "id": "slugloop",
    "name": "SlugLoop",
    "summary": "SlugLoop is a real-time bus tracking application developed to assist UCSC students in making informed transportation decisions by providing accurate and up-to-date locations of campus loop buses. By encouraging the use of loop buses, SlugLoop helps alleviate congestion on metro buses, ensuring that students who need to travel off-campus have better access to transportation. The app delivers a simple, mobile-friendly interface that allows users to quickly view live bus locations, enhancing the overall commuting experience on campus.\n\nThe project leverages existing GPS hardware installed on loop buses, with data processed through ExpressJS and stored in Firebase for real-time access. The frontend is built using React and Node.js, providing a seamless user experience by displaying bus locations on a map. Data transfer is handled with the LibCurl library and C, integrating multiple frameworks into a cohesive solution. Recognized for its innovative approach, SlugLoop was submitted to CruzHacks 2023 and won the [MLH – GitHub] Most Creative Use of GitHub award.",
    "details": "Inspiration:\nFor many UCSC students, the only transportation options are metro and loop buses. Because loop buses are less predictable, students often take metro buses, filling them up and leaving those who need to travel off-campus without space. SlugLoop was created to provide real-time loop bus tracking, encouraging students to use loop buses and reducing pressure on metro services.\n\nGoals:\n- Provide accurate, up-to-date loop bus locations.\n- Help UCSC students make informed transportation decisions.\n- Relieve metro congestion during peak hours.\n- Build a maintainable project that can be expanded by the school community.\n\nHow It Was Built:\n- Utilizes GPS hardware already installed on loop buses by UCSC.\n- Data from GPS receivers processed with ExpressJS.\n- Bus data stored in Firebase for real-time access.\n- Frontend built with React and Node.js, displaying live bus locations on a map.\n- Data transferred using LibCurl library and C.\n\nChallenges:\n- Difficulty obtaining GPS data from relay stations on campus (only 3 of 5 stations functional).\n- Technical hosting issues delayed development.\n- Limited loop bus operation on weekends restricted data.\n- Required coordination with UCSC staff to gain server access.\n\nAccomplishments:\n- Built an app with immediate, positive impact for UCSC students.\n- Delivered a simple, mobile-friendly interface for quick data access.\n- Created a system that improves metro bus availability by encouraging loop bus usage.\n- Integrated multiple frameworks into a reliable, real-time solution.\n\nWhat We Learned:\n- The importance of planning and design before coding.\n- How to integrate multiple frameworks for a seamless product.\n- Simplicity in interface design is key to adoption.\n- Assigning tasks based on team strengths improves efficiency.\n\nWhat’s Next:\n- Collecting more data to train a machine learning model for accurate bus arrival predictions.\n- Upgrading or replacing campus GPS hardware to improve coverage.\n- Installing missing GPS hardware on all loop buses.",
    "github": "https://github.com/IdkwhatImD0ing/SlugLoop",
    "demo": "https://youtu.be/DlAGp-IjtJM"
  },
  {
    "id": "architect-rev4cq",
    "name": "Architect",
    "summary": "Architect is an innovative AI-powered assistant designed to aid hackathon participants in rapidly transforming ideas into viable prototypes. It employs three specialized language models to generate frontend designs, suggest backend architectures, and conduct feasibility analyses, streamlining the development process under the intense pressure of hackathon environments. By providing recommendations on technologies, frameworks, and project viability, Architect empowers innovators to focus on creativity and execution.\n\nThe project is built on a robust pipeline that integrates three distinct language models: FLLM for frontend design, BLLM for backend architecture, and FALM for feasibility analysis, all accessible via a user-friendly interface. This meticulous integration ensures seamless data flow and accurate assessments. Architect was recognized for its innovation and effectiveness, earning the Third Overall award at the **Hacks for Hackers** competition.",
    "details": "Inspiration:\nArchitect was inspired by the fast-paced energy of hackathons, where participants struggle to quickly turn ideas into working prototypes. The project’s creators envisioned a digital ally powered by language models to generate frontend designs, backend architectures, and feasibility analyses, helping innovators bring concepts to life in the high-pressure hackathon environment.\n\nWhat It Does:\n- Uses three separate language models (LLM chains) to support ideation.\n- Generates frontend design ideas, features, and technologies.\n- Suggests backend architectures, database solutions, and frameworks.\n- Evaluates project feasibility based on technologies, skills, and time constraints.\n\nHow It Was Built:\n- Developed as a pipeline integrating three LLMs:\n  - FLLM: Frontend Design Language Model\n  - BLLM: Backend Architecture Language Model\n  - FALM: Feasibility Analysis Language Model\n- Simple UI for users to provide project ideas and skill categories.\n\nChallenges:\n- Ensuring feasibility analysis is accurate and realistic.\n- Managing data flow between multiple LLMs.\n- Designing an intuitive user experience with clear feedback.\n\nAccomplishments:\n- Built a functional LLM pipeline for frontend, backend, and feasibility tasks.\n- Implemented feasibility checks to assess project viability.\n- Designed a user-friendly interface to simplify inputs and outputs.\n\nWhat We Learned:\n- How to integrate multiple LLMs for distinct yet coordinated tasks.\n- The importance of intuitive UI/UX in enhancing user adoption.\n- The complexity of applying LLMs to feasibility evaluation.\n\nWhat’s Next:\n- Add an “Inspirational Mode” to suggest novel hackathon ideas based on trends.\n- Integrate educational resources to help users upskill if their project goals exceed their current capabilities.",
    "github": "https://github.com/Ananya2001-an/Architect",
    "demo": "https://youtu.be/xUdMTt6hMqA"
  },
  {
    "id": "fundriser",
    "name": "FundRiser",
    "summary": " \nFundRiser is a web3 crowdfunding platform designed to provide a seamless and secure fundraising experience. Utilizing the Hedera network for blockchain-based transparency, it enables users to create and manage campaigns with ease. The platform supports both anonymous cryptocurrency donations through MetaMask and traditional card payments via Circle, ensuring flexible and versatile funding options. Secure authentication is provided through Microsoft Azure, enhancing user trust and data protection.\n\nBuilt with a robust tech stack, FundRiser employs Express (Node.js) for backend logic and React with Tailwind CSS for a dynamic and responsive user interface. The integration of Hedera ensures immutable donation records, while Circle facilitates seamless payment processing. The project was recognized for its innovative use of technology, winning the awards for Best Use of Microsoft Cloud for Your Community and Best Use of Circle at the Web3Apps - Hosted by MLH Hackathon.",
    "details": "Inspiration:\nFundRiser was inspired by the idea of creating a web3 crowdfunding platform that uses blockchain (Hedera network) for transparency and security. The team wanted to make fundraising seamless, secure, and flexible—allowing both anonymous crypto donations and traditional card payments, with simple authentication via Microsoft Azure.\n\nWhat It Does:\n- Lets users create and manage fundraising campaigns with goals and project details.\n- Accepts donations via cryptocurrency (MetaMask) or traditional card payments (Circle → USDC).\n- Supports anonymous donations while keeping transaction records secure and transparent on Hedera.\n- Provides secure authentication through Microsoft Azure accounts.\n\nHow It Was Built:\n- Backend: Express (Node.js) for routing and server logic.\n- Frontend: React for a dynamic UI, styled with Tailwind CSS for responsive design.\n- Blockchain: Hedera network for secure, immutable donation records and smart contracts.\n- Authentication: Microsoft Azure for robust and trusted login.\n- Payments: Circle for credit card donations, converting them to USDC.\n\nChallenges:\n- Integrating multiple APIs and ensuring seamless functionality.\n- Learning and implementing Azure APIs for authentication.\n- Handling Circle’s payment integration with blockchain interoperability.\n\nAccomplishments:\n- Built a fully functional web3 crowdfunding platform.\n- Achieved smooth integration of blockchain, authentication, and payment solutions.\n- Delivered a user-friendly and secure fundraising experience.\n\nWhat We Learned:\n- Hands-on blockchain integration with Hedera.\n- Implementing secure authentication with Microsoft Azure.\n- Payment handling via Circle and anonymous crypto donations.\n- Strengthened skills with React, Express, and Tailwind CSS.\n\nWhat’s Next:\n- Enhanced campaign management (progress tracking, contributor updates).\n- Social media integration for easier campaign sharing.\n- Advanced analytics to give creators insights into donations and engagement.\n- Localization with multi-language and multi-currency support.\n- Partnerships with organizations and influencers.\n- Community-building features such as forums and collaboration spaces.",
    "github": "https://github.com/Satoshi-Sh/crowdfunding",
    "demo": "https://www.youtube.com/watch?v=AXu1dudtp-k"
  },
  {
    "id": "dreamcatch",
    "name": "DreamCatch",
    "summary": "DreamCatch is an innovative platform designed to harness the potential of dreams for self-discovery and personal growth. By recording and analyzing dreams using advanced AI, it helps users identify recurring themes and gain psychoanalytic insights. The platform also fosters an open community where dream enthusiasts can share and discuss their experiences, encouraging collective exploration and understanding of the dream world.\n\nThe project utilizes NextJS and TailwindCSS for a seamless frontend experience, while employing advanced AI technologies such as voice-to-text transcription and sentiment analysis for mood identification. Key integrations include Hume.AI for emotion extraction and AnyScale for scaling Whisper transcription models, all backed by Firebase's secure cloud storage. DreamCatch was submitted to the UC Berkeley AI Hackathon, highlighting its innovative approach to dream analysis.",
    "details": "Inspiration:\nDreams remain a fascinating mystery that often go unexplored. At DreamCatch, we believe dreams hold potential for self-discovery and personal growth. Our goal was to create a platform where users can record, analyze, and learn from their dreams with the help of AI.\n\nGoals:\n- Create an AI-based platform that records and analyzes dreams.\n- Identify recurring themes and patterns.\n- Provide psychoanalytic insights.\n- Build an open, community-driven platform.\n- Enable sharing and discussion among dream enthusiasts.\n\nBuilt With:\n- NextJS & TailwindCSS (frontend)\n- Advanced AI (voice-to-text transcription)\n- Sentiment Analysis (mood identification)\n- Keyword Extraction (themes & symbols)\n- Psychoanalysis algorithms (dream insights)\n- Firebase (secure cloud storage)\n\nChallenges:\n- Ensuring seamless and accurate transcription.\n- Interpreting moods, symbols, and themes effectively.\n- Building an intuitive, user-friendly interface.\n- Maintaining data security and privacy.\n- Encouraging meaningful community engagement.\n\nAccomplishments:\n- Developed an AI-driven dream analysis platform with an intuitive interface.\n- Enabled users to record, analyze, and share dreams.\n- Implemented advanced integrations:\n  • Hume.AI for emotion extraction\n  • AnyScale to scale Whisper transcription models\n- Fostered a community space for dream discussion.\n\nWhat We Learned:\n- Importance of user-centric design.\n- Potential of AI for interpreting human emotions.\n- Value of community collaboration.\n- Practical integration of advanced technologies into one cohesive platform.\n\nWhat’s Next:\n- Machine learning models for dream trend analysis.\n- Dream visualization with images or illustrations.\n- Expanded community interaction features.\n- Smart reminders for recording dreams.\n- Personalized insights and recommendations.",
    "github": "https://github.com/IdkwhatImD0ing/DreamCatcher",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/511/375/datas/gallery.jpg"
  },
  {
    "id": "intelliconverse",
    "name": "IntelliConverse",
    "summary": "IntelliConverse is an innovative AI-driven tool designed to assist individuals with learning disabilities such as dyslexia and ADHD, as well as those who face reading challenges. By integrating both text and voice input/output, the platform enhances communication and empowers users to effectively navigate their learning obstacles. Featuring capabilities like regular chat, voice chat, and voice chat with PDF references, IntelliConverse provides a versatile environment tailored to multiple learning styles. This tool aims to foster a more inclusive and supportive educational experience for its users.\n\nThe project employs a robust tech stack, including a custom NextJS server, Express server, and a frontend built with NextJS and Material UI. It integrates advanced functionalities such as speech-to-text and text-to-speech, utilizing Azure Speech-to-Text for precise audio processing. Data management is handled through Milvus and MongoDB, with ChatGPT integration enhancing interactivity. IntelliConverse was submitted to HackDavis 2023, where it received the accolade for Best Use of MongoDB Atlas, highlighting its technical excellence and innovative approach.",
    "details": "Inspiration:\nIntelliConverse is designed to support individuals with learning disabilities such as dyslexia and ADHD, as well as those with reading difficulties. By combining text and voice input/output, it fosters effective communication and empowers users to overcome learning challenges.\n\nHow We Built It:\nBackend Development:\n- Custom NextJS server and Express server\n- Routes for:\n  • Speech-to-text\n  • Text-to-speech\n  • Adding, querying, and removing data from Milvus and MongoDB\n  • ChatGPT integration\n\nFrontend Development:\n- NextJS with Material UI\n- Three chat interfaces:\n  • Regular chat\n  • Voice chat\n  • Voice chat with PDF (assistant answers questions with references from PDFs)\n\nChallenges:\n- Testing Azure Speech-to-Text integration for accuracy and reliability.\n- Recording and formatting frontend audio for Azure Speech-to-Text while ensuring high-quality input.\n- Debugging, troubleshooting, and fine-tuning audio processing for seamless performance.\n\nAccomplishments:\n- Successfully implemented text-to-speech and speech-to-text features.\n- Enabled voice chat and voice chat with PDF functionalities.\n- Delivered a versatile platform supporting multiple learning styles.\n\nWhat We Learned:\n- Gained deeper understanding of audio capture and processing.\n- Enhanced expertise in Azure Speech-to-Text integration.\n- Improved frontend audio handling and user experience design.\n\nWhat’s Next:\n- Customization and training of ChatGPT for a safer, more supportive environment.\n- Collecting and applying user feedback to refine user experience.\n- New features: multi-PDF uploads, audio streaming for faster responses.\n- Bug fixes: over-context limits, UI/UX improvements.",
    "github": "https://github.com/IdkwhatImD0ing/study-assistant",
    "demo": ""
  },
  {
    "id": "gitpt",
    "name": "GitPT",
    "summary": "GitPT is a tool designed to provide students with an affordable way to explore and understand GitHub repositories. By leveraging GPT-3, GitPT parses a repository link to extract key information, fetches data via the OpenAI API, and presents summaries of the repository's contents. This includes displaying files like README.md or code files with syntax highlighting and explanations, ultimately saving developers time and enhancing their learning experience when navigating new repositories.\n\nThe project utilizes a robust tech stack, including Three.js, Next.js, Tailwind CSS, and TypeScript for the front end, and Node.js with Express for the back end. MongoDB is used for storing summaries and content, while Milvus serves as a vector database. GitPT was recognized at SB Hacks IX, winning the \"Student Life Hack\" award for its innovative approach to making codebase exploration more accessible to students.",
    "details": "Inspiration:\nGPT-4 can store entire codebases in its 32k context, but each query costs almost $2. Ten queries could be nearly $20, which is unsustainable for students. GitPT provides an affordable alternative using low-context chatbots such as GPT-3.\n\nWhat It Does:\nGitPT parses a GitHub repository link to extract the owner and repository names. It then fetches repository information via the OpenAI API and summarizes the contents. The app displays files like README.md or code files in a user-friendly format with syntax highlighting and explanations, saving developers time when exploring new repositories.\n\nHow We Built It:\n- Frontend: Three.js, Next.js, Tailwind CSS, TypeScript\n- Backend: Node.js and Express\n- Database: MongoDB (non-SQL) for storing summaries and content, Milvus as a vector database\n- OpenAI API: Used to summarize repository files\n- Integrated interactive SSR 3D models into the UI/UX with Three.js\n\nChallenges:\n- Designing an intuitive and visually appealing UI with Three.js integration.\n- Managing complex data models and ensuring seamless search and messaging features.\n- Building a breadcrumb and tree navigation system for exploring repository paths.\n\nAccomplishments:\n- Successfully integrated a responsive Three.js wireframe torus knot into the UI.\n- Built a working system enabling affordable exploration of repositories.\n- Gained hands-on experience in front-end, back-end, and database management.\n\nWhat We Learned:\n- Best practices for front-end design with Three.js in a Next.js TypeScript project.\n- Effective use of server-side rendering and integration of vector databases with non-SQL databases.\n- Value of strong communication and collaboration within the team.\n\nWhat’s Next:\n- Official deployment to provide students with a low-cost alternative to GPT-4 queries.\n- Outreach to promote repository understanding and learning.\n- Expand features, improve usability, and develop a mobile app.\n- Build partnerships with educational organizations to grow the user base.",
    "github": "https://github.com/simon-quach/GitPT",
    "demo": "https://youtu.be/Sxe8s0MH3Qo"
  },
  {
    "id": "monkeysign",
    "name": "MonkeySign",
    "summary": "MonkeySign is an innovative platform designed to make learning American Sign Language (ASL) engaging and interactive. Inspired by the arcade-style experience of MonkeyType, it uses real-time hand gesture detection to recognize ASL letters, allowing users to improve their skills in a fun and game-like environment. The platform aims to promote education and social good by providing an accessible and enjoyable way to learn ASL. Users progress through the game by successfully signing letters, reinforcing their learning in a dynamic setting.\n\nThe project employs a robust tech stack, featuring Vite and TailwindCSS for a responsive frontend, alongside Flask and socket.io for seamless communication between the frontend and a machine learning model. Handtrack.js and a custom-trained CNN are used for hand detection and letter recognition, respectively. Recognized for its innovation, MonkeySign was awarded \"Best Tracks Hack 'New Frontiers'\" at Citrus Hack 2023, highlighting its potential to revolutionize ASL education.",
    "details": "Project Overview:\nMonkeySign is a game-like platform designed to teach American Sign Language (ASL) in an engaging and interactive way. Inspired by MonkeyType, it brings a similar arcade-style experience to ASL learning, emphasizing education and social good.\n\nCore Features:\n- Real-time hand gesture detection to recognize ASL letters.\n- Users progress through the game by successfully signing letters, improving their ASL skills in a fun environment.\n\nTechnical Details:\n- Frontend: Vite and TailwindCSS for a responsive UI.\n- Backend: Flask and socket.io for communication between frontend and ML model.\n- Detection: Handtrack.js for hand detection and a custom CNN trained on ASL data for letter recognition.\n\nChallenges & Solutions:\n- Raspberry Pi Camera malfunctioned during the hackathon, so the team pivoted to an arcade-like web experience.\n- Model training difficulties led to splitting the process into two models (hand detection and letter recognition).\n- Bounding box integration with webcam data required multiple iterations but was successfully implemented.\n\nAccomplishments:\n- Successfully integrated frontend, backend, and Raspberry Pi components into a cohesive system.\n- Created an interactive, arcade-style ASL learning tool.\n\nLessons Learned:\n- Training object detection models.\n- Handling live data streams and computer vision challenges.\n- Coordinating multiple technologies into one functional product.\n\nFuture Development:\n- Add multiplayer functionality for collaborative learning.\n- Fix Raspberry Pi integration for an arcade-style booth experience.\n- Improve ML models for better accuracy across conditions.",
    "github": "https://github.com/IdkwhatImD0ing/MonkeySign",
    "demo": "https://youtu.be/atvJ0q10_M8"
  },
  {
    "id": "memgen-focused-memory-gpt",
    "name": "MemGen: Intelligent Vector-Based Cover Letter Creator",
    "summary": "MemGen is an innovative platform designed to simplify the creation of customized cover letters. By leveraging large language models and vector databases, MemGen allows users to search through their past experiences to generate professional and personalized cover letters efficiently. This tool aims to alleviate the burden of writing unique cover letters, particularly for programmers, by providing a streamlined process that empowers users to present their career achievements effectively.\n\nThe project utilizes a robust tech stack, including React, NextJS, and Tailwind CSS for the frontend, and ExpressJS, OpenAI, and Firebase for the backend. It integrates vector databases like Milvus and Firebase to enable fast semantic search and data management. MemGen was showcased at **LA Hacks 2023**, highlighting its potential to transform how individuals approach job applications by combining cutting-edge technology with user-centric design.",
    "details": "Overview:\nMemGen enables users to create customized cover letters by searching through their past experiences using large language models and embeddings.\n\nInspiration:\nWriting unique and personalized cover letters can be tedious, especially for programmers. While sometimes optional, not submitting one often leaves candidates feeling guilty. MemGen was built to streamline this process, empowering users to generate professional, tailored cover letters quickly and effectively.\n\nGoals:\n- Use vector databases to store past experiences and enable semantic search with NLP.\n- Allow users to generate cover letters using LLMs and stored career data.\n- Provide a user-friendly interface for uploading documents and creating outputs.\n\nTech Stack:\n- Frontend: React, NextJS, Tailwind CSS, Axios, Material UI, Auth0.\n- Backend: ExpressJS, OpenAI, Firebase Admin, Auth0, Cohere, Google Cloud.\n- Database: Milvus, Firebase, Zillis.\n\nChallenges:\n- Integrating multiple technologies and databases.\n- Learning and applying vector database concepts.\n- Ensuring platform security and data protection.\n- Implementing new APIs.\n\nWhat We Learned:\n- Effective use of vector databases for embeddings and fast semantic search.\n- Integrating diverse technologies into a seamless product.\n- Building secure, scalable systems for user-facing applications.\n\nWhat’s Next:\n- Integrate Stripe API for billing.\n- Develop a custom vector database for scalability and performance.\n- Enhance UI/UX for improved accessibility.\n- Add new features like job posting and job search functionality.",
    "github": "https://github.com/IdkwhatImD0ing/MemGen",
    "demo": "https://youtu.be/v5PAwb0BYtw"
  },
  {
    "id": "assistance",
    "name": "Assistance: Multi-LLM Chatbot",
    "summary": "Assistance is an innovative platform designed to facilitate seamless interaction with multiple large language models (LLMs) from a single interface. By enabling users to query models like ChatGPT, Meta LLaMA, Google Bard, and Bing Chat simultaneously, the platform allows for efficient comparison of their unique strengths. Users can effortlessly switch between models while preserving the context of conversations, making it easier to harness the best features of each LLM without the need for manual testing.\n\nThe project employs a robust tech stack, including the Gatsby Framework and Material UI for the frontend, and FastAPI for the backend. It also utilizes reverse-engineered Python APIs to interact with the models. Assistance was recognized for its innovative approach at **ACMHacks 2023**, where it won the award for **Best Global Solution**. The platform's development involved overcoming challenges such as working with new frameworks and handling undocumented APIs, showcasing the team's technical prowess and creativity.",
    "details": "Overview:\nAssistance enables intuitive interaction with multiple LLMs in a single platform. Users can query models simultaneously, switch between them, and maintain context across conversations.\n\nInspiration:\nThe recent surge of LLMs—such as ChatGPT, Meta LLaMA, Google Bard, and Bing Chat—highlighted how each model has different strengths due to varying training data. Instead of manually testing each, Assistance unifies them into one interface for comparison and efficiency.\n\nGoals:\n- Query all supported models at once.\n- Allow users to select and continue conversations with specific models.\n- Preserve conversational context across multiple queries.\n\nTechnologies Used:\n- Frontend: Gatsby Framework, Material UI.\n- Backend: FastAPI.\n- APIs: Reverse-engineered Python APIs for model interaction.\n\nChallenges:\n- First experience with Gatsby and FastAPI.\n- Reverse-engineered APIs had minimal documentation, requiring creative problem-solving.\n\nWhat We Learned:\n- Working with new frontend and backend frameworks.\n- Handling incomplete or undocumented APIs.\n- Converting strings to markdown displays.\n- Advanced React context usage.\n- Importance of memoization to reduce re-renders.\n- Managing asynchronous code and custom Docker containers.\n\nWhat’s Next:\n- Strengthen backend and frontend security.\n- Add Firebase authentication for user accounts and multi-device access.\n- Enable users to integrate their own API keys.\n- Introduce paid subscription tiers with premium features.",
    "github": "https://github.com/IdkwhatImD0ing/Assistance",
    "demo": "https://youtu.be/DAuMnWCKtSk"
  },
  {
    "id": "sink-or-swim",
    "name": "Sink or Swim (SOS)",
    "summary": " \nSink or Swim (SOS) is an innovative web application that combines machine learning and storytelling to simulate the survival likelihood of passengers aboard the Titanic. By leveraging historical data, the platform not only predicts a user's chance of survival based on factors such as age, gender, and socio-economic class but also crafts a personalized narrative that immerses users in the historical context of the 1912 disaster. This unique blend of AI-driven predictions and creative storytelling aims to make history engaging and educational for users.\n\nThe tech stack for Sink or Swim (SOS) includes TensorFlow, PyTorch, and scikit-learn for machine learning, with the Kaggle Titanic dataset serving as the foundation for predictions. The frontend is built using HTML, Tailwind CSS, and NextJS 14, while the backend is powered by Flask, with additional tools like Axios and Framer Motion enhancing functionality and user experience. The project was recognized at the Planet Unity Spring 2023 competition, earning a Top 10 Award for its innovative approach and technical excellence.",
    "details": "Overview:\nSink or Swim (SOS) is a machine learning and storytelling webapp inspired by the Titanic disaster of April 15, 1912. It predicts a passenger's survival likelihood and generates a personalized narrative of their experience on the ship.\n\nInspiration:\nThe tragedy of the Titanic has long captivated the world. Our team wanted to combine historical exploration with modern AI to simulate survival chances while making history come alive through personalized storytelling.\n\nHow It Works:\n- A recurrent neural network trained on the Kaggle Titanic dataset predicts survival probability based on age, gender, and socio-economic class.\n- Users input personal details (e.g., name, age, class).\n- OpenAI’s language generation technology creates a unique narrative blending prediction results with user inputs, offering an immersive Titanic experience.\n\nTech Stack:\n- Machine Learning: TensorFlow, PyTorch, scikit-learn\n- Dataset: Kaggle Titanic dataset\n- Frontend: HTML, Tailwind CSS, NextJS 14\n- Backend: Flask\n- Other: Axios, Framer Motion (for animations)\n\nChallenges:\n- Cleaning and preprocessing the dataset for accuracy.\n- Balancing ML model accuracy with computational efficiency.\n- Integrating ML with web frameworks and language generation.\n- Learning new tools like NextJS 14.\n- Handling Axios post request limits.\n- Ensuring a user-friendly and secure interface.\n\nAccomplishments:\n- Developed an accurate, efficient recurrent neural network.\n- Successfully integrated ML predictions with generative storytelling.\n- Built a scalable, engaging, and user-friendly interface.\n- Ensured data privacy and security.\n- Overcame technical hurdles under time constraints.\n\nWhat We Learned:\n- The importance of thorough data cleaning for ML accuracy.\n- Building engaging, user-friendly apps with NextJS 13/14.\n- Implementing animations with Framer Motion.\n- Designing scalable ML-driven applications.\n\nSink or Swim (SOS) demonstrates how AI can bring history to life through interactive storytelling while tackling technical, data, and design challenges.",
    "github": "https://github.com/simon-quach/sink-or-swim",
    "demo": "https://youtu.be/4V-2l-Zel5s"
  },
  {
    "id": "progno-d",
    "name": "Progno-D",
    "summary": "Progno-D is an AI-powered system designed to streamline the process of selecting medications by providing users with informed recommendations. By analyzing past customer reviews, ratings, and sentiment data, it identifies the most suitable drugs based on user-specific needs and preferences. The platform also delivers comprehensive drug information, helping users make well-informed healthcare decisions. Progno-D aims to alleviate the challenges consumers face when choosing the right prescription from a vast array of options.\n\nBuilt using a robust tech stack that includes Python, Pandas, NumPy, and NLTK for data processing and model training, Progno-D offers a clean interface developed with React for enhanced user experience. The project utilizes datasets from the UCI Machine Learning Repository and incorporates sentiment analysis through NLTK and Matplotlib. Recognized for its innovative approach, Progno-D was submitted to AI Hacks, where it won the First Overall award, highlighting its impact and effectiveness in transforming healthcare decision-making.",
    "details": "Overview:\nProgno-D is an AI-powered medicine recommendation system designed to simplify the process of selecting prescriptions. It leverages past customer reviews, ratings, and sentiment analysis to provide users with reliable drug recommendations along with detailed information.\n\nInspiration:\nWith countless medications available, consumers often face difficulty choosing the right prescription. Progno-D was built to solve this problem by providing data-driven recommendations powered by natural language processing and historical feedback.\n\nWhat It Does:\n- Analyzes drug ratings, reviews, and customer sentiments.\n- Recommends the most suitable medications based on user needs.\n- Provides detailed drug information for better decision-making.\n\nHow We Built It:\n- Dataset: UCI Machine Learning Repository.\n- Data cleaning and sentiment processing: NLTK, Matplotlib.\n- Key features: Customer ratings and reviews.\n- Model training: Python, Pandas, NumPy.\n- Frontend: React for a clean, intuitive interface.\n\nChallenges:\n- Cleaning and preprocessing complex datasets.\n- Performing exploratory data analysis (EDA) to extract insights.\n- Deploying the AI model for usability and reliability.\n\nAccomplishments:\n- Built and deployed a recommendation model with strong accuracy.\n- Designed a practical tool that makes healthcare decisions more accessible.\n\nWhat We Learned:\n- Strengthening technical skills in React, Python, NLTK, Pandas, NumPy, and GitHub.\n- Improving teamwork, collaboration, and time management.\n\nWhat’s Next:\n- Expanding recommendations to cover more illnesses.\n- Suggesting local and online sources for purchasing medications.\n- Exploring integration for direct medication procurement.\n\nProgno-D highlights how AI can positively impact healthcare by empowering patients with informed, data-driven prescription choices.",
    "github": "https://github.com/RupakKadhare15/Progno-D",
    "demo": "https://youtu.be/eqZS5WqHXW4"
  },
  {
    "id": "paddyplantprognosis",
    "name": "PaddyPlantPrognosis",
    "summary": "PaddyPlantPrognosis is a web application designed to assist farmers in diagnosing diseases in rice crops using advanced computer vision technology. By simply capturing or uploading images of their paddy plants, farmers receive real-time diagnoses along with actionable treatment insights, helping to protect crops and minimize yield loss. This tool is particularly beneficial for those in regions with limited access to agricultural experts, offering a fast, affordable, and accessible solution to combat rice crop diseases and pests.\n\nThe project is built using a modern tech stack including Vite and Tailwind CSS for a responsive frontend, and Flask for backend API and model integration. Machine learning models are developed with TensorFlow and Keras, while image processing is handled using OpenCV and NumPy. PaddyPlantPrognosis was recognized for its innovation and effectiveness, winning the \"Best Data Science Hack\" award at Hackrithmitic 2.",
    "details": "Overview:\nPaddyPlantPrognosis is a web app designed to help farmers identify diseases in rice crops using computer vision. By analyzing images of paddy plants, it provides real-time diagnoses and actionable treatment insights to protect crops and reduce yield loss.\n\nInspiration:\nRice is the staple food for much of the world, especially across Asia. However, paddy cultivation is highly vulnerable to diseases and pests, which can cause severe crop damage and reduced yields. Manual diagnosis is slow and costly, motivating the creation of a fast, affordable, and accessible AI-powered solution.\n\nWhat It Does:\n- Farmers capture or upload images of rice plants.\n- Computer vision models analyze the image to detect diseases or pests.\n- The app returns diagnoses with recommended actions for treatment.\n- Optimized for mobile devices, supporting direct camera use even in low-connectivity regions.\n\nHow We Built It:\n- Frontend: Vite + Tailwind CSS for a responsive, mobile-friendly UI.\n- Backend: Flask (Python) for API and model integration.\n- Machine learning: TensorFlow + Keras for model training on rice disease datasets.\n- Image processing: OpenCV, with NumPy for numerical computation.\n- Focus on lightweight design for rural accessibility.\n\nChallenges:\n- Limited documentation for new frameworks like Vite and Tailwind CSS.\n- Hardware constraints during ML model training.\n- Handling Git merge conflicts during team collaboration.\n\nAccomplishments:\n- Built and deployed a computer vision web app for real-time rice disease detection.\n- Integrated ML models with strong accuracy in diagnosing crop issues.\n- Designed a mobile-first, accessible interface tailored for farmers.\n- Created a scalable platform that can directly improve farmers’ livelihoods.\n\nWhat We Learned:\n- Applying machine learning and computer vision in agriculture.\n- Preprocessing and augmenting image data for stronger ML performance.\n- Designing accessible UIs for rural and resource-limited use cases.\n- Team collaboration, Git conflict resolution, and project scaling.\n\nWhat’s Next:\n- Expand the model to detect more crop diseases and pests beyond rice.\n- Improve UI/UX for ease of use.\n- Integrate with weather forecasting and pest management tools.\n- Provide treatment recommendations with real-time updates.\n- Scale support for high-resolution images and larger global datasets.\n\nPaddyPlantPrognosis demonstrates how technology can empower farmers, reduce crop loss, and strengthen food security on a global scale.",
    "github": "https://github.com/IdkwhatImD0ing/PaddyPlantPrognosis",
    "demo": "https://youtu.be/M_xLQeglzgU"
  },
  {
    "id": "cash-prize-bounty",
    "name": "Volunteer Hub",
    "summary": "Volunteer Hub is a platform designed to broaden the scope of volunteering by offering diverse roles beyond traditional technical positions. It allows volunteers to find meaningful opportunities that align with their unique skills and interests through a structured system. By utilizing tailored forms and a dedicated opportunities page, the platform ensures that volunteers can easily sign up for roles and receive notifications about tasks that match their capabilities. This approach not only empowers nonprofits with a wider range of support but also enhances the overall impact of volunteer contributions.\n\nThe platform employs a tag-based data architecture to effectively match volunteers with suitable projects. This system maps attributes such as skills, interests, and tech stack experience to the specific requirements of nonprofit projects. Volunteer Hub was recognized for its innovative approach by winning 3rd place at Opportunity Hack 2022. The project continues to evolve, with plans to expand its matching framework and introduce new features such as automated role suggestions and enhanced reporting tools for nonprofits.",
    "details": "Overview:\nVolunteer Hub addresses the challenge of limited roles in existing volunteer systems, which traditionally only recognize Hackers and Mentors. It expands opportunities by introducing a structured way for volunteers from diverse backgrounds to find meaningful work aligned with their skills and interests.\n\nTask & Solution:\n- For project-specific roles, volunteers can fill out tailored forms indicating skills, interests, and availability.\n- A dedicated volunteer opportunities page introduces general tasks, enabling sign-ups and notifications.\n- Volunteers are onboarded into Slack and given autonomy to contribute meaningfully.\n\nArchitecture:\nThe system employs a tag-based data architecture to match volunteers with projects. Attributes are mapped across three categories:\n1. Skills ↔ Skill Requirements\n2. Interests ↔ Project Context Themes\n3. Tech Stack Experience ↔ Tech Stack Demands\n\nThis ensures nonprofits can quickly identify well-matched volunteers while volunteers can discover opportunities aligned with their expertise and passions.\n\nImpact:\nVolunteer Hub creates a more inclusive and efficient ecosystem for matching volunteers to projects. By expanding engagement beyond technical roles, it empowers nonprofits with broader support and increases the overall impact of the platform.\n\nAccomplishments:\n- Developed an innovative volunteer matching architecture using tag mapping.\n- Expanded engagement opportunities beyond engineering.\n- Built a scalable solution that nonprofits can use to onboard diverse volunteers efficiently.\n\nWhat’s Next:\n- Expand the tag-based matching framework to support more attributes.\n- Add automated role suggestions for new volunteers.\n- Enhance reporting tools for nonprofits to track volunteer contributions and impact.",
    "github": "https://github.com/IdkwhatImD0ing/frontend-ohack.dev",
    "demo": "https://youtu.be/D8iHwCP37Fk"
  },
  {
    "id": "pool-party-3icj7e",
    "name": "Pool Party",
    "summary": "Pool Party is a mobile-first application designed to streamline the process of organizing carpools for groups traveling to the same destination. It allows users to create a carpool for a trip and join either as a driver or a passenger. Drivers can view and select from available passengers, while passengers can browse and choose drivers they prefer to ride with, ensuring a more coordinated and efficient travel experience.\n\nThe project was built using React.js for the frontend, with JavaScript for scripting, and Express.js for the backend. Material UI was utilized for design, and the app was deployed on Hop.io with real-time database integration. Pool Party was recognized at GraceHacks, winning the award for Best Mobile, highlighting its effective user interface and mobile-first approach.",
    "details": "Overview:\nPool Party addresses the common challenge of organizing transportation for groups heading to the same destination. The app makes it easier to coordinate carpools by letting users join as drivers or passengers and choose their preferred match.\n\nWhat It Does:\n- Users can create a pool for a trip and join as a driver or passenger.\n- Drivers can view available passengers and decide who to pick up.\n- Passengers can browse drivers and select who they want to ride with.\n- Built to be mobile-first, but accessible on all platforms.\n\nHow It Was Built:\n- React.js for the frontend\n- JavaScript for scripting\n- Express.js for the backend\n- Material UI for design\n- Hop.io for deployment\n- Real-time database integration\n\nChallenges:\n- Learning and applying Material UI effectively\n- Some teammates were new to React and JavaScript\n- Styling edge cases\n- Handling version control and merge conflicts across four team members\n\nAccomplishments:\n- Built and deployed both frontend and backend on Hop.io\n- Designed a clean, minimalist UI\n- Implemented a real-time database\n- Completed most planned features within time constraints\n\nWhat We Learned:\n- JavaScript scripting and UI/UX design with React\n- Backend development using Express.js\n- Version control and collaboration with Git\n\nFuture Plans:\n- Improve UI/UX design\n- Explore React Native for mobile-first expansion\n- Add Firebase authentication for secure user management",
    "github": "https://github.com/IdkwhatImD0ing/PoolParty.git",
    "demo": "https://mypoolparty.tech"
  },
  {
    "id": "tetris-duels",
    "name": "Tetris Duels",
    "summary": "Tetris Duels reimagines the classic Tetris game with a competitive edge, offering both solo play and a multiplayer versus mode. Players can challenge friends and family by sharing match links, allowing for easy access to join or spectate games. The project maintains game state across players using Hop.io Channels, ensuring a seamless multiplayer experience.\n\nThis innovative project was developed using React for the frontend, leveraging useState, useRef, and React Router, while the backend was built with Express.js. The entire application is deployed on Hop.io Ignite, utilizing Hop.io Channels for client-server state management. Tetris Duels was submitted to the Funathon and received a Participation Prize, highlighting its creative approach to a classic game.",
    "details": "Overview:\nTetris Duels was inspired by competitive Tetris and created as an experiment to build a multiplayer version. It allows players to enjoy solo Tetris or compete against friends and family by sharing links to matches.\n\nWhat It Does:\n- Play Tetris solo or in versus mode\n- Generate and share links so friends can join or spectate matches\n- Maintain state across players using Hop.io Channels\n\nHow It Was Built:\n- Frontend: React (with useState, useRef, and React Router)\n- Backend: Express.js\n- Deployment: Hop.io Ignite\n- Hop.io Channels for client-server state management\n\nChallenges:\n- First time using React, Express.js, and Hop.io\n- Building both frontend and backend solo within hackathon time limits\n- Managing complex objects to handle versus link-sharing and state synchronization\n\nAccomplishments:\n- Successfully built and deployed a functional frontend and backend on Hop.io\n- Implemented versus mode with link sharing\n- Built a backend server for the first time\n- Completed two-thirds of the planned features\n\nWhat Was Learned:\n- React fundamentals: useState, useRef, React Router\n- Express.js for handling HTTP requests and responses\n- Basics of backend server development and deployment\n- Techniques for syncing client-server state\n\nFuture Plans:\n- Improve UI design\n- Add new modes like timed versus mode with extra controls\n- Switch to WebSockets for reduced latency\n- Explore advanced Hop.io features such as chat rooms, private channels, and custom games\n\nSetup Instructions:\n1. Install Node.js and npm\n2. Run 'npm install' to install dependencies\n3. Run 'npm start' to start the app\n4. Visit 'http://localhost:3000' to play locally",
    "github": "https://github.com/IdkwhatImD0ing/tetris-coop",
    "demo": "https://youtu.be/y8wPmVJEqlc"
  },
  {
    "id": "wonder-g6tym1",
    "name": "Wonder",
    "summary": "Wonder is a platform designed to raise awareness about endangered animals threatened by poaching, habitat destruction, and climate change. It educates users on these critical issues and encourages participation in conservation efforts. The platform offers features such as generating lists of endangered animals, providing educational content, and offering practical tips for contributing to conservation. Additionally, it utilizes Google Maps to help users locate nearby pet shops and includes multimedia resources to enhance learning.\n\nBuilt with a robust tech stack, Wonder's landing page employs Next.js, GitHub, and Vercel, while its Telegram bot is developed using Python. It leverages Natural Language Processing (NLP) for intelligent responses and integrates the Google Maps API for location-based features. Recognized for its impact, Wonder was submitted to Killabytez Hacks, EcoHacks, and WildHacks II, earning a Second Overall award for its effective promotion of conservation awareness.",
    "details": "Overview:\nWonder addresses the urgent need to raise awareness about endangered animals threatened by poaching, habitat destruction, and climate change. It aims to educate users and encourage participation in conservation efforts.\n\nWhat It Does:\n- Generates lists of endangered animals with YouTube videos\n- Provides educational information about endangered species\n- Uses Google Maps to find nearby pet shops\n- Offers tips on contributing to animal conservation\n- Includes an about section\n\nHow It Was Built:\n- Landing page: Next.js + GitHub + Vercel\n- Telegram bot: Python\n- Natural Language Processing (NLP) with prompt engineering for responses\n- Google Maps API for location-based features\n\nChallenges:\n- Integrating Google Maps API smoothly into the landing page\n\nAccomplishments:\n- Built a functional and educational Telegram bot in a short timeframe\n- Created a platform that promotes conservation awareness\n- Fostered strong teamwork and collaboration during the hackathon\n\nWhat Was Learned:\n- Developing Telegram bots\n- Implementing APIs and endpoints (Google Maps)\n- Building applications with Next.js\n- Increased understanding of endangered species and conservation issues\n\nFuture Plans:\n- Expand functionality into a full hub for endangered animal awareness\n- Add login/registration with personalized feeds (e.g., endangered animals near the user)\n- Broaden conservation-related educational content and tools",
    "github": "https://github.com/nightsailor/forests",
    "demo": "https://youtu.be/hMYypq6hvWQ"
  },
  {
    "id": "remotetrainer",
    "name": "RemoteTrainer",
    "summary": "RemoteTrainer is a fitness web application designed to assist students who find it challenging to visit the gym due to constraints like time, budget, or motivation. It enables users to perform workouts at home by leveraging available equipment and provides tools for tracking their progress to maintain motivation. The app features a robust exercise search function that filters by muscle group and equipment, offers detailed exercise information with GIF demonstrations, and tracks personal metrics such as BMI and body fat to help users monitor their fitness journey.\n\nBuilt with Next.js for the frontend and utilizing Firebase Firestore and Firebase Authentication for database management and user authentication, RemoteTrainer offers a seamless user experience enhanced by Material UI components. The application relies on separate APIs for exercise generation and BMI/body fat calculations, with Axios handling the API calls. Hosted on Cloudflare with a custom domain from Name.com, RemoteTrainer was recognized with the Innovation Prize (High School) at PeddieHacks 2022, highlighting its innovative approach to remote fitness training.",
    "details": "Overview:\nRemoteTrainer was built to help students who lack time, money, or motivation for gym workouts. It allows users to exercise from home using available equipment while tracking progress to stay motivated.\n\nWhat It Does:\n- Filters and searches exercises by muscle group and available equipment\n- Displays exercise details and GIF demonstrations\n- Tracks BMI and body fat as users update personal stats\n\nHow It Was Built:\n- Frontend: Next.js\n- Database & Authentication: Firebase Firestore & Firebase Authentication\n- UI: Material UI\n- APIs: Separate APIs for exercise generation and BMI/body fat calculation\n- Axios for API calls\n- Hosting: Cloudflare with a custom Name.com domain\n\nChallenges:\n- Adjusting after some team members were unable to participate\n- Managing time effectively while learning new libraries and tools\n\nAccomplishments:\n- Successfully developed and deployed the site\n- Built a multiselect tool for filtering by equipment and body parts\n- Integrated APIs to dynamically generate personalized exercise recommendations\n\nWhat Was Learned:\n- Advanced Material UI styling, themes, and components\n- Firebase APIs and handling asynchronous operations\n- Effective collaboration using branches, pull requests, and merge conflict resolution\n\nFuture Plans:\n- Add progress graphs\n- Introduce features such as workout favoriting, playlists, streaks, and daily notifications\n- Enhance styling, security, and overall user experience",
    "github": "https://github.com/IdkwhatImD0ing/ExerciseTracker",
    "demo": "https://youtu.be/vL-X-1XrIN0"
  },
  {
    "id": "makemelunch",
    "name": "MakeMeLunch",
    "summary": "MakeMeLunch is a web application designed to help users efficiently manage their kitchen inventory and discover new recipes using available ingredients. By tracking stored ingredients and suggesting relevant recipes, the app aims to reduce food waste and enhance meal variety. Users can create personalized accounts to maintain a digital inventory of their kitchen supplies, ensuring that ingredients are used before they spoil and meals remain interesting.\n\nThe app is built using Next.js and JavaScript, with Firebase Authentication and Firestore Realtime Database handling user data and authentication. It leverages the Spoonacular API for accessing ingredient and recipe data, with Axios managing API requests. MakeMeLunch is hosted on Vercel, ensuring a seamless deployment experience. This project was submitted to the **Tech Optimum Hacks 2022**, showcasing its innovative approach to addressing common kitchen challenges.",
    "details": "Overview:\nMakeMeLunch was inspired by the common issues of forgetting ingredients until they spoil and losing interest in meals due to repetition. The app helps users track their kitchen inventory and discover new recipes using those ingredients.\n\nWhat It Does:\n- Allows users to create an account\n- Stores a personalized list of kitchen ingredients in a database\n- Displays relevant recipes based on stored ingredients\n\nHow It Was Built:\n- Framework: Next.js\n- Language: JavaScript\n- API: Spoonacular API for ingredient and recipe data\n- Authentication & Database: Firebase Authentication & Firestore Realtime Database\n- Hosting: Vercel\n- Axios for API handling\n\nChallenges:\n- Learning Next.js, JavaScript, HTML, and React within one day\n- Handling API calls and parsing in Next.js\n- Fast refresh issues requiring restarts\n- Integrating Firebase with Spoonacular API under tight time constraints\n\nAccomplishments:\n- Implemented Firebase Authentication linked with Firestore\n- Enabled successful data transfer between two distinct APIs\n- Integrated Spoonacular API calls via Axios\n- Built both backend and frontend within a single day\n\nWhat Was Learned:\n- JavaScript scripting\n- HTML and CSS formatting in React\n- API calls and handling with Axios\n\nKnown Problems:\n- Ingredient view crashes if no ingredients are added\n- Security flaws in environment variables and database rules\n\nFuture Plans:\n- Add filters for recipe search (e.g., calories)\n- UI improvements\n- Display healthier ingredient substitutions\n- Improve security measures\n- Handle edge cases more effectively",
    "github": "https://github.com/IdkwhatImD0ing/MakeMeLunch",
    "demo": "https://youtu.be/SmRpW295Xkk"
  },
  {
    "id": "asl-transcription",
    "name": "ASL Transcription",
    "summary": "**Summary:**\n\nASL Transcription is an innovative augmented reality tool that translates American Sign Language into text in real time. It empowers content creators, particularly those who are deaf or hard of hearing, to produce TikTok and YouTube videos without the need for manual captioning, thus facilitating greater creative expression. Moreover, this tool serves as an educational resource for those looking to improve their ASL speed and accuracy, enhancing both learning and communication.\n\nBuilt using Lens Studio and JavaScript, the project employs a custom machine learning model developed through transfer learning and image augmentation. The model leverages the TensorFlow Hub's MobileNetV2 pretrained on ImageNet1k and fine-tunes it with an ASL dataset. Despite initial challenges with importing models and dataset limitations, the project achieved a 94% validation accuracy. Submitted to SnapAR projects, this tool marks a significant advancement in AR-based sign language transcription.",
    "details": "Overview:\nASL Transcription was inspired by a deaf friend who wanted to create TikTok and YouTube content without the hassle of manually adding captions. This tool enables real-time ASL transcription, allowing creators to express themselves freely, and it can also serve as a learning tool for practicing ASL speed and accuracy.\n\nWhat It Does:\n- Uses Machine Learning to classify hand motions into ASL in real time\n- Displays transcribed text in augmented reality\n- Provides high accuracy predictions\n- Allows toggling live predictions on/off with a screen tap\n\nHow It Was Built:\n- Lens Studio\n- JavaScript\n- Custom model created using transfer learning and image augmentation\n- Tensorflow Hub MobileNetV2 pretrained model (ImageNet1k)\n- American Sign Language Dataset by David Lee (via Roboflow)\n- Tensorflow & Keras API for fine-tuning\n- Achieved 94% validation accuracy\n\nChallenges:\n- MobileNet and EfficientNet models initially failed to import into Lens Studio\n- Lens Studio documentation was confusing and prone to crashes\n- First dataset was not official ASL, resulting in poor real-world accuracy\n- Small dataset required heavy image augmentation; some hand poses remained hard to classify\n\nAccomplishments:\n- Successfully trained and deployed a ML model in Lens Studio\n- Expanded dataset using advanced augmentation techniques\n- First ML model deployment in a new AR environment\n- Leveraged hand tracking for precise input and reduced erroneous predictions\n\nWhat Was Learned:\n- Advanced image augmentation techniques\n- Deeper Tensorflow/Keras usage, including .onnx and TFLite formats\n- Lens Studio development and JavaScript scripting\n- Greater understanding of ASL hand poses and recognition challenges\n\nKnown Problems:\n- Small dataset limits model accuracy\n- Similar letters (A/S/E, M/N/V) confuse the model\n- Movements for J and Z are not reliably recognized\n\nFuture Plans:\n- Retrain with a larger dataset for improved accuracy\n- Convert Python Word Ninja to JavaScript for probabilistic word splitting (currently unfeasible due to prediction errors)",
    "github": "https://github.com/IdkwhatImD0ing/SnapAr",
    "demo": "https://youtu.be/Nat5vQGsyxA"
  },
  {
    "id": "covinet",
    "name": "CoviNet",
    "summary": "CoviNet is a versatile networking and support application designed to connect individuals affected by Covid-19. Initially conceived as a roommate matching platform for Covid-positive individuals, it has evolved to offer broader networking capabilities. The app allows users to find and connect with nearby individuals in similar situations, receive verified Covid-related news updates, and access educational videos. Additionally, CoviNet assists users in tracking their Covid test records and locating nearby testing centers, fostering community support and information sharing during the pandemic.\n\nThe app is built using Dart and Flutter, and leverages APIs such as YouTube, Firebase, Google Cloud, and the News API to provide its features. CoviNet also utilizes a synthetic dataset from Gretel for testing purposes. Despite challenges with database integration, the team successfully implemented Firebase for real-time data storage and messaging. The project was recognized at CruzHacks 2022, where it won the Sponsor Award from QB3 @ UCSC, highlighting its innovative approach and impact.",
    "details": "Overview:\nCoviNet was originally inspired by the idea of AirBnB-style roommate matching for Covid-positive individuals, but evolved into a broader networking and support app. It allows users to connect with others nearby, stay informed, and access helpful Covid-related resources.\n\nWhat It Does:\n- Connects users with nearby people who also have Covid to share support and make friends\n- Provides verified Covid-related news updates\n- Suggests educational and informational videos\n- Tracks users’ Covid test report records\n- Helps users find nearby Covid testing centers\n\nHow It Was Built:\n- Dart\n- Flutter\n- APIs: YouTube, Firebase, Google Cloud, News API\n- Synthetic Dataset from Gretel\n\nChallenges:\n- Database access issues with Firebase\n- Implementing Firebase real-time database methods\n- Integrating multiple API links efficiently\n\nAccomplishments:\n- Successfully integrated Firebase for data storage and messaging\n- Added Google Maps and location storage\n- Built messaging features with Firebase\n- Implemented YouTube and News APIs\n\nWhat Was Learned:\n- Mobile app development using Flutter\n- How to integrate and link multiple APIs in a single application\n- Storing and managing user data securely in databases\n- Implementing Firebase Firestore real-time database\n- Using Gretel to generate synthetic datasets for testing\n\nFuture Plans:\n- Improve dataset accuracy\n- Enhance user interface and design\n- Strengthen security for sensitive health data",
    "github": "https://github.com/Arjun-Mishra-312/covinet",
    "demo": "https://youtu.be/PJ6XaJ0rFDQ"
  }
]
