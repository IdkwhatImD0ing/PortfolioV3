[
  {
    "id": "talktuahbank",
    "name": "TalkTuahBank",
    "summary": "TalkTuahBank is a voice-based banking assistant designed to make financial services more accessible to underserved populations. By enabling users to interact through simple phone calls, it removes barriers like internet access, smartphone ownership, or digital literacy. Users can check balances, transfer funds, and pay bills using natural voice commands in multiple languages, ensuring inclusivity for communities often overlooked by traditional banks. Built with Retell AI, OpenAI Swarm, and Pinata (IPFS), alongside a modern Next.js/Tailwind CSS admin dashboard, TalkTuahBank earned top recognition at HackUTD 2024: Ripple Effect, winning both the General Category and the Goldman Sachs Award for its innovative approach to democratizing financial technology.",
    "details": "Inspiration\nAccess to banking services is a fundamental aspect of economic participation, yet over 1.7 billion adults worldwide remain unbanked. Traditional banking solutions often require internet access, digital literacy, or proximity to physical branches—barriers that leave underserved populations without essential financial tools. Inspired by the need for inclusivity and the power of conversational AI, we envisioned TalkTuahBank, a voice-based banking assistant that democratizes financial services by making them accessible to everyone, regardless of their technological capabilities or geographical location.\n\nWhat It Does\nTalkTuahBank revolutionizes the banking experience by offering a fully conversational AI assistant accessible through simple phone calls. Users can effortlessly manage their finances by performing tasks such as checking account balances, transferring funds, and paying bills using natural voice commands. Key features include:\n\n- Voice-Activated Services: Conduct banking transactions hands-free via phone.\n- Multi-Language Support: Interact in multiple languages and dialects to accommodate diverse users.\n- Accessibility: Operates on any phone without the need for internet or smartphones.\n- AI Assistance: Receive personalized financial advice and support through intelligent interactions.\n\nHow We Built It\n- **Conversational AI**: Retell AI for natural language understanding and generation, enabling smooth voice interactions.\n- **Multi-Agent Framework**: OpenAI Swarm for orchestration of dialogues and backend processes.\n- **Secure Storage**: Pinata (IPFS) for decentralized, secure transaction and user data storage.\n- **Admin Dashboard**: Next.js, Tailwind CSS, ShadCN UI, and Aeternity for monitoring, management, and support.\n- **Telephony Integration**: Retell API for handling and processing calls.\n\nChallenges\n- Ensuring robust **security** for sensitive financial data.\n- Managing the **integration** of multiple advanced technologies.\n- Building a smooth, natural **voice user experience** across languages.\n\nAccomplishments\n- Delivered a **functional prototype** enabling essential banking tasks via voice.\n- Enabled **multi-language support**, broadening accessibility.\n- Implemented **secure decentralized storage** with Pinata.\n- Developed a **comprehensive admin dashboard** for user and transaction management.\n- Earned recognition at **HackUTD 2024: Ripple Effect**, winning both the **General Category** and the **Goldman Sachs Award** for innovation and inclusivity in financial technology.\n\nWhat We Learned\n- Nuances of **voice technology** and user-centered design.\n- Importance of **layered security** in financial systems.\n- Strategies for **integrating diverse technologies** into a seamless solution.\n- Value of **inclusive design** for underserved communities.\n\nWhat’s Next\n- Add more **languages and dialects**.\n- Introduce **fraud detection** with machine learning.\n- Provide **financial literacy modules** for users.\n- Develop a **complementary mobile app** for smartphone users.\n- Build partnerships with **local banks and NGOs** to expand reach.\n- Continuously enhance **security protocols** to protect user data.\n",
    "github": "https://github.com/aurelisajuan/TalkTuahBank",
    "demo": "https://youtu.be/YsH_z1azXSA"
  },
  {
    "id": "dispatch-ai",
    "name": "Dispatch AI",
    "summary": "Dispatch AI is an empathetic AI-powered system designed to transform how emergency calls are handled. It centralizes incoming 911 calls into one platform, categorizes them by severity, and extracts vital details such as location, timing, and caller emotions. Dispatchers stay in full control, making the final decisions while using AI recommendations to ensure both efficiency and human oversight. Built with Next.js, TailwindCSS, Framer Motion, and Leaflet on the frontend, and Python with Twilio, Hume, Retell, and Google Maps APIs on the backend, Dispatch AI integrates a fine-tuned Mistral model optimized with Intel’s Developer Cloud and Intel Extension for PyTorch, reducing inference time by 80%. Recognized at the UC Berkeley AI Hackathon 2024, Dispatch AI won the **Grand Prize ($25,000 Berkeley SkyDeck Fund investment + Golden Ticket to SkyDeck Pad-13)**, **AI For Good Award by AIC**, and **Best Use of Intel AI**, underscoring its innovation and impact in emergency response technology.",
    "details": "Inspiration\nImagine: A major earthquake hits. Thousands call 911 simultaneously. In the call center, a handful of operators face an impossible task. Every line is ringing. Every second counts. There aren't enough people to answer every call. This isn't just hypothetical. It's a real risk in today's emergency services. A startling 82% of emergency call centers are understaffed, pushed to their limits by non-stop demands. During crises, when seconds mean lives, staffing shortages threaten our ability to mitigate emergencies.\n\nWhat it does\nDispatchAI reimagines emergency response with an empathetic AI-powered system. It leverages advanced technologies to enhance the 911 call experience, providing intelligent, emotion-aware assistance to both callers and dispatchers.\n\nEmergency calls are aggregated onto a single platform, and filtered based on severity. Critical details such as location, time of emergency, and caller's emotions are collected from the live call. These details are leveraged to recommend actions, such as dispatching an ambulance to a scene.\n\nOur human-in-the-loop system enforces control of human operators is always put at the forefront. Dispatchers make the final say on all recommended actions, ensuring that no AI system stands alone.\n\nHow we built it\nWe developed DispatchAI using a comprehensive tech stack:\n\nFrontend:\n- Next.js with React for a responsive and dynamic user interface\n- TailwindCSS and Shadcn for efficient, customizable styling\n- Framer Motion for smooth animations\n- Leaflet for interactive maps\n\nBackend:\n- Python for server-side logic\n- Twilio for handling calls\n- Hume and Hume's EVI for emotion detection and understanding\n- Retell for implementing a voice agent\n- Google Maps geocoding API and Street View for location services\n- Custom-finetuned Mistral model using our proprietary 911 call dataset\n- Intel Dev Cloud for model fine-tuning and improved inference\n\nChallenges we ran into\n- Curated a diverse 911 call dataset\n- Integrating multiple APIs and services seamlessly\n- Fine-tuning the Mistral model to understand and respond appropriately to emergency situations\n- Balancing empathy and efficiency in AI responses\n\nAccomplishments that we're proud of\n- Successfully fine-tuned a Mistral model for emergency response scenarios\n- Developed a custom 911 call dataset for training\n- Integrated emotion detection to provide more empathetic responses\n- Leveraged Intel Dev Cloud for live demonstrations and optimized performance\n- Achieved an 80% reduction in inference time using Intel® Extension for PyTorch (IPEX)\n- **Winner of UC Berkeley AI Hackathon 2024 Grand Prize**: $25,000 Berkeley SkyDeck Fund investment + Golden Ticket to SkyDeck Pad-13\n- **Winner of AI For Good Award** by Academic Innovation Catalyst (AIC)\n- **Winner of Best Use of Intel AI**\n\nInnovation\n- First empathetic, AI-powered dispatcher agent designed to support first responders during resource-constrained situations\n- Novel integration of emotion detection and AI assistance in emergency calls\n\nImpact\n- Addresses the 82% of understaffed call centers\n- Aims to reduce wait times in critical emergencies (e.g., Oakland’s 1+ minute 911 wait times)\n- Potential to save lives by ensuring every emergency call is answered promptly\n\nBonus Points\n- Open-sourced fine-tuned LLM on HuggingFace: https://huggingface.co/spikecodes/ai-911-operator\n- Published training dataset: https://huggingface.co/datasets/spikecodes/911-call-transcripts\n- Submitted to Powered By Intel LLM leaderboard: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n- Promoted project on Twitter (X) using #HackwithIntel: https://x.com/spikecodes/status/1804826856354725941\n\nWhat we learned\n- How to integrate multiple technologies into a cohesive system\n- The potential of AI to augment critical public services\n\nWhat's next for DispatchAI\n- Expand dataset with more diverse emergency scenarios\n- Collaborate with local emergency services for real-world testing\n- Explore future integrations\n",
    "github": "https://github.com/IdkwhatImD0ing/DispatchAI",
    "demo": "https://youtu.be/hdpdgxrilQM"
  },
  {
    "id": "teachme-3p7bw1",
    "name": "AdaptEd",
    "summary": "AdaptEd is an AI-powered education platform that transforms traditional lectures into dynamic, interactive conversations with a humanoid AI lecturer. Instead of passively receiving information, students engage in real-time dialogue where slides and content adapt to their responses. Built with Gemini 1.5 Pro for multi-source aggregation, Fetch.ai for task agents, Hume for emotion detection, and Intel Developer Cloud for fine-tuning and inference, AdaptEd personalizes the learning experience by dynamically generating slides, modifying content on the fly, and gauging student attention and confusion. Recognized at LA Hacks 2024 as the **Winner of the Google Company Challenge** and at the Intel Dev Cloud Hackathon for its innovation in multidimensional agentic systems, AdaptEd showcases how AI can bridge the tutoring gap and make personalized learning more accessible worldwide.",
    "details": "As a K-12 tutor for Kumon, one of our teammates realized the massive impact that personalized, 1:1 tutoring had on students. However, not everyone has access to enrichment programs, creating a compounding limitation that affects success in future endeavors.\n\nOut of 16 million university students in the United States, 50% of them are falling behind due to static and one-sided teaching without personalized engagement. At the same time, less than 3% of students have access to quality tutoring programs.\n\nAdaptEd turns the educational paradigm on its head: instead of students adapting to the system, our AI lecturer adapts to students. Through real-time conversation, verbal/visual content changes dynamically.\n\nWhat it does\nAdaptEd turns lectures into conversations with a live humanoid lecturer that a user can speak to directly. Corresponding lecture slides are generated live based on the user's verbal response, creating an adaptive learning experience.\n\nIt performs 3 main functions:\n- Responsive AI: Users can hold natural conversations with the lecturer.\n- Dynamic Content: Slideshow and whiteboard adapt in real time based on verbal response.\n- Emotion Detection: Measures the user's attention and confusion.\n\nHow we built it\nMulti-source Aggregation Pipeline:\n- Gemini 1.5 Pro: Huge context length allows us to aggregate data from audio (YouTube videos, podcasts), video (lectures), and massive text sources (textbooks, Wikipedia).\n\nAction-Taking Dynamic Agent:\n- Interruption and End-of-Turn Detection allows for natural conversation.\n- Real-time action-taking lets the agent modify slides/whiteboard dynamically.\n\nIntel Developer Cloud:\n- Used to fine-tune an open-source model for more accurate responses.\n- Served as a real-time inference engine for fast generation.\n\nFetch.ai:\n- Agents perform side tasks without interrupting the main speaker.\n- Proxy queries and callbacks allow seamless multitasking while lectures continue.\n\nOther Integrations:\n- Google Search: Adds images and dynamic content to slides.\n- MongoDB: Stores lectures for later review.\n- Auth0: Saves user statistics and progress.\n- Hume: Real-time emotion detection for tailored responses.\n\nChallenges we ran into\n- Steep learning curve with Fetch.AI and Intel Developer Cloud (IDC).\n- Sparse documentation and hardware limitations restricted full model performance.\n\nAccomplishments that we're proud of\n- Functional multidimensional agentic system integrating multiple AI agents.\n- Successful fine-tuning of models on IDC for real-time education use.\n- **Winner of the Google Company Challenge at LA Hacks 2024.**\n- Recognition at the Intel Dev Cloud Hackathon for multidimensional agentic system design.\n\nWhat we learned\n- How to design agentic, multi-layered education systems.\n- Importance of leveraging example notebooks and platform-specific tools.\n- Trade-offs of hardware vs. model complexity in real-time systems.\n\nWhat's next for AdaptEd\n- Add more agents for expanded capabilities.\n- Integrate with Google Docs for collaborative learning.\n- Personal history/statistics visualizer for deeper personalization.\n- Enhance scaling to reach broader classrooms and institutions.",
    "github": "https://github.com/IdkwhatImD0ing/AdaptEd/pulls",
    "demo": "https://youtu.be/8o1YJUFBcAw"
  },
  {
    "id": "courtvision-gtui7w",
    "name": "CourtVision",
    "summary": "CourtVision is an interactive sports analysis tool that transforms 2D game footage into fully immersive 3D replays using Gaussian Splatting. By allowing athletes to revisit plays from multiple perspectives, it captures subtle details such as posture, foot placement, and timing that standard film often misses. The system integrates real-time voice-controlled feedback, enabling athletes to ask questions mid-replay and receive tailored insights, making performance analysis more dynamic and interactive. Built with Video Gaussian reconstruction, a multimodal LLM for interpreting posture and intent, and Vapi’s Voice API for natural interactions, CourtVision merges advanced AI with sports training. The platform supports immersive desktop and VR workflows, while providing remote coaching capabilities through shared 3D walkthroughs. Recognized at the **UC Berkeley AI Hackathon 2025** for advancing AI-driven sports analytics, CourtVision highlights how immersive replays and AI insights can deliver both immediate tactical feedback and long-term performance improvement.",
    "details": "Inspiration / Problem:\nAthletes often rely on film to improve, but standard 2D footage is flat, fixed, and misses critical details like body alignment and reaction timing. CourtVision was built to address these gaps.\n\nWhat It Does:\n- Converts 2D game clips into accurate 3D Gaussian Splat replays.\n- Allows athletes to rotate, zoom, slow down, and view plays from multiple perspectives.\n- Integrates with a multimodal LLM to interpret posture, intent, and motion.\n- Provides real-time voice-controlled Q&A using Vapi’s Voice API.\n\nKey Features:\n- **Skill Review:** Study real performance rather than limited camera angles.\n- **Posture & Form:** Detect back rounding, missteps, or poor landings.\n- **Decision Analysis:** Explore alternative moves and identify missed opportunities.\n- **Trainer Feedback:** Enables remote coaching with shared 3D walkthroughs.\n\nHow It Was Built:\n- **Video Gaussians:** 3D scene reconstruction from 2D footage with time consistency.\n- **Multimodal LLM:** Frame-by-frame interpretation of posture and intent.\n- **Vapi Voice API:** Natural voice interactions for immediate performance insights.\n\nAccomplishments:\n- Delivered immersive 3D replay technology that enhances player self-review.\n- Created a voice-first interface for intuitive tactical feedback.\n- Designed athlete-focused workflows for both desktop and VR use.\n- **Recognized at the UC Berkeley AI Hackathon 2025** for innovation in sports analytics.\n\nChallenges:\n- Ensuring accurate time-synced Gaussian splats.\n- Designing a natural feedback loop between athletes and AI.\n\nWhat We Learned:\n- Advanced integration of Gaussian reconstruction with real-time feedback systems.\n- Practical applications of multimodal AI in sports contexts.\n\nWhat’s Next:\n- Shared VR sessions for team film review.\n- Automated body tracking with statistical overlays.\n- AI-driven “What if” simulation tools for alternate decision testing.\n- Voice-enabled coach-athlete interactions during replays.",
    "github": "https://github.com/aurelisajuan/ai2025.git",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/502/448/datas/gallery.jpg"
  },
  {
    "id": "sentinelai-dec0jp",
    "name": "SentinelAI",
    "summary": "SentinelAI is a real-time public safety orchestration system designed for crowded environments such as schools, malls, and stadiums. It continuously monitors live audio streams to detect signs of chaos—including screams, alarms, or glass breaking—and triggers AI-driven responses in under a second. By integrating with intercoms, phones, and smart building systems, SentinelAI provides calm, context-aware evacuation or lockdown instructions while keeping human operators in control. Built with fine-tuned Whisper and YAMNet models for panic detection, vLLM paired with Llama 3.3 for streaming responses, and Twilio/Retell for real-time voice interaction, SentinelAI consistently achieved sub-500 ms detection speeds. Recognized at **HackDavis 2025 as the Winner of Best AI/ML Hack**, SentinelAI showcases how AI can transform buildings into proactive safety partners.",
    "details": "Inspiration / Problem:\nIn crisis scenarios such as shootings, fires, or terrorist threats, panic spreads within seconds, but coordinated emergency responses often take 9 minutes or more. Over 70% of casualties occur in the first 5 minutes, yet fewer than 5% of commercial spaces have real-time detection or orchestration systems. SentinelAI was created to close this gap and transform buildings into proactive safety partners.\n\nWhat It Does:\n- Monitors live audio streams to detect signs of chaos (screams, alarms, glass breaking).\n- Uses an LLM-powered Superintendent to provide calm, spoken evacuation or lockdown instructions in under one second.\n- Integrates with smart doors, alarms, signage, and lighting systems.\n- Coordinates responses across multiple zones with human-in-the-loop oversight via a 3D dashboard.\n\nKey Features:\n- **Panic Detection Engine:** Fine-tuned Whisper + YAMNet for real-time recognition of panic audio.\n- **Sentinel Superintendent:** Streaming vLLM + Llama 3.3 for instant guidance.\n- **Building Integration:** Direct control over IoT-connected alarms, locks, and signage.\n- **Multi-Zone Awareness:** Tailored responses for different areas simultaneously.\n- **Command Dashboard:** 3D visualization for security teams built with Next.js + Three.js.\n\nChallenges:\n- Achieving sub-second latency without compromising accuracy.\n- Limited real-world panic datasets for fine-tuning.\n- Simulating IoT integrations before physical hardware deployment.\n- Maintaining synchronized state across high-load environments with Supabase.\n\nAccomplishments:\n- Consistently reached <500 ms detection-to-response pipeline.\n- Built the first streaming LLM superintendent for evacuation and lockdown scenarios.\n- Integrated Twilio & Retell for real-time, interruptible phone-call guidance.\n- Delivered an interactive 3D building safety dashboard.\n- **Winner of Best AI/ML Hack at HackDavis 2025.**\n\nWhat We Learned:\n- Multi-modal pipelines require tight optimization for trust and reliability.\n- Human oversight features are critical to user adoption.\n- Edge-first design lowers latency and improves resilience.\n- Simulation environments accelerate hardware integration.\n\nWhat’s Next:\n- Heatmap and camera fusion for combined audio-visual incident detection.\n- On-device fail-safes for offline resilience.\n- Real-time multilingual instruction support.\n- A mobile companion app for alerts and remote overrides.\n\nHook:\n“What if every building had its own Jarvis during a crisis?” SentinelAI acts instantly, guiding people through danger before human responders can reach them.",
    "github": "https://github.com/Christopher-Chhim/HackDavis",
    "demo": "https://youtu.be/qr3pMPU8lFY"
  },
  {
    "id": "slugmeditate",
    "name": "SlugMeditate",
    "summary": "SlugMeditate is a VR meditation experience that transforms user thoughts into immersive 3D environments. A calming idea such as 'a peaceful forest at dusk' is generated into images, animated into video, and reconstructed into a VR scene enhanced with ambient soundscapes. The experience provides users with personalized meditative spaces that promote mindfulness, reduce stress, and create a sense of emotional well-being. Built with Google AI Gemini tools (Imagen 3, Veo 2), Gaussian Splatting for 3D scene mapping, Niantic Studio WebXR for browser-based VR, and Google MusicFX for generative audio, SlugMeditate combines creativity and wellness in a single pipeline. Recognized at **CruzHacks 2025**, it won the **MLH Best Use of Gemini API Award** and the **Niantic Studio WebXR Track Award**, showcasing how generative AI and spatial computing can reimagine mindfulness as an immersive and deeply personal experience.",
    "details": "Inspiration / Problem:\nMany students and individuals struggle with stress, burnout, and mental fatigue. Meditation is powerful, but traditional methods can feel repetitive or inaccessible. SlugMeditate reimagines mindfulness by turning any calming thought into a personalized VR escape — blending creativity, technology, and wellness.\n\nWhat It Does:\n- Converts a text prompt (e.g., 'a peaceful forest at dusk') into an immersive VR environment.\n- Generates images using Google Imagen 3 and animates them with Veo 2.\n- Reconstructs video into a navigable 3D world with Gaussian Splatting.\n- Renders the space in-browser via Niantic Studio WebXR.\n- Complements visuals with ambient music created through Google MusicFX.\n\nKey Features:\n- **Text-to-Image:** Rich visuals powered by Imagen 3.\n- **Image-to-Video:** Cinematic video loops with Veo 2.\n- **3D Scene Mapping:** Gaussian Splatting for VR navigation.\n- **Browser VR Rendering:** Seamless WebXR deployment using Niantic Studio.\n- **Generative Audio:** Personalized ambient music via MusicFX.\n\nChallenges:\n- Limited documentation and trial-and-error with Google AI tools.\n- Hardware constraints in Gaussian Splatting workflows.\n- Balancing camera motion for accurate 3D reconstruction.\n- Learning Niantic Studio WebXR from scratch.\n- Unstable Wi-Fi at the hackathon when generating AI assets.\n\nAccomplishments:\n- Built a complete text-to-VR pipeline within 48 hours.\n- Combined cutting-edge generative AI with VR rendering in real time.\n- Delivered a creative, emotionally resonant meditation experience.\n- Quickly mastered Imagen 3, Veo 2, and Gaussian Splatting.\n- **Winner of the MLH Best Use of Gemini API Award at CruzHacks 2025.**\n- **Winner of the Niantic Studio WebXR Track Award at CruzHacks 2025.**\n\nWhat We Learned:\n- Practical use of Google AI Gemini for generative workflows.\n- How to merge AI media generation with spatial computing.\n- Rapid prototyping and team collaboration under pressure.\n\nWhat’s Next:\n- **Interactivity:** Enable users to move and interact in VR spaces.\n- **Customization:** Let users adjust music, ambient sounds, or guided meditations.\n- **Optimization:** Reduce generation and rendering times.\n- **Community Gallery:** A platform to share meditative creations.\n\nHook:\n“Imagine turning your thoughts into a world you can step into.” SlugMeditate makes mindfulness deeply personal, immersive, and creative.",
    "github": "https://github.com/briankhoi/slugmeditate",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/374/704/datas/gallery.jpg"
  },
  {
    "id": "secway",
    "name": "SecWay",
    "summary": "SecWay is an AI-powered Chrome extension designed to simplify cybersecurity for everyday users. It continuously monitors websites and extensions to highlight risky permissions, suspicious data requests, and potential phishing threats. With features like real-time privacy scanning, visual risk ratings, and one-click permission cleanup, it empowers users to stay safe online without needing technical expertise. Built with Gemini 2.5-Turbo for conversational guidance, Google Safe Browsing and PhishTank APIs for risk assessment, and a modular architecture for cross-browser expansion, SecWay was recognized at **DiamondHacks 2025** for its mission to make digital privacy accessible, educational, and proactive.",
    "details": "Inspiration\nToday’s web is full of hidden threats—phishing scams, data leaks, and overreaching permissions. But most users don’t read the fine print. In fact, 62% never change app or website permissions after first use. This puts millions at risk, especially those who aren't tech-savvy.\n\nInspired by the need for simple, accessible cybersecurity, SecWay is an AI-powered Chrome extension that demystifies digital privacy.\n\nWhat It Does\nSecWay acts as a lightweight, always-on browser companion that helps users see and understand when websites or extensions are asking for too much. Key features include:\n\n- **Real-Time Privacy Scanner:** Detects risky permissions, overreaching data requests, and suspicious website behavior.\n- **AI-Powered Privacy Guidance:** Gemini 2.5-Turbo provides human-friendly explanations and smart recommendations—no technical knowledge needed.\n- **Educational Nudges:** “Did You Know?” style insights explain why a permission could be dangerous, tailored to user behavior and comfort level.\n- **Risk Indicators:** Simple visual privacy ratings appear as you browse—green (safe), orange (review), red (danger).\n- **Secure Me Button:** One-click cleanup lets users instantly apply safe permission defaults suggested by AI.\n\nChallenges We Ran Into\n- Balancing simplicity and depth—making the experience clear for non-tech users while still delivering powerful analysis.\n- Designing a local-only architecture without backend data collection.\n- Tuning phishing detection models to minimize false positives.\n\nAccomplishments We're Proud Of\n- Built a **privacy-first architecture**: no backend, no data collection.\n- Developed a **modular extension design** for easy expansion to Firefox and Edge.\n- Created a **Gemini 2.5-Turbo conversational UX** that educates while protecting.\n- Implemented **one-click AI-powered protection** for immediate safety.\n- **Recognized at DiamondHacks 2025** for advancing accessible cybersecurity.\n\nWhat We Learned\n- The importance of user-centric security design over fear-based tactics.\n- Practical applications of conversational AI in browser extensions.\n- How to combine APIs like Google Safe Browsing and PhishTank for real-time risk intelligence.\n\nWhat's Next for SecWay\n- Extend support to **Firefox and Edge**.\n- Build outreach programs with **schools and libraries** for privacy awareness.\n- Add a **community-driven scam detection leaderboard**.\n- Develop **mobile browser tips** for smartphones.\n- Introduce a **Parental Control Mode** for families.\n- Enhance **threat intelligence** with continual model updates.",
    "github": "https://github.com/aurelisajuan/DiamondHacks",
    "demo": "https://youtu.be/s5A4NHD3QEo"
  },
  {
    "id": "weeee-i-love-reading-documentation",
    "name": "Vocalyze",
    "summary": "Vocalyze is a conversational AI assistant that streamlines complex banking processes such as credit and loan applications. Instead of manually filling out confusing forms, users can complete applications through natural phone conversations. The system automatically generates accurate forms from speech, clarifies financial terminology in real time, and provides a humanized customer support experience. It integrates Retell AI for phone interactions, OpenAI GPT-4 for intelligent dialogue, Letta for conversational flow management, and Supabase for real-time database syncing. Built to be interruptible, scalable, and future-ready for multilingual support, Vocalyze reduces errors and eases the burden on overwhelmed banking staff. Recognized at **HackMerced X**, Vocalyze won both the **Letta AI Award** and the **Business and Finance Award**, highlighting its innovation in conversational banking.",
    "details": "Inspiration / Problem:\nBanking applications are often tedious, confusing, and filled with jargon. Customers abandon forms or make costly mistakes, while bank staff face high workloads from repetitive inquiries. Vocalyze was created to simplify this process and make finance more accessible through natural conversation.\n\nWhat It Does:\n- **Conversational Banking:** Users talk through credit and loan applications over a phone call.\n- **Automatic Form Filling:** Speech is converted directly into accurate, completed forms.\n- **Real-Time Clarification:** Explains financial terms as users encounter them.\n- **Error Reduction:** Ensures fewer mistakes through guided, conversational input.\n\nHow It Works:\n- **Retell AI:** Handles voice call integration for smooth conversational experiences.\n- **OpenAI GPT-4:** Powers intelligent, context-aware responses and term explanations.\n- **Letta:** Manages dialogue state to ensure seamless conversation flow.\n- **Supabase:** Provides real-time database management for instant updates and storage.\n\nChallenges:\n- Outdated and incomplete documentation slowed development.\n- Integrating voice calls, real-time databases, and conversational agents required extensive troubleshooting.\n- Ensuring accuracy and low latency in real-time data syncing was critical.\n\nAccomplishments:\n- Built a fully functional conversational AI banking assistant from scratch.\n- Successfully integrated Retell AI, GPT-4, Letta, and Supabase into one cohesive system.\n- Delivered a demo-ready prototype showcasing real-world applicability.\n- **Winner of the Letta AI Award at HackMerced X.**\n- **Winner of the Business and Finance Award at HackMerced X.**\n\nDifferentiators:\n- **Interruptible Conversations:** Users can naturally cut in, creating a human-like flow.\n- **Instant Application Completion:** Eliminates manual form entry entirely.\n- **Scalable Multilingual Support:** Architecture allows for future expansion to multiple languages.\n\nWhat We Learned:\n- Best practices for combining stateful conversational AI with real-time voice calls.\n- Effective strategies for handling unreliable documentation.\n- Building resilient systems under time and integration constraints.\n\nWhat’s Next:\n- Implement multilingual support to expand financial inclusivity.\n- Partner with banks to pilot real-world deployments.\n- Add financial literacy features to empower users with knowledge during conversations.\n\nHook:\n\"Banking should be as simple as a conversation.\" Vocalyze brings that vision to life by removing complexity, reducing stress, and making finance more accessible.",
    "github": "https://github.com/IdkwhatImD0ing/idkwhatthisprojectis",
    "demo": "https://youtu.be/s8wF-xCPY04"
  },
  {
    "id": "tft-team-food-tactics",
    "name": "TFT: TeamFood Tactics",
    "summary": "TFT: TeamFood Tactics is an AI-powered sustainable food distribution platform designed to reduce waste and combat food insecurity. The system connects suppliers, such as bakeries with surplus goods, to individuals in need of specialized diets, offering recommendations through a simple voice-first interface accessible to anyone with a phone. By processing both voice and image inputs, it bridges the gap between food supply and demand in real time, ensuring that leftover food is redirected to those who need it most. Built with Next.js, Tailwind CSS, Python FastAPI, Retell AI, OpenAI, OpenAI Swarm, and Supabase, the project delivers scalable real-time distribution capabilities. Recognized at **SpartaHack X**, it won the **Sustainability – Green City Award**, underscoring its social and environmental impact.",
    "details": "Inspiration\n\"Americans waste about 60 million tons of food every year.\"\n\nFood insecurity and food waste are major challenges in our communities. As we saw local bakeries and food banks struggling with leftover food and limited resources, we were inspired to create a solution that not only reduces waste but also bridges the gap between surplus food and those in need. The idea was born from the desire to make sustainable food distribution accessible—even for those without smart devices—by leveraging a simple, voice-first interface.\n\nWhat it does\nTeamFoodTactics is an AI-powered sustainable food distribution platform that connects suppliers (like gluten-free bakeries with surplus bread) with users who need specialized diets (such as individuals needing gluten-free options). Users simply call a dedicated phone number to describe their dietary requirements or inventory details, and our system:\n\n- **For Suppliers:** Facilitates the donation of leftover food by allowing bakery owners to update available food through voice calls or image uploads.\n- **For Users:** Provides personalized recommendations for the nearest food source that meets their specific dietary needs.\n- **Behind the Scenes:** Uses advanced AI to process voice and image data, generate dynamic SQL queries, and match supply with demand in real-time.\n\nHow we built it\n- **Frontend:** Next.js, Tailwind CSS\n- **Backend:** Python, FastAPI\n- **APIs:** Retell AI, OpenAI, OpenAI Swarm\n- **Database:** Supabase\n\nChallenges we ran into\n- Voice Interface Integration: Ensuring a natural, seamless voice interaction over any phone, particularly for users without access to smartphones.\n- Dynamic Query Generation: Training AI models to generate accurate SQL queries based on variable user inputs.\n- Real-Time Data Processing: Managing and updating live inventory data from multiple suppliers, including OCR extraction from uploaded images.\n- Scalability: Building a system secure and scalable enough to handle unpredictable donation and distribution volumes.\n\nAccomplishments that we're proud of\n- Inclusive Design: Built a voice-first interface accessible to anyone with a phone.\n- AI-Driven Matching: Developed real-time AI-powered query generation to connect surplus food with those in need.\n- Seamless Multi-Component Integration: Combined telephony, OCR, geolocation, and database management into a unified platform.\n- Sustainability Impact: Reduced food waste while improving equitable food access.\n- **Winner of the Sustainability – Green City Award at SpartaHack X.**\n\nWhat we learned\n- How to integrate conversational AI, OCR, and databases in real-world contexts.\n- Agile problem solving for dynamic queries and live data streams.\n- Building secure, scalable systems with Supabase and API orchestration.\n\nWhat's next for TeamFoodTactics\n- **Expanded Language Support:** Add multilingual access to serve diverse communities.\n- **Mobile App Integration:** Complement voice-first access with an app for enhanced functionality.\n- **Predictive Analytics:** Use ML to forecast demand and optimize logistics.\n- **Enhanced Personalization:** Introduce dashboards, profiles, and sustainability metrics.\n- **Partnership Expansion:** Scale impact through collaboration with food banks, nonprofits, and local governments.",
    "github": "https://github.com/aurelisajuan/spartaHacks",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/249/428/datas/gallery.jpg"
  },
  {
    "id": "talktuahduck",
    "name": "TalkTuahDuck",
    "summary": "TalkTuahDuck is an interactive AI-powered study companion designed to make reviewing messy, unstructured materials more effective. By allowing students to upload handwritten notes, diagrams, PDFs, or snapshots, the system parses the content into structured embeddings and enables conversational Q&A for better understanding. Drawing inspiration from RubberDuckyProgramming and active learning research, it helps users explain concepts to themselves, improving knowledge retention and comprehension. Built with the Sycamore parsing library, SingleStore for high-speed vector retrieval, and a retrieval-augmented generation (RAG) pipeline, the platform combines a dynamic Next.js frontend with Retell integrations for conversational learning. Recognized at **SB Hacks XI**, TalkTuahDuck demonstrates how AI-driven tools can transform study habits and accelerate comprehension.",
    "details": "Inspiration\nAccessing study materials can be a messy process—especially when dealing with handwritten notes, diagrams, PDFs, and image-based documents. Inspired by “RubberDuckyProgramming,” TalkTuahDuck encourages learners to “talk” through their study materials in a conversational setting. Research shows that actively explaining concepts can boost retention by up to 30% compared to passive study methods—TalkTuahDuck taps into this principle to supercharge understanding.\n\nWhat It Does\nTalkTuahDuck is an interactive study and explanation tool. Users can:\n- Upload PDFs, images, or messy whiteboard snapshots.\n- Parse & organize these documents into structured embeddings using the Sycamore parsing library.\n- Query the content through a conversational AI interface.\n- Retrieve targeted segments from SingleStore for ultra-fast, context-aware answers.\n- Optionally generate animations to visualize tricky concepts.\n\nArchitecture\n- **Data Ingestion:** Sycamore processes unstructured content via OCR and structured extraction.\n- **Data Storage:** SingleStore serves as a vector database for lightning-fast retrieval.\n- **Retrieval-Augmented Generation (RAG):** Real-time Q&A sessions reference exact document chunks.\n- **Frontend:** Next.js conversational interface with transcripts and dynamic source displays.\n\nKey Features\n- Advanced parsing for multi-format notes.\n- High-speed vector retrieval with SingleStore.\n- Conversational RAG for context-rich explanations.\n- Live transcripts of interactions.\n- Optional AI-generated animations for deeper clarity.\n\nAccomplishments\n- Built a complete RAG-powered study companion from scratch.\n- Integrated Sycamore, SingleStore, Retell, and Next.js into a cohesive system.\n- Delivered optional AI animation support for complex concepts.\n- Created accessible study workflows for students across diverse materials.\n- **Recognized at SB Hacks XI for innovation in AI-driven learning.**\n\nChallenges\n- Achieving reliable OCR parsing across varied input formats.\n- Training AI to generate dynamic SQL queries for flexible retrieval.\n- Balancing conversational flow with accurate sourcing.\n\nWhat We Learned\n- How to combine parsing, embeddings, and RAG for real-world study use cases.\n- The importance of building interfaces that reduce friction for overwhelmed students.\n- Best practices for interactive educational AI tools.\n\nWhat’s Next\n- **Better LLM Integrations:** More nuanced explanations for technical content.\n- **Enhanced Animations:** Richer instructional visuals.\n- **More Teaching Tools:** Expansion beyond Q&A into broader tutoring features.\n\nHook\n“Talk through your notes, and watch them explain themselves back.” TalkTuahDuck turns messy study material into an intelligent tutor that adapts to your needs.",
    "github": "https://github.com/AureliaSindhu/sbhacks",
    "demo": "https://youtu.be/hlBz5ejdrUc"
  },
  {
    "id": "splatnft",
    "name": "SplatNFT",
    "summary": "SplatNFT is a Web3-powered platform that transforms 2D videos into interactive 3D art experiences using Gaussian Splatting. By converting personal moments into dynamic, immersive digital assets, it allows users to create, own, and trade NFTs that go beyond static images. Built with Next.js, Node.js, Solana blockchain, and Anyone Protocol, it enables seamless video upload, Gaussian Splat previews, and secure NFT minting within a personal gallery interface. Recognized at **SoCal Tech Week 2024**, SplatNFT won the **SolanaU Sponsor Challenge**, showcasing how creative expression and blockchain technology can merge into a new form of digital ownership.",
    "details": "Inspiration\nWe're a team of artists and programmers passionate about exploring how art can intersect with Web3 and machine learning. SplatNFT was created as a way to turn personal moments into interactive pieces of art, redefining how creativity and technology intersect.\n\nWhat It Does\n- **Gaussian Splat Conversion:** Converts uploaded videos into Gaussian Splat representations, offering visually dynamic and interactive 3D art.\n- **Seamless Minting Process:** Allows users to mint their Gaussian Splat as an NFT with just a few clicks.\n- **Personal NFT Gallery:** Lets users view, browse, and manage their NFT collection.\n- **Secure Protocol:** Built with the Anyone Protocol for secure, decentralized NFT storage and minting.\n\nHow We Built It\n- **Frontend:** Next.js for responsive, dynamic UI.\n- **Backend:** Node.js for efficient data handling and video processing.\n- **Blockchain:** Solana and Anyone Protocol for decentralized minting and NFT storage.\n\nChallenges We Ran Into\n- Integrating blockchain functionality on the Solana network.\n- Managing secure API workflows for NFT minting and video conversion.\n\nAccomplishments We're Proud Of\n- Built an end-to-end platform where users can upload, convert, preview, and mint NFTs in one flow.\n- Combined Gaussian Splatting with blockchain to create a novel digital art medium.\n- Delivered an intuitive gallery experience for managing personal NFT collections.\n- **Winner of the SolanaU Sponsor Challenge at SoCal Tech Week 2024.**\n\nWhat We Learned\n- Gained hands-on experience in video processing and blockchain integration.\n- Improved backend process management and NFT minting workflows.\n- Learned how to design responsive, user-friendly interfaces for complex AI + blockchain systems.\n\nWhat's Next for SplatNFT\n- Support additional file formats for uploads.\n- Enhance scalability and optimization of Gaussian Splatting workflows.\n- Add customization options for 3D art rendering.\n- Partner with digital art communities to introduce galleries, tutorials, and collaborative showcases.",
    "github": "https://github.com/KevinWu098/SplatNFT",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/130/048/datas/gallery.jpg"
  },
  {
    "id": "magicloops",
    "name": "InstaRizz",
    "summary": "InstaRizz is a wearable-tech experiment that fuses AI with social interactions by delivering real-time personalized insights. Using Ray-Ban smart glasses streaming to Instagram Live, the system captures facial snapshots via OpenCV, identifies individuals, and generates custom bios and pickup lines on the fly. Submitted to **AI ATL 2024**, the project showcases how live streaming, machine learning, and natural language processing can converge into a playful yet technically advanced social tool, while also sparking important conversations about privacy and ethics in wearable AI.",
    "details": "Inspiration / Problem:\nTraditional wearable tech rarely blends entertainment, AI, and real-time social interaction. InstaRizz was created to explore how AI could augment human charm and live interactions while raising awareness of privacy implications.\n\nWhat It Does:\n- **Live Streaming:** Streams video from Ray-Ban smart glasses directly to Instagram Live.\n- **Facial Recognition:** Captures and processes snapshots using OpenCV.\n- **Identity Matching:** Uses a custom-trained classifier to identify consenting individuals.\n- **AI Personalization:** Runs a Magic Loops + perplexity search pipeline to retrieve public data, then generates a short bio and three pickup lines using Claude.\n\nHow It Works:\n1. Ray-Ban glasses stream the live feed.\n2. OpenCV captures and processes snapshots in real time.\n3. A custom classifier matches identities from a database of consenting participants.\n4. Magic Loops and Claude generate personalized bios and pickup lines displayed instantly.\n\nChallenges:\n- **Real-Time Synchronization:** Aligning live video streams with facial recognition was difficult.\n- **Classifier Accuracy:** Required fine-tuning to ensure identity recognition worked reliably.\n- **Privacy Concerns:** Designing safeguards and ethical considerations around facial recognition in public.\n\nAccomplishments:\n- Built a fully functioning, end-to-end system combining computer vision, LLMs, and real-time streaming.\n- Successfully generated bios and personalized pickup lines in seconds.\n- Proved the potential of integrating wearable AI into live social experiences.\n- **Showcased at AI ATL 2024, sparking discussions around the ethics of wearable AI.**\n\nWhat We Learned:\n- Optimizing real-time facial recognition pipelines.\n- Balancing fun, technical innovation, and ethics in AI design.\n- First-hand lessons in privacy risks associated with wearable AI.\n\nWhat’s Next:\n- Publish methods and best practices to protect individuals from unwanted scanning.\n- Release public guidelines on ethical use of wearable AI.\n- Explore broader social applications of real-time personalization technology.\n\nHook:\n\"AI meets charm.\" InstaRizz is a playful yet thought-provoking prototype that shows how AI can make live interactions more dynamic while prompting deeper discussions about ethics in the age of smart glasses.",
    "github": "https://github.com/coderkai03/AI-ATL-2024",
    "demo": "https://youtu.be/xjMcK34BBu4"
  },
  {
    "id": "maybe-zc19va",
    "name": "SoundSearch",
    "summary": "SoundSearch is an accessibility-focused tool that transforms website navigation into a guided voice experience. By placing a phone call, users receive step-by-step spoken instructions synchronized with the website they are browsing, making it easier to complete tasks like filling forms, applying filters, or exploring complex interfaces. Submitted to **AI ATL 2024**, where it won the **NLX Overall Award**, the platform highlights the potential of voice-driven guidance in lowering barriers for visually impaired individuals and users navigating websites in an unfamiliar language.",
    "details": "Inspiration\nNavigating new websites can be a daunting task. Platforms like Amazon and Google Flights, while powerful, often overwhelm users with countless filters and options. This complexity poses a significant hurdle, especially for individuals with visual impairments or those who are not fluent in the website's primary language. SoundSearch was created to bridge this accessibility gap, making the digital world more inclusive and user-friendly for everyone.\n\nWhat it does\nSoundSearch transforms the way users interact with websites by providing real-time, step-by-step voice guidance over a simple phone call. When a user dials the number, SoundSearch synchronizes with the website being used, highlighting specific sections and guiding them through each step of the process. It tells users how to fill out forms, adjust filters, and navigate complex interfaces. This makes online platforms more accessible, especially for those who have difficulty seeing or are not proficient in the website's language.\n\nHow it was built\nSeveral cutting-edge technologies were combined to bring SoundSearch to life:\n\nAWS (Amazon Web Services): Provided backend infrastructure, ensuring scalability and reliability of the service.\nNLX.ai: Powered the natural language processing, enabling voice interaction, understanding inputs, and delivering accurate, context-aware guidance.\nGoogle Flights Search: Integrated as a demonstration of navigating complex websites with numerous filters and options.\nNext.js: Used for the frontend, providing a seamless and responsive interface that works in tandem with the voice guidance system.\n\nChallenges encountered\nThe development process involved several obstacles:\n\nSponsor Platform Integration: Integrating with the sponsor's platform presented hurdles due to unfamiliar APIs and limited documentation.\nAWS Complexity: Configuring AWS services was challenging, requiring significant effort to set up infrastructure, manage permissions, and deploy the application.\n\nAccomplishments\n- Mastered AWS deployment despite it being the team’s first time working with the platform.\n- Delivered a functional prototype demonstrating real-time, voice-guided web navigation.\n- **Winner of the NLX Overall Award at AI ATL 2024**, validating the project’s impact on accessibility and innovation.\n\nKey learnings\nAWS Proficiency: Gained extensive knowledge about AWS services, deployment processes, debugging, and log management.\nIntegration Skills: Developed effective strategies for combining multiple technologies (AWS, NLX.ai, Google Flights API, Next.js) into a cohesive system.\nProblem-Solving: Enhanced ability to troubleshoot issues that arise during development, especially with unfamiliar platforms.\n\nWhat's next for SoundSearch\nUser Input Integration: Enabling users to input information via voice commands to make interaction even more seamless.\nExpanding Website Support: Extending compatibility to more websites, ensuring broader accessibility.\nPersonalization: Adding user profiles to tailor guidance based on individual preferences and needs.\nMultilingual Support: Introducing support for multiple languages to assist non-native speakers more effectively.",
    "github": "https://github.com/IdkwhatImD0ing/AIATL",
    "demo": "https://youtu.be/RgH-i9SYj-o"
  },
  {
    "id": "skysearch-t5ci1v",
    "name": "SkySearch",
    "summary": "SkySearch is an intelligent search-and-rescue platform that enables operators to deploy and manage fleets of drones, reconstruct disaster environments, and uncover hidden insights in real time. The system consolidates drone video feeds, telemetry, and environmental data into a single interface, allowing operators to identify hazards, locate people, and generate optimized, risk-aware rescue routes. Core features include semantic search, Gaussian splatting for 3D reconstruction, and autonomous swarming for large-scale missions. Submitted to **Cal Hacks 11.0**, SkySearch demonstrates how AI and drones can transform emergency response by scaling operations and accelerating life-saving decisions.",
    "details": "Inspiration\nHurricane Milton, the most devastating disaster in over 30 years, left more than 3 million people without power and overwhelmed emergency services scrambling to respond. While drone technology now floods us with vast amounts of data, the real challenge lies in making sense of it in rapidly evolving environments—operators are still stuck manually sifting through critical information when time is running out. In moments of chaos, the ability to scale autonomous search-and-rescue missions and intelligently uncover patterns from data becomes essential.\n\nSkySearch enables operators to uncover hidden insights on the environment in vast seas of data by integrating real-time data on the environment, telemetry, and previous missions into a single pane of glass (software mission control system). Operators can deploy fleets of drones to investigate regions and collect video feed used to reconstruct the scene, enabling them to drill-down on areas of interest through a semantic search engine.\n\nWhat it does\nOur goal is to enable operators to interact with data and uncover hidden patterns effortlessly.\n\nSkySearch is built around the end-to-end search-and-rescue workflow in the following use cases:\n\nSearch: Drones are deployed through the software by operators and autonomously navigate through terrain to identify objects of interest in real-time.\nRescue: Operators can interact with live data to isolate hazards and locate people through a unified search interface. Based on this data, the system then recommends risk-aware, optimized rescue routes for first responders.\n\nCore features\n- Environment reconstruction of damaged regions and infrastructure with Gaussian splatting\n- Risk-aware pathfinding for rescue operations\n- Semantic search through disparate data sources to uncover patterns and recommend actions\n\nHow we built it\nWe designed an embedded architecture that enables software and hardware interfaces to bidirectionally communicate information and commands.\n- Drone SDK for live video streaming\n- TP-Link antennas for a local WiFi system to create a more robust data pipeline between the drone and the software interface\n- OpenCV and Apple Depth Pro to process footage and classify data\n- SingleStore for real-time database management\n\nChallenges we ran into\n- Accounting for low-battery drones\n- Integration between hardware and software interfaces\n- Balancing human judgment with autonomy\n\nAccomplishments that we're proud of\n- Implemented an autonomous swarming framework for large-scale missions\n- Integrated Gaussian splatting for realistic 3D reconstruction\n- Developed risk-aware map traversal and recommended \"safe routes\" for emergency responders\n- Built dynamic data generation for efficient testing, analysis, and responsiveness\n- **Submitted to Cal Hacks 11.0**, showcasing the platform’s ability to merge computer vision, AI, and drone swarms for emergency response\n",
    "github": "https://github.com/KevinWu098/calhacks24",
    "demo": "https://youtu.be/_AtPD2fjSvs"
  },
  {
    "id": "swarmaid",
    "name": "SwarmAid",
    "summary": "SwarmAid is a multi-agent system designed to combat global food waste by connecting suppliers with surplus food to food banks and shelters in real time. Built on Swarm’s orchestration framework, it leverages intelligent agents for supply detection, demand matching, logistics optimization, and compliance checks. The system automates redistribution, minimizing waste while helping feed communities. Developed with Python, Leaflet.js, and real-time APIs, the proof of concept demonstrates smooth agent collaboration and efficient coordination. Submitted to **Hack Dearborn: Rewind Reality**, SwarmAid won **2nd Place**, highlighting the potential of decentralized AI to address urgent global challenges.",
    "details": "Inspiration / Problem:\nRoughly one-third of all food produced globally is wasted while millions go hungry. SwarmAid was born out of the urgent need to bridge this gap by applying intelligent, decentralized coordination to redirect surplus food to communities in need.\n\nWhat It Does:\n- **Supply Agents:** Detect surplus food from suppliers like restaurants, grocery stores, and farms.\n- **Demand Agents:** Match available surplus with food banks and shelters.\n- **Logistics Agents:** Plan efficient transport routes, minimizing delays and environmental impact.\n- **Compliance Agents:** Ensure all food handling and transport meet safety standards.\n\nHow It Works:\nSwarmAid uses Swarm’s multi-agent orchestration framework to manage collaboration across agents. The backend is built with Python, while Leaflet.js provides an interactive mapping interface. Real-time APIs enable data synchronization, creating a smooth, automated redistribution process.\n\nChallenges:\n- Achieving seamless communication between agents without creating bottlenecks.\n- Coordinating real-time logistics and compliance while maintaining scalability.\n- Experimenting with architectures and APIs to balance efficiency and reliability.\n\nAccomplishments:\n- Built a working proof of concept showcasing decentralized coordination in action.\n- Demonstrated real-time supply-demand matching with efficient logistics planning.\n- Highlighted the potential for technology to significantly reduce food waste while feeding communities.\n- **Won 2nd Place at Hack Dearborn: Rewind Reality**, recognizing SwarmAid’s innovative approach to tackling food waste.\n\nWhat We Learned:\n- Practical implementation of Swarm’s multi-agent framework.\n- Techniques for optimizing logistics and handling real-time data.\n- Insights into applying technology to large-scale social challenges such as hunger and waste reduction.\n\nWhat’s Next:\n- Integrate with real-world food inventory systems for live surplus data.\n- Incorporate traffic and weather data for more accurate logistics planning.\n- Expand coverage to broader geographic regions.\n- Develop a mobile app for accessibility and use predictive machine learning to anticipate supply and demand patterns.\n\nHook:\n\"Turning surplus into sustenance.\" SwarmAid shows how decentralized AI can tackle food waste and hunger through intelligent collaboration.",
    "github": "https://github.com/IdkwhatImD0ing/SwarmAid",
    "demo": "https://youtu.be/Vk73UiZksvo"
  },
  {
    "id": "bikstar",
    "name": "BikstAR",
    "summary": "BikstAR is a mixed-reality biking game that transforms real-world cycling into a competitive and social AR experience. Players bike along actual trails enhanced with virtual overlays, collecting carrots, power-ups, and competing in 2v2 races. The system gamifies outdoor exercise with features like live voice chat, real-time position tracking, and interactive challenges. Built in 36 hours using Oculus Quests for AR immersion, Unity as the game engine, Normcore for multiplayer, and Blender for custom 3D assets, BikstAR blends fitness, fun, and community in an engaging active sport. Submitted to **DreamXR**, the project showcases how mixed reality can bring new life to everyday outdoor activities.",
    "details": "Inspiration:\nAs natives of suburban Vermont and Irvine, we were all too familiar with the exciting nature of biking. Yet, despite biking being one of the most popular active sports with over 10,000 bikers per square mile, the repetitiveness of daily routes gets boring. What if we gamified biking into a mixed-reality experience that transforms mundane daily hassles into magical experiences?\n\nWhat It Does:\nbikstAR is an active AR game where bikers can bike IRL on a live path with an extra competitive kick: collecting carrots (coin system) and power-ups to compete with another player. Our goal is to transform a typically individual venture into a social sport connecting communities through a fun, immersive game.\n\nAs you pedal through your favorite trails, bikstAR overlays a captivating, bunny-themed virtual universe onto your surroundings. Your goal is to collect as many carrots as possible within a set time limit while navigating your actual bike trail, now enhanced with power-ups and challenges.\n\nCore Game Loop:\n- Collect carrots (reward system) on a user-generated path\n- Unpredictability through randomization of power-ups\n- Outpacing rivals in 2v2 competitive biking\n\nKey Features:\n- Real-time teammate position tracking\n- Live voice chat for communication\n- Dynamic updates on virtual objects and power-ups\n\nWhether you're stealing carrots from your opponents, launching projectiles, or racing to collect the most points, bikstAR offers a unique and entertaining experience that motivates outdoor activity.\n\nHow We Built It:\nIn just 36 sleepless hours, our team integrated cutting-edge technologies into a live, immersive environment:\n- **Oculus Quests:** Enabled immersive, mixed reality gameplay\n- **Normcore:** Facilitated seamless multiplayer connectivity\n- **Unity:** Powered the game engine and asset integration\n- **Blender:** Created custom 3D models and environments\n\nThe Team:\n- **Dieter:** 3D modeling, multiplayer implementation, and pitching\n- **Bill:** Backend development and real-time data synchronization\n- **Thomas:** Frontend design and user experience\n- **Jasmine:** Spatial design and bunny-themed game aesthetic",
    "github": "https://github.com/SerenityUX/dream",
    "demo": "https://youtu.be/Mcirws6Bh4A"
  },
  {
    "id": "linguify-katunw",
    "name": "Linguify",
    "summary": "Linguify is an AI-powered language learning platform that transforms everyday environments into immersive classrooms. By combining contextual image recognition, adaptive AI conversations, instant pronunciation feedback, and cultural insights, it helps learners build confidence for real-world interactions. Unlike traditional apps that focus on repetition, Linguify bridges the gap between study and authentic conversation, keeping users motivated with gamified features and personalized learning paths. Submitted to **VTHacks 12**, where it won the **Apex Center – Best Startup Hack** award.",
    "details": "Linguify: Revolutionizing Language Learning Through Real-World Interaction\n\nThe Problem:\nTraditional language learning apps are repetitive and fail to prepare users for real-life conversations. A Rosetta Stone survey found that 73% of language learners feel anxious about using their new skills in daily life, and users of popular apps often lack listening proficiency and speaking practice.\n\nOur Solution:\nLinguify transforms your environment into an immersive language learning experience through:\n- **Contextual Learning with Image Recognition:** Snap a photo and generate relevant vocabulary and phrases from your surroundings.\n- **AI-Powered Conversations:** Practice dialogues that adapt to your level and progress.\n- **Immediate Feedback:** Get instant guidance on pronunciation and comprehension.\n- **Cultural Integration:** Learn social norms and cultural nuances to build deeper connections.\n\nValue Proposition:\n- Bridges the gap to real conversations by simulating everyday scenarios.\n- Personalized, adaptive learning keeps users engaged and motivated.\n- Gamified elements like streaks and leaderboards encourage consistency.\n- Covers speaking, listening, reading, and writing for holistic mastery.\n\nMarket Opportunity:\nWith over 500 million global language learners and widespread dissatisfaction with existing apps, there’s a strong demand for more effective, real-world solutions.\n\nWhy Linguify:\nBy turning the world into your classroom, Linguify offers a unique, immersive approach that builds fluency, confidence, and cultural awareness beyond what traditional apps provide.",
    "github": "https://github.com/IdkwhatImD0ing/Linguify",
    "demo": "https://youtu.be/1JRYNXzAk-A"
  },
  {
    "id": "safety-blanket-vyp089",
    "name": "Safety Blanket",
    "summary": "Safety Blanket is a mobile app that serves as a lifeline for individuals walking alone, combining AI companionship with real-time safety features. It offers live conversational calls with AI for reassurance, automated check-ins that escalate to location sharing, and a safety timer that alerts emergency contacts if not deactivated. Built with FastAPI, WebSockets, Firebase, Langchain, Twilio, and Google Maps API on the backend, and Next.js with TailwindCSS on the frontend, the project demonstrates how AI can provide emotional support while enhancing personal safety. Submitted to **VenusHacks 2024**.",
    "details": "Inspiration:\nImagine feeling a knot of anxiety every time you walk alone after dark, even in your own neighborhood. One in two women feel unsafe walking alone after dark in a quiet street near their home, compared to just one in seven men. This fear is even greater in busier areas—in parks or open spaces, the figure jumps to four out of five.\n\nThe statistics are even more alarming for younger women: two out of three women aged 16 to 34 have experienced harassment in the past year. Women often rely on friends or family for check-ins, but what happens when no one is available? Safety Blanket was created to be a lifeline — a virtual AI companion offering emotional security and reliable safety features.\n\nWhat It Does:\n- **AI Companion:** Real-time phone calls with conversational AI for reassurance.\n- **Check-ins:** AI text chat that escalates to location sharing with trusted contacts if you don’t respond.\n- **Safety Timer:** Alerts emergency contacts with your location if not deactivated with a password.\n\nHow We Built It:\n- **Backend:** FastAPI, WebSockets, Firebase, OpenAI API, Retell AI, Langchain, Twilio, Google Maps API.\n- **Frontend:** Next.js, TailwindCSS, Figma.\n\nChallenges:\n- Integrating individually built parts into one unified product.\n- Balancing varying skill sets in a team working together for the first time.\n\nAccomplishments:\n- Built a cohesive app with AI companionship, real-time location sharing, and emergency protocols.\n- Overcame integration and design challenges as a new team.\n\nWhat’s Next:\n- Integrating with wearables.\n- Enhancing AI personalization.\n- Expanding international emergency services.\n\nSafety Blanket continues to evolve, providing confidence and peace of mind to users worldwide.",
    "github": "https://github.com/IdkwhatImD0ing/Linguify",
    "demo": "https://youtu.be/E8bzwcbllKY"
  },
  {
    "id": "abseas",
    "name": "ABSeas",
    "summary": "ABSeas is a web app that teaches preschool concepts like counting, letters, and social skills through music, visuals, and interactive play. By combining AI-generated songs, colorful images, and voice interaction, it transforms early learning into a fun, engaging experience. Built with Firebase, Claude, Suno AI, DALL·E 3, Whisper, and Eleven Labs, ABSeas synchronizes multiple generative AI pipelines to deliver an intuitive platform for kids and parents alike. Submitted to **DiamondHacks 2024**, where it won **Track 4: Captain’s Classroom**.",
    "details": "Inspiration:\nSchooling is expensive. The average private preschool tuition in California is $12,967 per year—a cost not every parent can afford. Preschool sets up children for long-term academic success, but access is limited. Music, however, enhances recall and brain development. This inspired us to create a way to teach preschool knowledge through song and dance.\n\nWhat It Does:\nABSeas is a web app that teaches preschool topics such as counting, letters, sharing, and manners through music and visuals. Kids can sing along while engaging with colorful pictures that reinforce learning. Our approach focuses on:\n- **Content Synchronization:** Blending preschool concepts with catchy songs and visuals.\n- **Intuitive Interaction:** Bold visuals and interactive elements designed for children.\n- **Progressive Learning:** Starting with basics like ABCs and advancing to social skills as children grow.\n\nDesign Process:\nABSeas emphasizes simplicity, bold colors, and large interactive elements. Content flows smoothly so children and parents can navigate easily. Each module feels natural and fun, creating a tailored and educational experience.\n\nHow We Built It:\nFrontend: Modern web development stack.\nBackend: AI-powered pipeline integrating:\n- Anthropic Claude (text generation)\n- Suno AI (AI-generated music)\n- OpenAI DALL·E 3 (images)\n- Eleven Labs (voice cloning & TTS)\n- OpenAI Whisper (speech recognition)\n- Firebase (real-time database)\n\nChallenges:\nSynchronizing Firebase with multiple APIs was a key challenge, requiring careful integration of interactivity and stability.\n\nAccomplishments:\nWe infused generative AI into children’s learning, combining visuals and music in a fun and educational way.\n\nWhat We Learned:\nWe improved skills in multiprocessing, audio processing, frontend design, and user experience refinement, while building a professional and intuitive app.",
    "github": "https://github.com/IdkwhatImD0ing/ABSeas",
    "demo": "https://youtu.be/YeeddNGpoN8"
  },
  {
    "id": "tidbits-n9mrxa",
    "name": "Tidbits",
    "summary": "Tidbits is an AI-powered platform that transforms long university lectures into short, story-driven, scrollable videos. By combining storytelling, real-time translation, and adaptive personalization, it makes learning fast, engaging, and accessible. Built with Claude, Suno AI, Eleven Labs, D-iD, Whisper, and DALL·E, Tidbits optimizes video creation from 30 minutes down to 5. Submitted to **YHACK 2024**, where it won **1st Place**.",
    "details": "Inspiration:\nUniversity students spend over 3000 hours in lectures, yet 80% of lecture content is often forgotten. Traditional note-taking and memorization are inefficient. Tidbits leverages storytelling and short-form video to transform lectures into engaging, bite-sized educational content.\n\nWhat It Does:\nTidbits converts lectures (audio or visual) into short, scrollable videos with one-click upload. It mimics TikTok’s familiar interface, making learning fast, engaging, and accessible. Key features include:\n- Story-based learning: 1–2 minute summaries of 1–2 hour lectures.\n- International translation: complex English material translated into any language.\n- Personalized flows: AI adapts content for each student.\n\nDesign Process:\nFocused on accessibility and engagement:\n- Bold branding and easy navigation for mobile.\n- Automated personalization—users only choose topics, AI handles the rest.\n\nHow We Built It:\nTech stack integrates Anthropic Claude, Suno AI, Eleven Labs, D-iD, Whisper, and DALL·E. The pipeline uses FastAPI + Next.js, Supabase, and advanced concurrency techniques (asyncio, aiohttp, threading). Optimizations reduced reel creation time from 30 minutes to 5.\n\nChallenges:\nSynchronizing multiple AI outputs (captions, audio, images, timestamps) was complex. Reverse-engineering Suno AI, frontend video auto-play limitations, and managing state/UI updates were key hurdles.\n\nAccomplishments:\nSuccessfully produced synchronized educational reels. Optimized processing for a 7.5x speedup. Built a functional prototype within hackathon time constraints.\n\nWhat We Learned:\n- Async multithreading for performance optimization\n- Audio/video editing with Python\n- Supabase schema design and flexibility\n- Importance of planned frontend state management\n\nWhat’s Next:\nMore customization options for generated tidbits, including screenshots from lectures and supplementary web images.",
    "github": "https://github.com/dylanvu/Tidbits",
    "demo": "https://youtu.be/6EjKhcIuaYo"
  },
  {
    "id": "gemui",
    "name": "GemUI",
    "summary": "GemUI is an AI-powered accessibility layer that dynamically simplifies websites into intuitive, minimal UIs for elderly users, children, and people with accessibility needs. It interprets user intent and generates only the relevant buttons, forms, and text, making navigation effortless. Built with Gemini 1.5, Whisper, Selenium, FastAPI, and Next.js, GemUI was submitted to the **Google AI Hackathon** and the **Google x MHacks AI Hackathon**.",
    "details": "Inspiration:\nGemUI was inspired by Dylan’s grandmother, a breast cancer survivor who struggles with online navigation for tasks like booking labs. The team wanted to simplify the internet for users like her. Inspired by Vercel’s generative UI demo at Google Next 2024, they built a dynamic solution that generates simplified UI directly from real websites.\n\nWhat It Does:\nGemUI interprets user requests and generates only the relevant UI elements—buttons, forms, and text—making the web accessible to elderly users, children, and anyone needing a simplified interface. It acts as a universal, stripped-down UI layer for any website.\n\nHow We Built It:\nThe system integrates Gemini 1.5 for large-context reasoning, Whisper for transcription, Selenium for live browser control, and real-time WebSockets for chatbot-browser interaction. Frontend: Next.js, TypeScript, ShadCN, Tailwind, and Vercel AI SDK. Backend: FastAPI, Python SDK, Gemini 1.5, and Selenium.\n\nCommunity Impact:\nGemUI empowers elderly users, children, and people with accessibility needs by simplifying website navigation. It also allows personalization of fonts, display size, and interaction methods to fit unique user requirements.\n\nWhat’s Next:\nThe team plans to incorporate direct voice interaction (bypassing transcription) so users who can’t type or read can navigate the internet through speech alone.",
    "github": "https://github.com/KevinWu098/gemUI",
    "demo": "https://youtu.be/27MgQSSHuuQ"
  },
  {
    "id": "doggo-ai",
    "name": "Doggo AI",
    "summary": "Doggo AI is an interactive plush companion designed to support hospitalized children during critical developmental stages. Combining responsive AI conversations, real-time emotion detection, and a caretaker dashboard, it provides both comfort and connection. By merging the familiarity of stuffed animals with advanced AI, Doggo AI fosters emotional well-being and learning in long-term care environments. Submitted to **HackDavis 2024**, where it won **Best Use of Intel® Developer Cloud**.",
    "details": "Inspiration:\nOver 5 million children are hospitalized in the US every year, often without caretakers during critical developmental periods. Many toddlers bond with stuffed animals, which aid healthy attachment. Doggo AI was created to combine that comfort with an AI companion to support hospitalized children.\n\nWhat It Does:\nDoggo AI is an interactive plush companion that children can talk to. It tells stories, teaches, and responds naturally. Key functions include:\n1. Responsive AI conversations that adapt to interruptions, questions, and statements.\n2. Real-time emotion detection that allows the plush to respond empathetically.\n3. A caretaker dashboard with live transcripts and emotion reports.\n\nAll of this is built into a soft stuffed toy designed for hugging and play, encouraging intuitive and inclusive engagement.\n\nDesign Process:\nThe team created paper mockups, conducted user research with patients and caretakers, and developed personas to guide design. They designed low- and high-fidelity prototypes, prioritizing accessibility with WCAG-compliant colors, high-contrast text, and natural voice interaction. The plush encourages intuitive, inclusive engagement.\n\nHow We Built It:\nDoggo AI combines multi-threading, multiprocessing, and asyncio for natural conversation flow. Emotion detection is powered by the Hume EVI API, conversations by GPT-4, and real-time communication via WebSockets with a FastAPI backend. The dashboard is built with Next.js, TypeScript, ShadCN, Tailwind, and Figma. The plush contains a webcam, microphone, speaker, Raspberry Pi, and power bank, all sewn into a custom toy.\n\nImpact:\nDoggo AI bridges emotional and educational gaps for children in long-term care, offering companionship, comfort, and a sense of connection in otherwise isolating environments.",
    "github": "https://github.com/IdkwhatImD0ing/DoggoAI",
    "demo": "https://youtu.be/RAe3fLyPIa0"
  },
  {
    "id": "mad-lyrics",
    "name": "Mad Lyrics",
    "summary": "Mad Lyrics is a multiplayer party game that combines humor, collaboration, and AI music generation. Players fill in blanks to create absurd song lyrics, which are then transformed into fully rendered AI songs with 3D visualizations. Submitted to **Uncommon Hacks 2024**, where it won **Best in Track: Programmatic Art**.",
    "details": "Inspiration:\nMadLyrics was born from the idea of blending music creation, multiplayer collaboration, and party dynamics. Inspired by Mad Libs, the team wanted to create a game that unites people through humor and creativity.\n\nWhat It Does:\nMadLyrics challenges users to laugh, create, and make beats together. Players collaboratively fill in blanks to generate lyrics, which are then brought to life with AI-generated music and immersive 3D visualizations.\n\nHow We Built It:\n- Multiplayer state management via WebSockets\n- AI pipeline using OpenAI for text generation and Suno for AI music\n- 3D music visualizations powered by Three.js\n- Web client built with Next.js and React\n- Backend built with Python and FastAPI\n\nChallenges:\nManaging multiplayer state across users proved to be the most difficult technical challenge.\n\nAccomplishments:\n- Reverse-engineered Suno’s custom AI music generation\n- Built a live 3D music visualizer\n- Survived Chicago’s cold weather (as Californians!)\n\nWhat We Learned:\n- Efficient techniques for handling multiplayer state\n- How to integrate cutting-edge AI models into a party game\n- CapitalOne discounts coffee by 50%\n- Five Guys gives free peanuts",
    "github": "https://github.com/dylanvu/Mad-Lyrics",
    "demo": "https://youtu.be/Yu8hFL5SS0g"
  },
  {
    "id": "pypointer",
    "name": "PyPointer",
    "summary": "PyPointer reimagines computer interaction by letting users control any screen with just their fingers and voice. The fingertip acts as the cursor, while voice commands like 'click' execute actions. Submitted to **HackMerced IX**, where it won **2nd Place in the Spatial & Interactivity Track**.",
    "details": "Inspiration:\nPyPointer was inspired by a video showcasing a gaming setup without a keyboard or mouse. The team wanted to challenge conventional interaction methods by enabling hand and voice as the only inputs, pushing the boundaries of what’s possible under hackathon constraints.\n\nWhat It Does:\nPyPointer allows users to interact with any screen through natural gestures and speech. The index finger moves the cursor, while simple voice commands (e.g., 'click') perform actions like a left mouse click.\n\nHow We Built It:\n- Screen segmentation using Meta’s SAM model, OpenCV contour detection, and convex hull algorithms\n- Finger tracking with OpenCV and MediaPipe\n- Voice commands powered by OpenAI Whisper for speech recognition\n- Cursor/keyboard control via PyAutoGUI\n\nChallenges:\n- **Multithreading:** Python’s single-threaded nature caused conflicts when running webcam tracking and speech-to-text simultaneously\n- **Perspective distortion:** Mapping finger movements consistently across angled screens was a challenge\n- **Pivot:** Originally planned gesture shortcuts for controls but pivoted to voice commands for feasibility and accessibility\n\nAccomplishments:\n- Successfully segmented laptop screens with high accuracy\n- Achieved rough but functional cursor control via finger tracking\n- Overcame threading limitations by implementing multiprocessing in Python\n\nWhat We Learned:\n- Handling parallel processes in Python with the multiprocessing library\n- Techniques for robust image segmentation\n- Practical applications of PyAutoGUI for input control\n\nWhat’s Next:\n- Add full gesture recognition to complement voice commands\n- Improve coordinate accuracy for smoother cursor movement\n- Fine-tune PyAutoGUI drag speed for seamless interaction",
    "github": "https://github.com/dylanvu/PyPointer",
    "demo": "https://youtu.be/EUbloWRjQHo"
  },
  {
    "id": "journeyes",
    "name": "JournEyes",
    "summary": "JournEyes is a VR travel assistant that identifies landmarks, signs, and objects in real time through voice commands and image recognition. Submitted to **TreeHacks 2024**.",
    "details": "Inspiration:\nOur team’s love for travel and curiosity about diverse cultures sparked the idea for JournEyes. We wanted a travel companion that enhances journeys by instantly translating signs, identifying objects, and guiding explorers in unfamiliar environments.\n\nWhat It Does:\nJournEyes is a VR travel assistant that lets users point and ask questions about the world around them. Using voice commands and image recognition, it identifies monuments, plants, or objects, and provides instant explanations—acting as a local guide, historian, and botanist all in one.\n\nHow We Built It:\n- Unity with Meta XR SDK for VR experience\n- Backend: Python with FastAPI, integrated with AI capabilities\n- Image recognition: Google Lens via SERP API\n- Voice transcription: OpenAI Whisper\n- Image cropping: Meta research paper techniques for precise object focus\n- Infrastructure: Docker for deployment, ngrok for connectivity, real-time communication via websockets and SocketIO\n\nChallenges:\n- Steep learning curve with Unity and Meta XR SDK\n- Meta restriction: no passthrough video feed capture from third-party software, requiring a workaround to stream casted video feed into the backend\n\nAccomplishments:\n- Built a functional VR travel app from scratch\n- Overcame video feed limitations with a custom streaming solution\n- Transitioned from web development to VR innovation within the hackathon timeframe\n\nWhat We Learned:\n- Unity integration with Meta Quest 2 and 3\n- Handling HTTP calls within Unity\n- Transforming screenshots into Base64 strings\n- Using Google Cloud for hosting and Google Lens for real-time object identification\n- Deeper knowledge of VR development and AI integration",
    "github": "https://github.com/spikecodes/JournEyes",
    "demo": "https://youtu.be/QdcCzyXl3Cg"
  },
  {
    "id": "talking-terry",
    "name": "Talking Terry",
    "summary": "Talking Terry is a voice-powered portable device that answers questions, makes calls, and provides recommendations using retrieval-augmented generation. Submitted to **QWER Hacks 2024**. 🏆 Winner: Best Use of Google Cloud (sponsored by Google Cloud).",
    "details": "Inspiration:\nTalking Terry was inspired by our mission to foster inclusivity and empower those unable to interact with phone screens. We wanted to create a portable, accessible solution that elevates quality of life through speech-first interactions.\n\nWhat It Does:\nTalking Terry is a Raspberry Pi–powered, voice-first device equipped with GPS, microphone, and speaker. It integrates multiple APIs (SerpAPI, Yelp, Twilio, Weather, BruinLearn/Canvas, Global-Warming.org) and uses retrieval-augmented generation to answer questions, provide recommendations, make calls/texts, and analyze climate data. It also continuously stores transcripts in Google Cloud and remembers past interactions for personalized responses.\n\nHow We Built It:\n- Python with LangChain, OpenAI, and GPT-4 Turbo for core intelligence\n- APIs: SerpAPI, Yelp, Twilio, Weather API, BruinLearn/Canvas, Global-Warming.org\n- Hardware: Raspberry Pi, GPS module, microphone, and speaker\n- Google Cloud for transcript storage\n- Web scraping Ebsco’s LGBTQ+ Source for book recommendations\n\nChallenges:\n- API restrictions (Uber, Lyft, Spotify required logins/approvals)\n- Microphone issues (static recordings)\n- Hardware supply shortages and lack of soldering tools until late in hackathon\n\nAccomplishments:\n- Integrated multiple APIs into a retrieval-augmented generation pipeline\n- Built a working portable device with speech input/output\n- Achieved multilingual text-to-speech, including fluent Chinese\n- Enabled calling and texting via Twilio\n\nWhat We Learned:\n- Advanced use of LangChain and large language models\n- Retrieval-augmented generation with multiple APIs\n- Hands-on hardware integration (Raspberry Pi, GPS, mic, speaker)\n- Text-to-speech across multiple languages\n\nWhat’s Next:\n- Launching a web interface for extended functionality\n- Adding new agents (e.g., voice reminder app)\n- Gmail API integration for drafting and sending emails\n- Refining Yelp agent for richer restaurant and event insights\n- Expanding personalized and accessible features for users",
    "github": "https://github.com/tranbrandon1233/TalkingTerry",
    "demo": "https://youtu.be/nFpyDH8eSrE"
  },
  {
    "id": "xplore-p1dnvc",
    "name": "Xplore",
    "summary": "Xplore is an AI-powered travel companion app that generates optimized itineraries, routes, and attraction recommendations. Submitted to **IrvineHacks 2024**. 🏆 Winner: Best Travel Hack.",
    "details": "Inspiration:\nThe idea for Xplore came from a lighthearted slip-up when one of us couldn’t pronounce the word 'itinerary.' From that moment came the concept of an intuitive app that makes travel planning effortless.\n\nWhat It Does:\nXplore is a personalized travel companion with three core functions:\n- Travelers input city destinations, stay duration, travel radius, and preferred attractions.\n- AI-driven recommendations suggest tailored attractions, with the choice of manual or automatic selection.\n- Xplore generates and saves optimized travel routes for a seamless experience.\n\nHow We Built It:\n- User-centered design optimized for both desktop and mobile\n- Authentication system to save and retrieve routes\n- MelissaAPI integration for address verification\n- GraphQL for batching API calls\n- Next.js and Chakra UI for the frontend\n- Firebase and Clerk for backend and authentication\n- Multithreading for performance improvements\n\nChallenges:\n- Integrating and configuring MelissaAPI\n- Learning and applying new frameworks quickly\n- Refining the codebase for stability and performance\n- Synchronizing frontend and backend components\n- Implementing secure authentication\n- Managing fatigue during long hackathon nights\n\nAccomplishments:\n- Successfully batching API calls with GraphQL\n- Adding Progressive Web App (PWA) functionality\n- Designing and implementing a custom ML algorithm to optimize travel routes\n\nWhat We Learned:\n- Backend: GraphQL, Firebase, Clerk\n- Frontend: Next.js, Chakra UI\n- Git lesson: never let two people work on the same branch simultaneously\n- And most importantly: 'Itinerary' is essential in travel planning—and trickier to say than you’d think!\n\nWhat’s Next:\n- Real-time weather updates for destinations\n- Local public transport details (routes, schedules, fares)\n- Quick access to emergency contacts such as hospitals and embassies",
    "github": "https://github.com/IdkwhatImD0ing/Xplore",
    "demo": "https://youtu.be/U1GTyyG9-MY"
  },
  {
    "id": "besustainable",
    "name": "BeSustainable",
    "summary": "BeSustainable is a mobile-first app that encourages small, daily sustainability habits through barcode scans, ingredient insights, and personalized scores. Submitted to **CruzHacks 2024**.",
    "details": "Inspiration:\nAs college students juggling heavy coursework, tackling global issues can feel overwhelming. We wanted a way for busy individuals to still make an impact. That led us to create BeSustainable — an app that empowers average people to take small, meaningful steps toward sustainability. Our belief is that small changes have a ripple effect, creating big change.\n\nWhat It Does:\n- Encourages sustainability habits by integrating them into daily life.\n- Empowers individuals to make small but impactful changes.\n- Facilitates smooth adoption of sustainable practices, improving health and well-being without major disruption.\n\nHow We Built It:\n- Designs in Figma\n- Frontend with MaterialUI\n- Backend with Next.js\n- Database: MongoDB\n- APIs: Spoonacular API, Google OCR\n\nChallenges:\n- Balancing creativity with feasibility in design (e.g., camera page flow).\n- Learning and adapting to MaterialUI framework.\n- Merge conflicts and formatting consistency.\n- Creating a sustainability score using ingredient lists and barcodes.\n- Connecting MongoDB to the project.\n- Integrating Auth0 for login/registration.\n\nAccomplishments:\n- Building an app designed to encourage sustainability.\n- Creating a simple, easy-to-use interface accessible via mobile.\n- Learning and applying new technologies to deliver a working product.\n\nWhat We Learned:\n- The importance of discussing features and flows to stay aligned.\n- The value of timelines and role assignments for development efficiency.\n- Technical growth in frameworks, APIs, and authentication systems.\n\nWhat’s Next:\n- Add an AI chatbot for recommendations, feedback, and ingredient insights.\n- Enable users to connect and interact with friends’ posts.\n- Allow users with high sustainability scores to connect globally, not just locally.",
    "github": "https://github.com/IdkwhatImD0ing/BeSustainable",
    "demo": "https://youtu.be/ukji3S3eCow"
  },
  {
    "id": "counsel",
    "name": "Counsely",
    "summary": "Counsely is a real-time AI assistant for therapists, helping reduce cognitive stress and provide unobtrusive insights during and after sessions. Submitted to **SB Hacks X**.",
    "details": "Inspiration:\nLife is hard, and mental health professionals play a crucial role in helping us navigate it. The demand for mental health services has surged — Statista reports that 41.7 million U.S. adults received services in 2021, reflecting massive need. Yet new counselors are often thrown into the deep end, and even seasoned therapists face challenges in the digital age, especially with telehealth. Counsely aims to bridge the gap between counselor and client, reducing cognitive stress and providing unobtrusive insights.\n\nWhat It Does:\n- Acts as a real-time assistant for therapists.\n- During sessions: Provides AI-powered conclusions and suggestions in real time.\n- After sessions: Delivers dashboards with performance data and client insights to help counselors reflect and better understand their clients.\n\nHow It Was Built:\n- User-centered design guided by problem space research.\n- User-flow diagrams for both therapist and patient.\n- Feature prioritization stories to ensure efficient, user-focused implementation.\n\nChallenges:\n- Noisy laptop microphone quality, solved with noise filtering.\n- Backend package version errors when integrating statistics into the dashboard.\n\nAccomplishments:\n- Achieved real-time feedback flow using multithreading, ensuring insights arrive during sessions, not just after.\n- Designed a calming, empathetic UI to support mental health contexts.\n\nWhat We Learned:\n- Frontend: deeper experience with Material UI.\n- Backend: greater comfort with Firebase.\n- General: adaptability in the face of API issues, background noise, and other unexpected challenges.\n\nWhat’s Next:\n- Add a video-call feature to make the tool more versatile for telehealth sessions.",
    "github": "https://github.com/kaeladair/sbhacks24",
    "demo": "https://youtu.be/Q56mbQdtSnk"
  },
  {
    "id": "webweaver",
    "name": "WebWeaver",
    "summary": "WebWeaver is an AI-powered collaborative website builder that helps non-technical users and developers create professional websites. Submitted to **AI ATL (Atlanta)**, Winner of **MLH: Most Innovative Startup Idea**.",
    "details": "Inspiration / Problem:\nBuilding and maintaining a professional website remains a challenge for small businesses, freelancers, and non-profits due to technical barriers and limited resources. WebWeaver aims to democratize website creation with AI-powered collaboration.\n\nWhat It Does:\n- Provides an interactive AI chatbot to gather requirements and generate website drafts.\n- Allows real-time collaboration with the AI to refine content and design.\n- Includes advanced code editing interfaces for experienced developers.\n- Offers a streamlined dashboard for managing the full website lifecycle.\n\nHow It Was Built:\n- Frontend: Next.js with Material UI for sleek, responsive design.\n- Backend: FastAPI for APIs, Redis for state synchronization, Axios for client-server requests.\n- AI: GPT-Vision for multimodal website generation and drafting.\n\nChallenges:\n- Ensuring real-time synchronization across multiple clients.\n- Balancing simplicity for non-technical users with flexibility for developers.\n\nAccomplishments:\n- Successfully integrated AI into the web development workflow.\n- Built collaborative real-time editing tools.\n- Designed an accessible UI for both beginners and advanced users.\n\nWhat We Learned:\n- Strategies for combining AI with traditional web development.\n- Technical approaches to real-time collaborative editing.\n- Best practices in component-based architecture and state management.\n- The importance of UX in lowering barriers for non-technical users.\n\nWhat’s Next:\n- Support for more frameworks like React, Angular, and Vue.\n- AI-powered optimization for performance and SEO.\n- A mobile app version for on-the-go management.\n- Tutorials, guides, and a community forum for user support.",
    "github": "https://github.com/IdkwhatImD0ing/WebsiteGenerator",
    "demo": "https://youtu.be/V977Ydky1DI"
  },
  {
    "id": "pilltok",
    "name": "PillTok",
    "summary": "PillTok is an AI-powered healthtech app that simplifies medication management through photo-based prescription uploads, adaptive scheduling, and interaction alerts. Submitted to **HackSC X**.",
    "details": "Inspiration:\nPillTok was created to address the challenges of medication non-adherence and prescription interaction risks. The goal is to streamline medication management, safeguard patient health, and reduce healthcare costs, while ensuring accessibility for disabled individuals, non-native English speakers, and those in remote areas.\n\nWhat It Does:\n- Allows effortless prescription data uploads via photo capture.\n- Identifies and alerts users to problematic prescription interactions.\n- Generates personalized weekly medication schedules tailored to user routines.\n- Sends reminders and nudges to improve adherence.\n- Adapts schedules in real time based on user feedback.\n\nHow It Was Built:\n- Google Vision OCR for extracting text from prescription labels.\n- React and Next.js for building a dynamic, engaging frontend.\n- FastAPI for robust, high-performance backend services.\n- RedisCloud for fast and reliable database operations.\n\nChallenges:\n- Obtaining real prescription bottles for accurate data testing.\n- Extracting text from the curved surfaces of pill bottles.\n- Designing a scheduling algorithm capable of handling complex medication regimens.\n- Updating schedules responsively in real time based on user interactions.\n\nAccomplishments:\n- Successful OCR implementation with Google Vision API.\n- Built a thorough medication interaction checker.\n- Developed a smart, adaptive scheduling system.\n- Delivered a responsive, user-friendly interface with Next.js.\n- Integrated FastAPI and RedisCloud for optimal backend performance.\n\nWhat We Learned:\n- How to integrate Next.js, FastAPI, and RedisCloud into a seamless healthtech app.\n- The complexities of building secure backend services for health data.\n- Techniques for accurate OCR text extraction from curved and irregular surfaces.\n\nWhat’s Next:\n- Improving computer vision algorithms for greater accuracy.\n- Adding multi-language support for accessibility worldwide.\n- Developing an AI-powered chatbot for personalized health guidance.\n\nHow To Run:\n1. Clone the repository: git clone <url>\n2. Run frontend: cd client && npm install && npm start\n3. Run backend: cd server && pip install -r requirements.txt && uvicorn main:app --reload",
    "github": "https://github.com/rdszhao/pilltok",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/660/588/datas/gallery.jpg"
  },
  {
    "id": "studyai",
    "name": "StudyAI",
    "summary": "StudyAI is a voice-activated study assistant that uses AI to streamline self-learning through summarization, visualization, and adaptive note-taking. Submitted to **Cal Hacks 10.0**.",
    "details": "Overview:\nStudyAI (Systematic Teaching Using Dynamic Yielding and Autonomous Intelligence) is a voice-activated study assistant designed to redefine the self-study experience. By leveraging machine learning, voice recognition, and natural language understanding, it provides students with efficient and intuitive study tools.\n\nInspiration:\nWith the overwhelming amount of information available today, self-study often leads to information overload, poor note management, and a lack of personalization. StudyAI was built to address these issues, offering a streamlined and intelligent approach to learning.\n\nGoals:\n- Deliver a seamless, voice-activated study assistant.\n- Provide text and video summarization.\n- Generate explanatory images and visualizations.\n- Recommend educational videos.\n- Manage personalized study notes.\n\nBuilt With:\n- Frontend: Reflex.dev with custom React components\n- Backend: FastAPI (via Reflex.dev)\n- Voice Recognition & TTS: 11Labs, Whisper\n- Autonomous Agents: OpenAI function calling agents\n- Text Summarization: mistralai/Mistral-7B-Instruct-v0.1 (TogetherAI)\n- Image Generation: stabilityai/stable-diffusion-2-1 (TogetherAI)\n\nChallenges:\n- Achieving real-time voice recognition with less than 5 second latency.\n- Seamless integration of multiple ML models and APIs.\n- Balancing scalability, performance, and reliability.\n- Maintaining strict data privacy and security.\n\nAccomplishments:\n- Built a voice-activated command parsing system.\n- Integrated autonomous decision-making via OpenAI function calling.\n- Developed accurate data-fetching and summarization modules.\n- Integrated YouTube API for educational content recommendations.\n\nWhat We Learned:\n- Effective integration of machine learning with voice technologies.\n- Importance of modular design for scalability.\n- Handling and optimizing large datasets for quick retrieval.\n- Designing user-friendly experiences tailored for education.\n\nWhat’s Next:\n- Personalized learning paths based on user behavior.\n- Expanding academic resources and journal access.\n- Developing a mobile app for accessibility on the go.\n- Partnering with educational institutions for wider adoption.\n\nFeatures:\n- Voice activation and command parsing.\n- Autonomous decision-making to adapt to student needs.\n- Text summarization for concise academic material.\n- Image and visualization generation for complex topics.\n- YouTube video suggestions for extended learning.",
    "github": "https://github.com/IdkwhatImD0ing/STUDYAI",
    "demo": ""
  },
  {
    "id": "slugloop",
    "name": "SlugLoop",
    "summary": "SlugLoop is a real-time bus tracking app designed for UCSC students, helping balance campus loop and metro usage. Submitted to **CruzHacks 2023**, Winner of [MLH – GitHub] Most Creative Use of GitHub.",
    "details": "Inspiration:\nFor many UCSC students, the only transportation options are metro and loop buses. Because loop buses are less predictable, students often take metro buses, filling them up and leaving those who need to travel off-campus without space. SlugLoop was created to provide real-time loop bus tracking, encouraging students to use loop buses and reducing pressure on metro services.\n\nGoals:\n- Provide accurate, up-to-date loop bus locations.\n- Help UCSC students make informed transportation decisions.\n- Relieve metro congestion during peak hours.\n- Build a maintainable project that can be expanded by the school community.\n\nHow It Was Built:\n- Utilizes GPS hardware already installed on loop buses by UCSC.\n- Data from GPS receivers processed with ExpressJS.\n- Bus data stored in Firebase for real-time access.\n- Frontend built with React and Node.js, displaying live bus locations on a map.\n- Data transferred using LibCurl library and C.\n\nChallenges:\n- Difficulty obtaining GPS data from relay stations on campus (only 3 of 5 stations functional).\n- Technical hosting issues delayed development.\n- Limited loop bus operation on weekends restricted data.\n- Required coordination with UCSC staff to gain server access.\n\nAccomplishments:\n- Built an app with immediate, positive impact for UCSC students.\n- Delivered a simple, mobile-friendly interface for quick data access.\n- Created a system that improves metro bus availability by encouraging loop bus usage.\n- Integrated multiple frameworks into a reliable, real-time solution.\n\nWhat We Learned:\n- The importance of planning and design before coding.\n- How to integrate multiple frameworks for a seamless product.\n- Simplicity in interface design is key to adoption.\n- Assigning tasks based on team strengths improves efficiency.\n\nWhat’s Next:\n- Collecting more data to train a machine learning model for accurate bus arrival predictions.\n- Upgrading or replacing campus GPS hardware to improve coverage.\n- Installing missing GPS hardware on all loop buses.",
    "github": "https://github.com/IdkwhatImD0ing/SlugLoop",
    "demo": "https://youtu.be/DlAGp-IjtJM"
  },
  {
    "id": "architect-rev4cq",
    "name": "Architect",
    "summary": "Architect is an AI-powered hackathon assistant that generates frontend designs, backend architectures, and feasibility analyses. Submitted to **Hacks for Hackers**, Winner of Third Overall.",
    "details": "Inspiration:\nArchitect was inspired by the fast-paced energy of hackathons, where participants struggle to quickly turn ideas into working prototypes. The project’s creators envisioned a digital ally powered by language models to generate frontend designs, backend architectures, and feasibility analyses, helping innovators bring concepts to life in the high-pressure hackathon environment.\n\nWhat It Does:\n- Uses three separate language models (LLM chains) to support ideation.\n- Generates frontend design ideas, features, and technologies.\n- Suggests backend architectures, database solutions, and frameworks.\n- Evaluates project feasibility based on technologies, skills, and time constraints.\n\nHow It Was Built:\n- Developed as a pipeline integrating three LLMs:\n  - FLLM: Frontend Design Language Model\n  - BLLM: Backend Architecture Language Model\n  - FALM: Feasibility Analysis Language Model\n- Simple UI for users to provide project ideas and skill categories.\n\nChallenges:\n- Ensuring feasibility analysis is accurate and realistic.\n- Managing data flow between multiple LLMs.\n- Designing an intuitive user experience with clear feedback.\n\nAccomplishments:\n- Built a functional LLM pipeline for frontend, backend, and feasibility tasks.\n- Implemented feasibility checks to assess project viability.\n- Designed a user-friendly interface to simplify inputs and outputs.\n\nWhat We Learned:\n- How to integrate multiple LLMs for distinct yet coordinated tasks.\n- The importance of intuitive UI/UX in enhancing user adoption.\n- The complexity of applying LLMs to feasibility evaluation.\n\nWhat’s Next:\n- Add an “Inspirational Mode” to suggest novel hackathon ideas based on trends.\n- Integrate educational resources to help users upskill if their project goals exceed their current capabilities.",
    "github": "https://github.com/Ananya2001-an/Architect",
    "demo": "https://youtu.be/xUdMTt6hMqA"
  },
  {
    "id": "fundriser",
    "name": "FundRiser",
    "summary": "FundRiser is a Web3 crowdfunding platform built on the Hedera blockchain, supporting both anonymous crypto and card-based donations. Submitted to **Web3Apps - Hosted by MLH Hackathon**, Winner of **Best Use of Microsoft Cloud for Your Community** and **Best Use of Circle**.",
    "details": "Inspiration:\nFundRiser was inspired by the idea of creating a web3 crowdfunding platform that uses blockchain (Hedera network) for transparency and security. The team wanted to make fundraising seamless, secure, and flexible—allowing both anonymous crypto donations and traditional card payments, with simple authentication via Microsoft Azure.\n\nWhat It Does:\n- Lets users create and manage fundraising campaigns with goals and project details.\n- Accepts donations via cryptocurrency (MetaMask) or traditional card payments (Circle → USDC).\n- Supports anonymous donations while keeping transaction records secure and transparent on Hedera.\n- Provides secure authentication through Microsoft Azure accounts.\n\nHow It Was Built:\n- Backend: Express (Node.js) for routing and server logic.\n- Frontend: React for a dynamic UI, styled with Tailwind CSS for responsive design.\n- Blockchain: Hedera network for secure, immutable donation records and smart contracts.\n- Authentication: Microsoft Azure for robust and trusted login.\n- Payments: Circle for credit card donations, converting them to USDC.\n\nChallenges:\n- Integrating multiple APIs and ensuring seamless functionality.\n- Learning and implementing Azure APIs for authentication.\n- Handling Circle’s payment integration with blockchain interoperability.\n\nAccomplishments:\n- Built a fully functional web3 crowdfunding platform.\n- Achieved smooth integration of blockchain, authentication, and payment solutions.\n- Delivered a user-friendly and secure fundraising experience.\n\nWhat We Learned:\n- Hands-on blockchain integration with Hedera.\n- Implementing secure authentication with Microsoft Azure.\n- Payment handling via Circle and anonymous crypto donations.\n- Strengthened skills with React, Express, and Tailwind CSS.\n\nWhat’s Next:\n- Enhanced campaign management (progress tracking, contributor updates).\n- Social media integration for easier campaign sharing.\n- Advanced analytics to give creators insights into donations and engagement.\n- Localization with multi-language and multi-currency support.\n- Partnerships with organizations and influencers.\n- Community-building features such as forums and collaboration spaces.",
    "github": "https://github.com/Satoshi-Sh/crowdfunding",
    "demo": "fundriser"
  },
  {
    "id": "dreamcatch",
    "name": "DreamCatch",
    "summary": "DreamCatch is an AI-driven platform that records, analyzes, and interprets dreams, helping users uncover recurring themes and subconscious insights. Submitted to **UC Berkeley AI Hackathon**.",
    "details": "Inspiration:\nDreams remain a fascinating mystery that often go unexplored. At DreamCatch, we believe dreams hold potential for self-discovery and personal growth. Our goal was to create a platform where users can record, analyze, and learn from their dreams with the help of AI.\n\nGoals:\n- Create an AI-based platform that records and analyzes dreams.\n- Identify recurring themes and patterns.\n- Provide psychoanalytic insights.\n- Build an open, community-driven platform.\n- Enable sharing and discussion among dream enthusiasts.\n\nBuilt With:\n- NextJS & TailwindCSS (frontend)\n- Advanced AI (voice-to-text transcription)\n- Sentiment Analysis (mood identification)\n- Keyword Extraction (themes & symbols)\n- Psychoanalysis algorithms (dream insights)\n- Firebase (secure cloud storage)\n\nChallenges:\n- Ensuring seamless and accurate transcription.\n- Interpreting moods, symbols, and themes effectively.\n- Building an intuitive, user-friendly interface.\n- Maintaining data security and privacy.\n- Encouraging meaningful community engagement.\n\nAccomplishments:\n- Developed an AI-driven dream analysis platform with an intuitive interface.\n- Enabled users to record, analyze, and share dreams.\n- Implemented advanced integrations:\n  • Hume.AI for emotion extraction\n  • AnyScale to scale Whisper transcription models\n- Fostered a community space for dream discussion.\n\nWhat We Learned:\n- Importance of user-centric design.\n- Potential of AI for interpreting human emotions.\n- Value of community collaboration.\n- Practical integration of advanced technologies into one cohesive platform.\n\nWhat’s Next:\n- Machine learning models for dream trend analysis.\n- Dream visualization with images or illustrations.\n- Expanded community interaction features.\n- Smart reminders for recording dreams.\n- Personalized insights and recommendations.",
    "github": "https://github.com/IdkwhatImD0ing/DreamCatcher",
    "demo": "https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/511/375/datas/gallery.jpg"
  },
  {
    "id": "intelliconverse",
    "name": "IntelliConverse",
    "summary": "IntelliConverse is an AI-powered learning support tool that uses ChatGPT, speech-to-text, and text-to-speech to assist individuals with dyslexia, ADHD, and reading challenges. Submitted to **HackDavis 2023** and awarded **Best Use of MongoDB Atlas**.",
    "details": "Inspiration:\nIntelliConverse is designed to support individuals with learning disabilities such as dyslexia and ADHD, as well as those with reading difficulties. By combining text and voice input/output, it fosters effective communication and empowers users to overcome learning challenges.\n\nHow We Built It:\nBackend Development:\n- Custom NextJS server and Express server\n- Routes for:\n  • Speech-to-text\n  • Text-to-speech\n  • Adding, querying, and removing data from Milvus and MongoDB\n  • ChatGPT integration\n\nFrontend Development:\n- NextJS with Material UI\n- Three chat interfaces:\n  • Regular chat\n  • Voice chat\n  • Voice chat with PDF (assistant answers questions with references from PDFs)\n\nChallenges:\n- Testing Azure Speech-to-Text integration for accuracy and reliability.\n- Recording and formatting frontend audio for Azure Speech-to-Text while ensuring high-quality input.\n- Debugging, troubleshooting, and fine-tuning audio processing for seamless performance.\n\nAccomplishments:\n- Successfully implemented text-to-speech and speech-to-text features.\n- Enabled voice chat and voice chat with PDF functionalities.\n- Delivered a versatile platform supporting multiple learning styles.\n\nWhat We Learned:\n- Gained deeper understanding of audio capture and processing.\n- Enhanced expertise in Azure Speech-to-Text integration.\n- Improved frontend audio handling and user experience design.\n\nWhat’s Next:\n- Customization and training of ChatGPT for a safer, more supportive environment.\n- Collecting and applying user feedback to refine user experience.\n- New features: multi-PDF uploads, audio streaming for faster responses.\n- Bug fixes: over-context limits, UI/UX improvements.",
    "github": "https://github.com/IdkwhatImD0ing/study-assistant",
    "demo": ""
  },
  {
    "id": "gitpt",
    "name": "GitPT",
    "summary": "GitPT is a tool that helps students explore and understand GitHub repositories affordably using GPT-3 instead of GPT-4. Submitted to **SB Hacks IX** and awarded **Winner: Student Life Hack**.",
    "details": "Inspiration:\nGPT-4 can store entire codebases in its 32k context, but each query costs almost $2. Ten queries could be nearly $20, which is unsustainable for students. GitPT provides an affordable alternative using low-context chatbots such as GPT-3.\n\nWhat It Does:\nGitPT parses a GitHub repository link to extract the owner and repository names. It then fetches repository information via the OpenAI API and summarizes the contents. The app displays files like README.md or code files in a user-friendly format with syntax highlighting and explanations, saving developers time when exploring new repositories.\n\nHow We Built It:\n- Frontend: Three.js, Next.js, Tailwind CSS, TypeScript\n- Backend: Node.js and Express\n- Database: MongoDB (non-SQL) for storing summaries and content, Milvus as a vector database\n- OpenAI API: Used to summarize repository files\n- Integrated interactive SSR 3D models into the UI/UX with Three.js\n\nChallenges:\n- Designing an intuitive and visually appealing UI with Three.js integration.\n- Managing complex data models and ensuring seamless search and messaging features.\n- Building a breadcrumb and tree navigation system for exploring repository paths.\n\nAccomplishments:\n- Successfully integrated a responsive Three.js wireframe torus knot into the UI.\n- Built a working system enabling affordable exploration of repositories.\n- Gained hands-on experience in front-end, back-end, and database management.\n\nWhat We Learned:\n- Best practices for front-end design with Three.js in a Next.js TypeScript project.\n- Effective use of server-side rendering and integration of vector databases with non-SQL databases.\n- Value of strong communication and collaboration within the team.\n\nWhat’s Next:\n- Official deployment to provide students with a low-cost alternative to GPT-4 queries.\n- Outreach to promote repository understanding and learning.\n- Expand features, improve usability, and develop a mobile app.\n- Build partnerships with educational organizations to grow the user base.",
    "github": "https://github.com/simon-quach/GitPT",
    "demo": "https://youtu.be/Sxe8s0MH3Qo"
  },
  {
    "id": "monkeysign",
    "name": "MonkeySign",
    "summary": "MonkeySign is an arcade-style platform for learning American Sign Language (ASL) through real-time hand gesture recognition. Submitted to **Citrus Hack 2023** and awarded **Winner: Best Tracks Hack 'New Frontiers'**.",
    "details": "Project Overview:\nMonkeySign is a game-like platform designed to teach American Sign Language (ASL) in an engaging and interactive way. Inspired by MonkeyType, it brings a similar arcade-style experience to ASL learning, emphasizing education and social good.\n\nCore Features:\n- Real-time hand gesture detection to recognize ASL letters.\n- Users progress through the game by successfully signing letters, improving their ASL skills in a fun environment.\n\nTechnical Details:\n- Frontend: Vite and TailwindCSS for a responsive UI.\n- Backend: Flask and socket.io for communication between frontend and ML model.\n- Detection: Handtrack.js for hand detection and a custom CNN trained on ASL data for letter recognition.\n\nChallenges & Solutions:\n- Raspberry Pi Camera malfunctioned during the hackathon, so the team pivoted to an arcade-like web experience.\n- Model training difficulties led to splitting the process into two models (hand detection and letter recognition).\n- Bounding box integration with webcam data required multiple iterations but was successfully implemented.\n\nAccomplishments:\n- Successfully integrated frontend, backend, and Raspberry Pi components into a cohesive system.\n- Created an interactive, arcade-style ASL learning tool.\n\nLessons Learned:\n- Training object detection models.\n- Handling live data streams and computer vision challenges.\n- Coordinating multiple technologies into one functional product.\n\nFuture Development:\n- Add multiplayer functionality for collaborative learning.\n- Fix Raspberry Pi integration for an arcade-style booth experience.\n- Improve ML models for better accuracy across conditions.",
    "github": "https://github.com/IdkwhatImD0ing/MonkeySign",
    "demo": "https://youtu.be/atvJ0q10_M8"
  },
  {
    "id": "memgen-focused-memory-gpt",
    "name": "MemGen: Intelligent Vector-Based Cover Letter Creator",
    "summary": "MemGen is a platform that generates tailored cover letters and resumes using vector databases and large language models. Submitted to **LA Hacks 2023**.",
    "details": "Overview:\nMemGen enables users to create customized cover letters by searching through their past experiences using large language models and embeddings.\n\nInspiration:\nWriting unique and personalized cover letters can be tedious, especially for programmers. While sometimes optional, not submitting one often leaves candidates feeling guilty. MemGen was built to streamline this process, empowering users to generate professional, tailored cover letters quickly and effectively.\n\nGoals:\n- Use vector databases to store past experiences and enable semantic search with NLP.\n- Allow users to generate cover letters using LLMs and stored career data.\n- Provide a user-friendly interface for uploading documents and creating outputs.\n\nTech Stack:\n- Frontend: React, NextJS, Tailwind CSS, Axios, Material UI, Auth0.\n- Backend: ExpressJS, OpenAI, Firebase Admin, Auth0, Cohere, Google Cloud.\n- Database: Milvus, Firebase, Zillis.\n\nChallenges:\n- Integrating multiple technologies and databases.\n- Learning and applying vector database concepts.\n- Ensuring platform security and data protection.\n- Implementing new APIs.\n\nWhat We Learned:\n- Effective use of vector databases for embeddings and fast semantic search.\n- Integrating diverse technologies into a seamless product.\n- Building secure, scalable systems for user-facing applications.\n\nWhat’s Next:\n- Integrate Stripe API for billing.\n- Develop a custom vector database for scalability and performance.\n- Enhance UI/UX for improved accessibility.\n- Add new features like job posting and job search functionality.",
    "github": null,
    "demo": "https://youtu.be/v5PAwb0BYtw"
  },
  {
    "id": "assistance",
    "name": "Assistance: Multi-LLM Cover Letter Creator",
    "summary": "Assistance is a platform that lets users converse with multiple large language models simultaneously, including Bing Chat, Google Bard, and ChatGPT. Submitted to **ACMHacks 2023**, Winner of **Best Global Solution**.",
    "details": "Overview:\nAssistance enables intuitive interaction with multiple LLMs in a single platform. Users can query models simultaneously, switch between them, and maintain context across conversations.\n\nInspiration:\nThe recent surge of LLMs—such as ChatGPT, Meta LLaMA, Google Bard, and Bing Chat—highlighted how each model has different strengths due to varying training data. Instead of manually testing each, Assistance unifies them into one interface for comparison and efficiency.\n\nGoals:\n- Query all supported models at once.\n- Allow users to select and continue conversations with specific models.\n- Preserve conversational context across multiple queries.\n\nTechnologies Used:\n- Frontend: Gatsby Framework, Material UI.\n- Backend: FastAPI.\n- APIs: Reverse-engineered Python APIs for model interaction.\n\nChallenges:\n- First experience with Gatsby and FastAPI.\n- Reverse-engineered APIs had minimal documentation, requiring creative problem-solving.\n\nWhat We Learned:\n- Working with new frontend and backend frameworks.\n- Handling incomplete or undocumented APIs.\n- Converting strings to markdown displays.\n- Advanced React context usage.\n- Importance of memoization to reduce re-renders.\n- Managing asynchronous code and custom Docker containers.\n\nWhat’s Next:\n- Strengthen backend and frontend security.\n- Add Firebase authentication for user accounts and multi-device access.\n- Enable users to integrate their own API keys.\n- Introduce paid subscription tiers with premium features.",
    "github": "https://github.com/IdkwhatImD0ing/Assistance",
    "demo": "https://youtu.be/DAuMnWCKtSk"
  },
  {
    "id": "sink-or-swim",
    "name": "Sink or Swim (SOS)",
    "summary": "Sink or Swim (SOS) is an interactive webapp that predicts Titanic survival chances and generates personalized passenger stories using AI. Submitted to **Planet Unity Spring 2023**, Winner of **Top 10 Award**.",
    "details": "Overview:\nSink or Swim (SOS) is a machine learning and storytelling webapp inspired by the Titanic disaster of April 15, 1912. It predicts a passenger's survival likelihood and generates a personalized narrative of their experience on the ship.\n\nInspiration:\nThe tragedy of the Titanic has long captivated the world. Our team wanted to combine historical exploration with modern AI to simulate survival chances while making history come alive through personalized storytelling.\n\nHow It Works:\n- A recurrent neural network trained on the Kaggle Titanic dataset predicts survival probability based on age, gender, and socio-economic class.\n- Users input personal details (e.g., name, age, class).\n- OpenAI’s language generation technology creates a unique narrative blending prediction results with user inputs, offering an immersive Titanic experience.\n\nTech Stack:\n- Machine Learning: TensorFlow, PyTorch, scikit-learn\n- Dataset: Kaggle Titanic dataset\n- Frontend: HTML, Tailwind CSS, NextJS 14\n- Backend: Flask\n- Other: Axios, Framer Motion (for animations)\n\nChallenges:\n- Cleaning and preprocessing the dataset for accuracy.\n- Balancing ML model accuracy with computational efficiency.\n- Integrating ML with web frameworks and language generation.\n- Learning new tools like NextJS 14.\n- Handling Axios post request limits.\n- Ensuring a user-friendly and secure interface.\n\nAccomplishments:\n- Developed an accurate, efficient recurrent neural network.\n- Successfully integrated ML predictions with generative storytelling.\n- Built a scalable, engaging, and user-friendly interface.\n- Ensured data privacy and security.\n- Overcame technical hurdles under time constraints.\n\nWhat We Learned:\n- The importance of thorough data cleaning for ML accuracy.\n- Building engaging, user-friendly apps with NextJS 13/14.\n- Implementing animations with Framer Motion.\n- Designing scalable ML-driven applications.\n\nSink or Swim (SOS) demonstrates how AI can bring history to life through interactive storytelling while tackling technical, data, and design challenges.",
    "github": "https://github.com/simon-quach/sink-or-swim",
    "demo": "https://youtu.be/4V-2l-Zel5s"
  },
  {
    "id": "progno-d",
    "name": "Progno-D",
    "summary": "Progno-D is an AI-based medicine recommendation system that suggests multiple drugs using past customer reviews and sentiment analysis. Submitted to **AI Hacks**, Winner of **First Overall**.",
    "details": "Overview:\nProgno-D is an AI-powered medicine recommendation system designed to simplify the process of selecting prescriptions. It leverages past customer reviews, ratings, and sentiment analysis to provide users with reliable drug recommendations along with detailed information.\n\nInspiration:\nWith countless medications available, consumers often face difficulty choosing the right prescription. Progno-D was built to solve this problem by providing data-driven recommendations powered by natural language processing and historical feedback.\n\nWhat It Does:\n- Analyzes drug ratings, reviews, and customer sentiments.\n- Recommends the most suitable medications based on user needs.\n- Provides detailed drug information for better decision-making.\n\nHow We Built It:\n- Dataset: UCI Machine Learning Repository.\n- Data cleaning and sentiment processing: NLTK, Matplotlib.\n- Key features: Customer ratings and reviews.\n- Model training: Python, Pandas, NumPy.\n- Frontend: React for a clean, intuitive interface.\n\nChallenges:\n- Cleaning and preprocessing complex datasets.\n- Performing exploratory data analysis (EDA) to extract insights.\n- Deploying the AI model for usability and reliability.\n\nAccomplishments:\n- Built and deployed a recommendation model with strong accuracy.\n- Designed a practical tool that makes healthcare decisions more accessible.\n\nWhat We Learned:\n- Strengthening technical skills in React, Python, NLTK, Pandas, NumPy, and GitHub.\n- Improving teamwork, collaboration, and time management.\n\nWhat’s Next:\n- Expanding recommendations to cover more illnesses.\n- Suggesting local and online sources for purchasing medications.\n- Exploring integration for direct medication procurement.\n\nProgno-D highlights how AI can positively impact healthcare by empowering patients with informed, data-driven prescription choices.",
    "github": "https://github.com/RupakKadhare15/Progno-D",
    "demo": "https://youtu.be/eqZS5WqHXW4"
  },
  {
    "id": "paddyplantprognosis",
    "name": "PaddyPlantPrognosis",
    "summary": "PaddyPlantPrognosis is a computer vision–powered web app that helps farmers diagnose rice crop diseases quickly and accurately. Submitted to **Hackrithmitic 2**, Winner of **Best Data Science Hack**.",
    "details": "Overview:\nPaddyPlantPrognosis is a web app designed to help farmers identify diseases in rice crops using computer vision. By analyzing images of paddy plants, it provides real-time diagnoses and actionable treatment insights to protect crops and reduce yield loss.\n\nInspiration:\nRice is the staple food for much of the world, especially across Asia. However, paddy cultivation is highly vulnerable to diseases and pests, which can cause severe crop damage and reduced yields. Manual diagnosis is slow and costly, motivating the creation of a fast, affordable, and accessible AI-powered solution.\n\nWhat It Does:\n- Farmers capture or upload images of rice plants.\n- Computer vision models analyze the image to detect diseases or pests.\n- The app returns diagnoses with recommended actions for treatment.\n- Optimized for mobile devices, supporting direct camera use even in low-connectivity regions.\n\nHow We Built It:\n- Frontend: Vite + Tailwind CSS for a responsive, mobile-friendly UI.\n- Backend: Flask (Python) for API and model integration.\n- Machine learning: TensorFlow + Keras for model training on rice disease datasets.\n- Image processing: OpenCV, with NumPy for numerical computation.\n- Focus on lightweight design for rural accessibility.\n\nChallenges:\n- Limited documentation for new frameworks like Vite and Tailwind CSS.\n- Hardware constraints during ML model training.\n- Handling Git merge conflicts during team collaboration.\n\nAccomplishments:\n- Built and deployed a computer vision web app for real-time rice disease detection.\n- Integrated ML models with strong accuracy in diagnosing crop issues.\n- Designed a mobile-first, accessible interface tailored for farmers.\n- Created a scalable platform that can directly improve farmers’ livelihoods.\n\nWhat We Learned:\n- Applying machine learning and computer vision in agriculture.\n- Preprocessing and augmenting image data for stronger ML performance.\n- Designing accessible UIs for rural and resource-limited use cases.\n- Team collaboration, Git conflict resolution, and project scaling.\n\nWhat’s Next:\n- Expand the model to detect more crop diseases and pests beyond rice.\n- Improve UI/UX for ease of use.\n- Integrate with weather forecasting and pest management tools.\n- Provide treatment recommendations with real-time updates.\n- Scale support for high-resolution images and larger global datasets.\n\nPaddyPlantPrognosis demonstrates how technology can empower farmers, reduce crop loss, and strengthen food security on a global scale.",
    "github": "https://github.com/IdkwhatImD0ing/PaddyPlantPrognosis",
    "demo": "https://youtu.be/M_xLQeglzgU"
  },
  {
    "id": "cash-prize-bounty",
    "name": "Volunteer Hub",
    "summary": "Volunteer Hub is a platform that broadens volunteering opportunities beyond software engineering by matching diverse volunteers with nonprofit projects. Submitted to **Opportunity Hack 2022**, Winner of **3rd Place**.",
    "details": "Overview:\nVolunteer Hub addresses the challenge of limited roles in existing volunteer systems, which traditionally only recognize Hackers and Mentors. It expands opportunities by introducing a structured way for volunteers from diverse backgrounds to find meaningful work aligned with their skills and interests.\n\nTask & Solution:\n- For project-specific roles, volunteers can fill out tailored forms indicating skills, interests, and availability.\n- A dedicated volunteer opportunities page introduces general tasks, enabling sign-ups and notifications.\n- Volunteers are onboarded into Slack and given autonomy to contribute meaningfully.\n\nArchitecture:\nThe system employs a tag-based data architecture to match volunteers with projects. Attributes are mapped across three categories:\n1. Skills ↔ Skill Requirements\n2. Interests ↔ Project Context Themes\n3. Tech Stack Experience ↔ Tech Stack Demands\n\nThis ensures nonprofits can quickly identify well-matched volunteers while volunteers can discover opportunities aligned with their expertise and passions.\n\nImpact:\nVolunteer Hub creates a more inclusive and efficient ecosystem for matching volunteers to projects. By expanding engagement beyond technical roles, it empowers nonprofits with broader support and increases the overall impact of the platform.\n\nAccomplishments:\n- Developed an innovative volunteer matching architecture using tag mapping.\n- Expanded engagement opportunities beyond engineering.\n- Built a scalable solution that nonprofits can use to onboard diverse volunteers efficiently.\n\nWhat’s Next:\n- Expand the tag-based matching framework to support more attributes.\n- Add automated role suggestions for new volunteers.\n- Enhance reporting tools for nonprofits to track volunteer contributions and impact.",
    "github": "https://github.com/IdkwhatImD0ing/frontend-ohack.dev",
    "demo": "https://youtu.be/D8iHwCP37Fk"
  },
  {
    "id": "pool-party-3icj7e",
    "name": "Pool Party",
    "summary": "Pool Party is a mobile-first web app that simplifies carpool coordination by connecting drivers and passengers. Submitted to **GraceHacks**, Winner of **Best Mobile**.",
    "details": "Overview:\nPool Party addresses the common challenge of organizing transportation for groups heading to the same destination. The app makes it easier to coordinate carpools by letting users join as drivers or passengers and choose their preferred match.\n\nWhat It Does:\n- Users can create a pool for a trip and join as a driver or passenger.\n- Drivers can view available passengers and decide who to pick up.\n- Passengers can browse drivers and select who they want to ride with.\n- Built to be mobile-first, but accessible on all platforms.\n\nHow It Was Built:\n- React.js for the frontend\n- JavaScript for scripting\n- Express.js for the backend\n- Material UI for design\n- Hop.io for deployment\n- Real-time database integration\n\nChallenges:\n- Learning and applying Material UI effectively\n- Some teammates were new to React and JavaScript\n- Styling edge cases\n- Handling version control and merge conflicts across four team members\n\nAccomplishments:\n- Built and deployed both frontend and backend on Hop.io\n- Designed a clean, minimalist UI\n- Implemented a real-time database\n- Completed most planned features within time constraints\n\nWhat We Learned:\n- JavaScript scripting and UI/UX design with React\n- Backend development using Express.js\n- Version control and collaboration with Git\n\nFuture Plans:\n- Improve UI/UX design\n- Explore React Native for mobile-first expansion\n- Add Firebase authentication for secure user management",
    "github": "https://github.com/IdkwhatImD0ing/PoolParty.git",
    "demo": "https://mypoolparty.tech"
  },
  {
    "id": "tetris-duels",
    "name": "Tetris Duels",
    "summary": "Tetris Duels is a multiplayer twist on the classic Tetris game, enabling solo and versus modes with friends and family. Submitted to **Funathon**, Winner of **Participation Prize**.",
    "details": "Overview:\nTetris Duels was inspired by competitive Tetris and created as an experiment to build a multiplayer version. It allows players to enjoy solo Tetris or compete against friends and family by sharing links to matches.\n\nWhat It Does:\n- Play Tetris solo or in versus mode\n- Generate and share links so friends can join or spectate matches\n- Maintain state across players using Hop.io Channels\n\nHow It Was Built:\n- Frontend: React (with useState, useRef, and React Router)\n- Backend: Express.js\n- Deployment: Hop.io Ignite\n- Hop.io Channels for client-server state management\n\nChallenges:\n- First time using React, Express.js, and Hop.io\n- Building both frontend and backend solo within hackathon time limits\n- Managing complex objects to handle versus link-sharing and state synchronization\n\nAccomplishments:\n- Successfully built and deployed a functional frontend and backend on Hop.io\n- Implemented versus mode with link sharing\n- Built a backend server for the first time\n- Completed two-thirds of the planned features\n\nWhat Was Learned:\n- React fundamentals: useState, useRef, React Router\n- Express.js for handling HTTP requests and responses\n- Basics of backend server development and deployment\n- Techniques for syncing client-server state\n\nFuture Plans:\n- Improve UI design\n- Add new modes like timed versus mode with extra controls\n- Switch to WebSockets for reduced latency\n- Explore advanced Hop.io features such as chat rooms, private channels, and custom games\n\nSetup Instructions:\n1. Install Node.js and npm\n2. Run 'npm install' to install dependencies\n3. Run 'npm start' to start the app\n4. Visit 'http://localhost:3000' to play locally",
    "github": "https://github.com/IdkwhatImD0ing/tetris-coop",
    "demo": "https://youtu.be/y8wPmVJEqlc"
  },
  {
    "id": "wonder-g6tym1",
    "name": "Wonder",
    "summary": "Wonder is a Telegram bot that raises awareness about endangered animals and educates users on how they can help. Submitted to **Killabytez Hacks**, **EcoHacks**, and **WildHacks II**, Winner of **Second Overall**.",
    "details": "Overview:\nWonder addresses the urgent need to raise awareness about endangered animals threatened by poaching, habitat destruction, and climate change. It aims to educate users and encourage participation in conservation efforts.\n\nWhat It Does:\n- Generates lists of endangered animals with YouTube videos\n- Provides educational information about endangered species\n- Uses Google Maps to find nearby pet shops\n- Offers tips on contributing to animal conservation\n- Includes an about section\n\nHow It Was Built:\n- Landing page: Next.js + GitHub + Vercel\n- Telegram bot: Python\n- Natural Language Processing (NLP) with prompt engineering for responses\n- Google Maps API for location-based features\n\nChallenges:\n- Integrating Google Maps API smoothly into the landing page\n\nAccomplishments:\n- Built a functional and educational Telegram bot in a short timeframe\n- Created a platform that promotes conservation awareness\n- Fostered strong teamwork and collaboration during the hackathon\n\nWhat Was Learned:\n- Developing Telegram bots\n- Implementing APIs and endpoints (Google Maps)\n- Building applications with Next.js\n- Increased understanding of endangered species and conservation issues\n\nFuture Plans:\n- Expand functionality into a full hub for endangered animal awareness\n- Add login/registration with personalized feeds (e.g., endangered animals near the user)\n- Broaden conservation-related educational content and tools",
    "github": "https://github.com/nightsailor/forests",
    "demo": "https://youtu.be/hMYypq6hvWQ"
  },
  {
    "id": "remotetrainer",
    "name": "RemoteTrainer",
    "summary": "RemoteTrainer is a fitness web app that helps students work out from home, track BMI and body fat, and stay motivated. Submitted to **PeddieHacks 2022**, Winner of the **Innovation Prize (High School)**.",
    "details": "Overview:\nRemoteTrainer was built to help students who lack time, money, or motivation for gym workouts. It allows users to exercise from home using available equipment while tracking progress to stay motivated.\n\nWhat It Does:\n- Filters and searches exercises by muscle group and available equipment\n- Displays exercise details and GIF demonstrations\n- Tracks BMI and body fat as users update personal stats\n\nHow It Was Built:\n- Frontend: Next.js\n- Database & Authentication: Firebase Firestore & Firebase Authentication\n- UI: Material UI\n- APIs: Separate APIs for exercise generation and BMI/body fat calculation\n- Axios for API calls\n- Hosting: Cloudflare with a custom Name.com domain\n\nChallenges:\n- Adjusting after some team members were unable to participate\n- Managing time effectively while learning new libraries and tools\n\nAccomplishments:\n- Successfully developed and deployed the site\n- Built a multiselect tool for filtering by equipment and body parts\n- Integrated APIs to dynamically generate personalized exercise recommendations\n\nWhat Was Learned:\n- Advanced Material UI styling, themes, and components\n- Firebase APIs and handling asynchronous operations\n- Effective collaboration using branches, pull requests, and merge conflict resolution\n\nFuture Plans:\n- Add progress graphs\n- Introduce features such as workout favoriting, playlists, streaks, and daily notifications\n- Enhance styling, security, and overall user experience",
    "github": "https://github.com/IdkwhatImD0ing/ExerciseTracker",
    "demo": "https://youtu.be/vL-X-1XrIN0"
  },
  {
    "id": "makemelunch",
    "name": "MakeMeLunch",
    "summary": "MakeMeLunch is a web app that tracks kitchen ingredients and suggests recipes to reduce food waste and encourage variety. Submitted to **Tech Optimum Hacks 2022**.",
    "details": "Overview:\nMakeMeLunch was inspired by the common issues of forgetting ingredients until they spoil and losing interest in meals due to repetition. The app helps users track their kitchen inventory and discover new recipes using those ingredients.\n\nWhat It Does:\n- Allows users to create an account\n- Stores a personalized list of kitchen ingredients in a database\n- Displays relevant recipes based on stored ingredients\n\nHow It Was Built:\n- Framework: Next.js\n- Language: JavaScript\n- API: Spoonacular API for ingredient and recipe data\n- Authentication & Database: Firebase Authentication & Firestore Realtime Database\n- Hosting: Vercel\n- Axios for API handling\n\nChallenges:\n- Learning Next.js, JavaScript, HTML, and React within one day\n- Handling API calls and parsing in Next.js\n- Fast refresh issues requiring restarts\n- Integrating Firebase with Spoonacular API under tight time constraints\n\nAccomplishments:\n- Implemented Firebase Authentication linked with Firestore\n- Enabled successful data transfer between two distinct APIs\n- Integrated Spoonacular API calls via Axios\n- Built both backend and frontend within a single day\n\nWhat Was Learned:\n- JavaScript scripting\n- HTML and CSS formatting in React\n- API calls and handling with Axios\n\nKnown Problems:\n- Ingredient view crashes if no ingredients are added\n- Security flaws in environment variables and database rules\n\nFuture Plans:\n- Add filters for recipe search (e.g., calories)\n- UI improvements\n- Display healthier ingredient substitutions\n- Improve security measures\n- Handle edge cases more effectively",
    "github": "https://github.com/IdkwhatImD0ing/MakeMeLunch",
    "demo": "https://youtu.be/SmRpW295Xkk"
  },
  {
    "id": "asl-transcription",
    "name": "ASL Transcription",
    "summary": "ASL Transcription is an augmented reality tool that converts American Sign Language into text in real time, designed to help creators and learners. Submitted to **SnapAR projects**.",
    "details": "Overview:\nASL Transcription was inspired by a deaf friend who wanted to create TikTok and YouTube content without the hassle of manually adding captions. This tool enables real-time ASL transcription, allowing creators to express themselves freely, and it can also serve as a learning tool for practicing ASL speed and accuracy.\n\nWhat It Does:\n- Uses Machine Learning to classify hand motions into ASL in real time\n- Displays transcribed text in augmented reality\n- Provides high accuracy predictions\n- Allows toggling live predictions on/off with a screen tap\n\nHow It Was Built:\n- Lens Studio\n- JavaScript\n- Custom model created using transfer learning and image augmentation\n- Tensorflow Hub MobileNetV2 pretrained model (ImageNet1k)\n- American Sign Language Dataset by David Lee (via Roboflow)\n- Tensorflow & Keras API for fine-tuning\n- Achieved 94% validation accuracy\n\nChallenges:\n- MobileNet and EfficientNet models initially failed to import into Lens Studio\n- Lens Studio documentation was confusing and prone to crashes\n- First dataset was not official ASL, resulting in poor real-world accuracy\n- Small dataset required heavy image augmentation; some hand poses remained hard to classify\n\nAccomplishments:\n- Successfully trained and deployed a ML model in Lens Studio\n- Expanded dataset using advanced augmentation techniques\n- First ML model deployment in a new AR environment\n- Leveraged hand tracking for precise input and reduced erroneous predictions\n\nWhat Was Learned:\n- Advanced image augmentation techniques\n- Deeper Tensorflow/Keras usage, including .onnx and TFLite formats\n- Lens Studio development and JavaScript scripting\n- Greater understanding of ASL hand poses and recognition challenges\n\nKnown Problems:\n- Small dataset limits model accuracy\n- Similar letters (A/S/E, M/N/V) confuse the model\n- Movements for J and Z are not reliably recognized\n\nFuture Plans:\n- Retrain with a larger dataset for improved accuracy\n- Convert Python Word Ninja to JavaScript for probabilistic word splitting (currently unfeasible due to prediction errors)",
    "github": "https://github.com/IdkwhatImD0ing/SnapAr",
    "demo": "https://youtu.be/Nat5vQGsyxA"
  },
  {
    "id": "covinet",
    "name": "CoviNet",
    "summary": "CoviNet is a networking and support app built during the Covid pandemic to connect people, share updates, and provide resources. Submitted to **CruzHacks 2022**. Winner of **Sponsor Award – QB3 @ UCSC**.",
    "details": "Overview:\nCoviNet was originally inspired by the idea of AirBnB-style roommate matching for Covid-positive individuals, but evolved into a broader networking and support app. It allows users to connect with others nearby, stay informed, and access helpful Covid-related resources.\n\nWhat It Does:\n- Connects users with nearby people who also have Covid to share support and make friends\n- Provides verified Covid-related news updates\n- Suggests educational and informational videos\n- Tracks users’ Covid test report records\n- Helps users find nearby Covid testing centers\n\nHow It Was Built:\n- Dart\n- Flutter\n- APIs: YouTube, Firebase, Google Cloud, News API\n- Synthetic Dataset from Gretel\n\nChallenges:\n- Database access issues with Firebase\n- Implementing Firebase real-time database methods\n- Integrating multiple API links efficiently\n\nAccomplishments:\n- Successfully integrated Firebase for data storage and messaging\n- Added Google Maps and location storage\n- Built messaging features with Firebase\n- Implemented YouTube and News APIs\n\nWhat Was Learned:\n- Mobile app development using Flutter\n- How to integrate and link multiple APIs in a single application\n- Storing and managing user data securely in databases\n- Implementing Firebase Firestore real-time database\n- Using Gretel to generate synthetic datasets for testing\n\nFuture Plans:\n- Improve dataset accuracy\n- Enhance user interface and design\n- Strengthen security for sensitive health data",
    "github": "https://github.com/Arjun-Mishra-312/covinet",
    "demo": "https://youtu.be/PJ6XaJ0rFDQ"
  }
]